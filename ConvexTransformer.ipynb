{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJJChQ+7EW3wrdcy5MDcEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/PyITD/blob/main/ConvexTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMyAv_7rxOyW",
        "outputId": "42b513c9-aa72-487d-f65e-ee67a761db02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Prepare the Shakespeare dataset for character-level language modeling.\n",
        "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
        "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
        "encoder and decoder and some other related info.\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    base_dir = Path(__file__).parent\n",
        "except NameError:\n",
        "    base_dir = Path(os.getcwd())  # fallback if __file__ is not defined (e.g. in REPL)\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(base_dir), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(base_dir), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(base_dir), 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(os.path.join(os.path.dirname(base_dir), 'meta.pkl'), 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#if you use my ideas, please credit me, dont just steal\n",
        "joshuah.rainstar@gmail.com\n"
      ],
      "metadata": {
        "id": "q47er8Ni84xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F, math\n",
        "from typing import Literal\n",
        "\n",
        "# --------‑‑‑‑ helper ---------------------------------------------------\n",
        "@torch.jit.script   # optional: proves it scripts\n",
        "def next_pow_two(x: int) -> int:\n",
        "    \"\"\"\n",
        "    TorchScript‑safe power‑of‑two ceiling using tensor math.\n",
        "    \"\"\"\n",
        "    t = torch.tensor(float(x))            # scalar tensor\n",
        "    k = torch.ceil(torch.log2(t)).int()   # round‑up exponent\n",
        "    return int((2 ** k).item())           # convert back to Python int\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  S4DFFT  (augmented)\n",
        "# ---------------------------------------------------------------------\n",
        "class S4DFFT(nn.Module):\n",
        "    \"\"\"\n",
        "    Diagonal State‑Space (S4D) layer with length‑agnostic FFT or recurrent scan.\n",
        "\n",
        "      x : (B,T,D)  ➜  y : (B,T,D)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        N: int          = 64,          # # diagonal modes\n",
        "        init: str       = \"hippoD\",    # 'hippoD' | 'inverse' | 'linear'\n",
        "        short_thresh: int = 512,       # switch to recurrent if T ≤ this\n",
        "        tau_min: float  = 1e-4,        # clamp on exp(log_tau)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert N % 2 == 0, \"N must be even (conjugate pairs).\"\n",
        "\n",
        "        self.d_model, self.N = d_model, N\n",
        "        self.tau_min = tau_min\n",
        "\n",
        "        # unconstrained parameters for N/2 distinct modes\n",
        "        self.log_tau = nn.Parameter(torch.randn(N // 2))\n",
        "        self.freq    = nn.Parameter(torch.randn(N // 2))\n",
        "        self.B       = nn.Parameter(torch.randn(N // 2))\n",
        "        self.C       = nn.Parameter(torch.randn(N // 2))\n",
        "\n",
        "        # input/output projections\n",
        "        self.in_proj  = nn.Linear(d_model, N // 2, bias=False)\n",
        "        self.out_proj = nn.Linear(N // 2, d_model, bias=False)\n",
        "\n",
        "        # learnable global time‑scale Δt  (log‑domain)\n",
        "        self.log_dt = nn.Parameter(torch.zeros(()))\n",
        "\n",
        "        self._init_modes(init)\n",
        "\n",
        "    # ----- initialisers --------------------------------------------------\n",
        "    def _init_modes(self, kind: Literal[\"hippoD\", \"inverse\", \"linear\"]):\n",
        "        n = torch.arange(self.N // 2)\n",
        "        with torch.no_grad():\n",
        "            self.log_tau.fill_(math.log(0.5))\n",
        "            if kind == \"hippoD\":\n",
        "                self.freq.copy_(math.pi * (2*n + 1) / 2)\n",
        "            elif kind == \"inverse\":\n",
        "                self.freq.copy_((self.N / math.pi) / (2*n + 1))\n",
        "            elif kind == \"linear\":\n",
        "                self.freq.copy_(math.pi * n)\n",
        "            else:\n",
        "                raise ValueError(kind)\n",
        "            nn.init.normal_(self.B,  mean=1.0, std=0.2)\n",
        "            nn.init.normal_(self.C,  std=1.0 / math.sqrt(self.N/2))\n",
        "\n",
        "    # ---------------------------------------------------------------------------\n",
        "    # Real‑only kernel builder\n",
        "    # ---------------------------------------------------------------------------\n",
        "    def _kernel_fft(self, T: int):\n",
        "        \"\"\"\n",
        "        Return RFFT(K) where K is the real convolution kernel of length T.\n",
        "          output: (N, L/2+1) complex\n",
        "        Everything up to the final rfft is real‑typed.\n",
        "        \"\"\"\n",
        "        L   = next_pow_two(2 * T)\n",
        "\n",
        "        dt   = torch.exp(self.log_dt)                      # scalar\n",
        "        tau  = torch.exp(self.log_tau).clamp(min=self.tau_min)   # (N/2,)\n",
        "        angle = self.freq * dt                                   # (N/2,)\n",
        "\n",
        "        # |lam|  = exp(-tau*dt)            (real)\n",
        "        # arg(lam)= angle                  (real)\n",
        "        lam_mag = torch.exp(-tau * dt)                         # (N/2,)\n",
        "        log_gain = (self.B.abs() + 1e-9).log() + \\\n",
        "                  (self.C.abs() + 1e-9).log()                 # (N/2,)\n",
        "\n",
        "        i = torch.arange(T, device=tau.device)                 # (T,)\n",
        "\n",
        "        # amplitude term  (N/2,T)   — still real\n",
        "        amp = torch.exp(log_gain[:, None] + i[None] * torch.log(lam_mag)[:, None])\n",
        "\n",
        "        # phase term\n",
        "        phase = i[None] * angle[:, None]                       # (N/2,T)\n",
        "\n",
        "        K_half = amp * torch.cos(phase)                        # (N/2,T) real\n",
        "\n",
        "        # build full length‑N kernel (conjugate pair ⇒ symmetry in mode index)\n",
        "        K_full = torch.cat([K_half, K_half.flip(0)], dim=0)     # (N,T) real\n",
        "\n",
        "        return torch.fft.rfft(K_full, n=L, dim=-1)             # (N,L/2+1) complex\n",
        "\n",
        "    # ----- forward (FFT or scan) ----------------------------------------\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        B, T, _ = x.shape\n",
        "        x_proj  = self.in_proj(x)                               # (B,T,N/2)\n",
        "        x_modes = torch.cat([x_proj, x_proj.flip(-1)], dim=-1)  # (B,T,N)  real\n",
        "\n",
        "        L  = next_pow_two(2 * T)\n",
        "        Uf = torch.fft.rfft(x_modes, n=L, dim=1).transpose(1, 2)   # (B,N,L/2+1)\n",
        "\n",
        "        Kf = self._kernel_fft(T)                                   # (N,L/2+1)\n",
        "        Yf = Uf * Kf[None]                                         # broadcast\n",
        "\n",
        "        y_modes = torch.fft.irfft(Yf, n=L, dim=2)[..., :T]          # (B,N,T)\n",
        "        y_modes = y_modes.transpose(1, 2)                          # (B,T,N)\n",
        "        y       = y_modes[..., : self.N // 2]                       # (B,T,N/2)\n",
        "        return self.out_proj(y)\n",
        "\n",
        "\n",
        "class S4PreMix(nn.Module):\n",
        "    def __init__(self, embed_dim, inner_dim, heads, N_modes=64):\n",
        "        super().__init__()\n",
        "        assert inner_dim % heads == 0\n",
        "        self.heads = heads\n",
        "        self.d_k   = inner_dim // heads\n",
        "\n",
        "        # ---- NEW: linear up‑proj to inner_dim -------------------------\n",
        "        self.up   = nn.Linear(embed_dim, inner_dim, bias=False)\n",
        "\n",
        "        # ---- S4D operates at reduced width ---------------------------\n",
        "        self.s4d  = S4DFFT(d_model=inner_dim, N=N_modes)\n",
        "        self.qkv  = nn.Linear(inner_dim, inner_dim * 3, bias=False)\n",
        "\n",
        "    def forward(self, x):                       # x: (B,S,E)\n",
        "        z  = self.s4d(self.up(x))               # (B,S,inner_dim)\n",
        "        q, k, v = self.qkv(z).chunk(3, dim=-1)  # each (B,S,inner_dim)\n",
        "\n",
        "        B, S, _ = x.shape\n",
        "        new_shape = (B, S, self.heads, self.d_k)\n",
        "\n",
        "        # safe reshape regardless of contiguity\n",
        "        q = q.reshape(new_shape).transpose(1, 2)   # (B,H,S,d_k)\n",
        "        k = k.reshape(new_shape).transpose(1, 2)\n",
        "        v = v.reshape(new_shape).transpose(1, 2)\n",
        "        return q, k, v\n",
        "# ----------  Positive weight layer with Hoedt–Klambauer init ----------\n",
        "# ---------------------------------------------------------------------\n",
        "# PositiveLinear – strictly‑positive weights with HL init + safe interface\n",
        "# ---------------------------------------------------------------------\n",
        "class PositiveLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, bias=True):\n",
        "        super().__init__()\n",
        "        self.raw  = nn.Parameter(torch.empty(d_out, d_in))\n",
        "        self.bias = nn.Parameter(torch.empty(d_out)) if bias else None\n",
        "        with torch.no_grad():\n",
        "            nn.init.normal_(self.raw, mean=math.log(math.sqrt(2/d_in)), std=0.2)\n",
        "            if self.bias is not None: self.bias.zero_()\n",
        "\n",
        "    @property\n",
        "    def weight(self):                        # strictly positive\n",
        "        return F.softplus(self.raw)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.weight, self.bias)\n",
        "\n",
        "# ---------------- ICNN petal -----------------------------------------\n",
        "class ICNN(nn.Module):\n",
        "    def __init__(self, dim, hidden_dims):\n",
        "        super().__init__()\n",
        "        layers = [PositiveLinear(dim if i==0 else h, h) for i, h in enumerate(hidden_dims)]\n",
        "        layers.append(PositiveLinear(hidden_dims[-1], dim))        # keep dimension\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):                                          # (..., D)\n",
        "        z = x\n",
        "        for layer in self.layers:\n",
        "            z = F.softplus(layer(z))\n",
        "        return z\n",
        "\n",
        "#locally convex gate\n",
        "class ConvexGate(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, 1, bias=True)\n",
        "\n",
        "    def forward(self, x):                       # (...,D)\n",
        "        u = F.softplus(self.lin(x))             # convex, ≥0\n",
        "        return 1.0 - torch.exp(-u)              # convex, ∈(0,1)\n",
        "\n",
        "\n",
        "class ConvexGate(nn.Module):\n",
        "    \"\"\"\n",
        "    Convex & bounded gate: g(x) = 1 - exp(-softplus(Wx + b)) ∈ (0,1)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim: int):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, 1, bias=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        u = F.softplus(self.lin(x))      # convex, ≥ 0\n",
        "        return 1.0 - torch.exp(-u)       # convex, ∈ (0,1)\n",
        "\n",
        "class ScalarHull(nn.Module):\n",
        "    \"\"\"\n",
        "    Scalar-valued convex hull with a bounded convex gate.\n",
        "      x : (..., D)  ->  y : (...,)     (scalar output)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim: int, hidden: List[int], petals: int):\n",
        "        super().__init__()\n",
        "        # convex ICNN petals\n",
        "        self.petals = nn.ModuleList(ICNN(in_dim, hidden) for _ in range(petals))\n",
        "        # convex & bounded gate\n",
        "        self.gate   = ConvexGate(in_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        g      = self.gate(x)                                          # (...,1)\n",
        "        xg     = x * g                                                 # (...,D)\n",
        "        scores = [p(xg).mean(-1, keepdim=True) for p in self.petals]    # list of (...,1)\n",
        "        return torch.logsumexp(torch.cat(scores, dim=-1), dim=-1)      # (...,)\n",
        "\n",
        "class VectorHull(nn.Module):\n",
        "    \"\"\"\n",
        "    Vector-valued convex hull with a bounded convex gate.\n",
        "      x : (..., D)  ->  y : (..., D)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, hidden: List[int], petals: int):\n",
        "        super().__init__()\n",
        "        # convex ICNN petals\n",
        "        self.petals = nn.ModuleList(ICNN(dim, hidden) for _ in range(petals))\n",
        "        # convex & bounded gate\n",
        "        self.gate   = ConvexGate(dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        g    = self.gate(x)                                           # (...,1)\n",
        "        xg   = x * g                                                 # (...,D)\n",
        "        outs = torch.stack([p(xg) for p in self.petals], dim=-1)      # (...,D,P)\n",
        "        return torch.logsumexp(outs, dim=-1)                         # (...,D)\n",
        "\n",
        "class ValueICNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Produce a vector-valued value embedding via a convex ICNN hull.\n",
        "      x : (..., embed_dim) -> v : (..., embed_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim: int, hidden_dims: List[int], petals: int):\n",
        "        super().__init__()\n",
        "        # a VectorHull already returns a vector of same dim\n",
        "        self.vh = VectorHull(embed_dim, hidden_dims, petals)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, S, E)\n",
        "        B, S, E = x.shape\n",
        "        # flatten tokens, run through hull, then restore shape\n",
        "        v = self.vh(x.reshape(-1, E))    # (B*S, E)\n",
        "        return v.view(B, S, E)\n",
        "# --------------- Pairwise Hull Attention -----------------------------\n",
        "# ---------------------------------------------------------------------\n",
        "# Pair‑wise Hull attention • with Rotary Positional Embedding (RoPE) and S4D\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "class LinearPreMix(nn.Module):\n",
        "    \"\"\"\n",
        "    Pure‑attention pre‑projection:\n",
        "        x  : (B,S,E)\n",
        "        qkv: (B,S,3·inner_dim)   with a single Linear\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, inner_dim, heads,N_modes):\n",
        "        super().__init__()\n",
        "        assert inner_dim % heads == 0\n",
        "        self.heads = heads\n",
        "        self.d_k   = inner_dim // heads\n",
        "\n",
        "        self.up   = nn.Linear(embed_dim, inner_dim,  bias=False)   # optional width lift\n",
        "        self.qkv  = nn.Linear(inner_dim, inner_dim * 3, bias=False)\n",
        "\n",
        "    def forward(self, x):               # x:(B,S,E)\n",
        "        z        = self.up(x)           # (B,S,inner_dim)\n",
        "        q, k, v  = self.qkv(z).chunk(3, dim=-1)\n",
        "\n",
        "        B, S, _  = x.shape\n",
        "        new_shape= (B, S, self.heads, self.d_k)\n",
        "\n",
        "        q = q.reshape(new_shape).transpose(1,2)  # (B,H,S,d_k)\n",
        "        k = k.reshape(new_shape).transpose(1,2)\n",
        "        v = v.reshape(new_shape).transpose(1,2)\n",
        "        return q, k, v\n",
        "\n",
        "class ConvexHullMixer(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard multi-head attention mixer:\n",
        "      y[b,h,i,d] = sum_j softmax( (q·k)/√d + bias )[i,j] · val(v)[b,h,j,d]\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k: int, petals=None):\n",
        "        super().__init__()\n",
        "        # we don’t use “petals” any more; just keep a learnable linear for values\n",
        "        self.val = nn.Linear(d_k, d_k, bias=False)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        q: torch.Tensor,            # (B, H, S, d_k)\n",
        "        k: torch.Tensor,            # (B, H, S, d_k)\n",
        "        v: torch.Tensor,            # (B, H, S, d_k)\n",
        "        extra_score: torch.Tensor = None  # (H, S, S) or broadcastable\n",
        "    ) -> torch.Tensor:\n",
        "        B, H, S, D = q.shape\n",
        "\n",
        "        # 1) dot-products\n",
        "        #    (B,H,S,S) = sum over last dim of q * k\n",
        "        scores = torch.einsum('bhid,bhjd->bhij', q, k) / math.sqrt(D)\n",
        "\n",
        "        # 2) add any positional / bias term\n",
        "        if extra_score is not None:\n",
        "            # extra_score: (H,S,S) → (1,H,S,S)\n",
        "            scores = scores + extra_score.unsqueeze(0)\n",
        "\n",
        "        # 3) softmax\n",
        "        attn = F.softmax(scores, dim=-1)  # over j\n",
        "\n",
        "        # 4) linear-project values\n",
        "        v_lin = self.val(v)               # (B,H,S,D)\n",
        "\n",
        "        # 5) weighted sum\n",
        "        y = torch.einsum('bhij,bhjd->bhid', attn, v_lin)  # (B,H,S,D)\n",
        "\n",
        "        return y                                 # (B,H,S,D)\n",
        "\n",
        "\n",
        "class ConvexPositionalBias(nn.Module):\n",
        "    \"\"\"\n",
        "    Bias(i,j) = - w * |i-j|    with   w ≥ 0  (learned per head)\n",
        "    Convex in positional indices; monotone non‑increasing with distance.\n",
        "    \"\"\"\n",
        "    def __init__(self, heads):\n",
        "        super().__init__()\n",
        "        self.w_raw = nn.Parameter(torch.zeros(heads))   # raw parameter\n",
        "    def forward(self, S: int):\n",
        "        device = self.w_raw.device\n",
        "        w = F.softplus(self.w_raw)                      # (H,)\n",
        "        pos  = torch.arange(S, device=device, dtype=torch.float32)\n",
        "        dist = (pos.unsqueeze(0) - pos.unsqueeze(1)).abs()  # (S,S)\n",
        "        bias = - w[:, None, None] * dist                # (H,S,S)\n",
        "        return bias\n",
        "\n",
        "# --- REPLACED PairwiseHullAttention -----------------------------------\n",
        "class PairwiseHullAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, heads, petals,\n",
        "                 inner_dim=128, use_s4d=False):\n",
        "        super().__init__()\n",
        "        assert inner_dim % heads == 0\n",
        "        self.heads = heads\n",
        "        self.d_k   = inner_dim // heads\n",
        "\n",
        "        # q‑k‑v projections (linear or S4D pre‑mix)\n",
        "        self.pre = (S4PreMix if use_s4d else LinearPreMix)(\n",
        "            embed_dim, inner_dim, heads, N_modes=64\n",
        "        )\n",
        "\n",
        "        # convex mixer\n",
        "        self.mixer = ConvexHullMixer(self.d_k, petals)\n",
        "        self.posbias = ConvexPositionalBias(heads)\n",
        "        self.W_O   = nn.Linear(inner_dim, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):                      # x: (B,S,E)\n",
        "        B, S, _ = x.shape\n",
        "        device  = x.device\n",
        "\n",
        "        # 1. q, k, v\n",
        "        q, k, v = self.pre(x)\n",
        "\n",
        "\n",
        "        # 3. convex rope\n",
        "        p_bias = self.posbias(S)    # (H,S,S)\n",
        "\n",
        "        # ConvexHullMixer expects q,k,v; include bias by\n",
        "        # adding it to the scalar hull score inside the mixer:\n",
        "        y = self.mixer(q, k, v, extra_score=p_bias)  # see below\n",
        "\n",
        "        # 4. reshape & output proj\n",
        "        out = y.transpose(1, 2).reshape(B, S, -1)   # (B,S,inner_dim)\n",
        "        return self.W_O(out)\n",
        "\n",
        "# --------------- Convex Residual Block -------------------------------\n",
        "class OmniHullBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convex GPT block:               y = (1‑α₁)·x + α₁·Attn(LN₁(x))\n",
        "                                    z = (1‑α₂)·y + α₂·HFF (LN₂(y))\n",
        "    Both α₁ and α₂ are learned, shared across positions.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, heads, petals,use_s4d):\n",
        "        super().__init__()\n",
        "        self.attn = PairwiseHullAttention(dim, heads, petals,use_s4d=use_s4d)\n",
        "        self.hff  = VectorHull(dim, [dim*2], petals)\n",
        "\n",
        "        self.ln1  = nn.LayerNorm(dim)\n",
        "        self.ln2  = nn.LayerNorm(dim)\n",
        "\n",
        "        # unconstrained scalars → α ∈ (0,1)\n",
        "        self.alpha_raw1 = nn.Parameter(torch.zeros(()))\n",
        "        self.alpha_raw2 = nn.Parameter(torch.zeros(()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _mix(x, gx, alpha_raw):\n",
        "        alpha = F.softplus(alpha_raw) / (1 + F.softplus(alpha_raw))  # sigmoid-softplus\n",
        "        return (1 - alpha) * x + alpha * gx\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self._mix(x, self.attn(self.ln1(x)), self.alpha_raw1)\n",
        "        z = self._mix(y, self.hff (self.ln2(y)), self.alpha_raw2)\n",
        "        return z\n",
        "\n",
        "\n",
        "# --------------- OmniHull GPT ----------------------------------------\n",
        "class BigFatMommaGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, depth, heads, petals):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 1024, embed_dim))  # fixed max length\n",
        "        self.blocks = nn.ModuleList([\n",
        "            OmniHullBlock(embed_dim, heads, petals, use_s4d=(i < depth - 1)) for i in range(depth)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb[:, :S, :]\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        logits = self.head(self.ln_f(x))\n",
        "        return logits\n",
        "#big, correct, and convex everywhere. She's your ideal woman.\n"
      ],
      "metadata": {
        "id": "MLs7qQUOxRPQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class ICNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Input-Convex Neural Network module.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super().__init__()\n",
        "        self.V = nn.Linear(input_dim, hidden_dims[0], bias=True)\n",
        "        self.U = nn.Linear(hidden_dims[0], hidden_dims[0], bias=True)\n",
        "        self.hidden = nn.ModuleList(\n",
        "            nn.Linear(hidden_dims[i], hidden_dims[i+1], bias=True)\n",
        "            for i in range(len(hidden_dims)-1)\n",
        "        )\n",
        "        nn.init.uniform_(self.U.weight, a=0.0, b=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = F.softplus(self.V(x))\n",
        "        z = F.softplus(self.U(z) + z)\n",
        "        for layer in self.hidden:\n",
        "            z = F.softplus(layer(z))\n",
        "        return z.mean(dim=-1, keepdim=True)\n",
        "\n",
        "class HullModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Constructs a convex‐hull via log‐sum‐exp over gated ICNN petals.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, petals, beta=1.0, tanh_kwargs=None):\n",
        "        super().__init__()\n",
        "        # petals: a list of convex ICNN modules\n",
        "        self.petals = nn.ModuleList([ICNN(input_dim, hidden_dims) for _ in range(petals)])\n",
        "        # gate: produces a positive scalar per input, using TighteningTanh\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1, bias=True),\n",
        "            nn.Softplus(),              # clamp ≥ 0\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (..., input_dim)\n",
        "        # 1) compute a positive gate per example\n",
        "        g = self.gate(x)          # (..., 1), ≥0\n",
        "        # 2) scale inputs into each petal\n",
        "        x_scaled = x * g          # broadcasting over last dim\n",
        "        # 3) compute each petal’s score\n",
        "        scores = torch.stack([p(x_scaled) for p in self.petals], dim=-1)  # (..., petals)\n",
        "        # 4) log‐sum‐exp hull\n",
        "        return  torch.logsumexp(scores, dim=-1, keepdim=False)\n",
        "\n",
        "class DotSelfAttentionWithRoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard multi-head dot-product self-attention with RoPE.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim: int, heads: int):\n",
        "        super().__init__()\n",
        "        assert embed_dim % heads == 0, \"embed_dim must be divisible by heads\"\n",
        "        self.h = heads\n",
        "        self.d_k = embed_dim // heads\n",
        "\n",
        "        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        self.W_K = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        self.W_V = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        self.W_O = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _rope(x, sin, cos):\n",
        "        # x: (..., D) where D = 2 * (D/2)\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "        y = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
        "        return y.flatten(-2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_sin_cos(seq_len:int, d_half:int, device:torch.device):\n",
        "        pos  = torch.arange(seq_len, device=device).unsqueeze(1)           # (S,1)\n",
        "        freq = 1. / (10000 ** (torch.arange(0, d_half, 2, device=device) / d_half))  # (D/2,)\n",
        "        ang  = pos * freq                                                  # (S,D/2)\n",
        "        sin, cos = torch.sin(ang), torch.cos(ang)\n",
        "        # shape (1, S, 1, D/2) so they broadcast over batch & heads\n",
        "        return sin[None, :, None, :], cos[None, :, None, :]\n",
        "\n",
        "    def forward(self, x):                         # x: (B, S, E)\n",
        "        B, S, _ = x.shape\n",
        "\n",
        "        # 1) project and split heads → (B, S, H, D)\n",
        "        q = self.W_Q(x).view(B, S, self.h, self.d_k)\n",
        "        k = self.W_K(x).view(B, S, self.h, self.d_k)\n",
        "        v = self.W_V(x).view(B, S, self.h, self.d_k)\n",
        "\n",
        "        # 2) apply RoPE\n",
        "        sin, cos = self._get_sin_cos(S, self.d_k, x.device)\n",
        "        q = self._rope(q, sin, cos)\n",
        "        k = self._rope(k, sin, cos)\n",
        "\n",
        "        # 3) reshape for matmul: (B, H, S, D)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        # 4) scaled dot-product: (B, H, S, S)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # 5) softmax and attention output: (B, H, S, D)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out  = torch.matmul(attn, v)\n",
        "\n",
        "        # 6) merge heads back → (B, S, E) and final projection\n",
        "        out = out.permute(0, 2, 1, 3).reshape(B, S, self.h * self.d_k)\n",
        "        return self.W_O(out)\n",
        "\n",
        "class OmniHullBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, heads, petals):\n",
        "        super().__init__()\n",
        "        self.attn = DotSelfAttentionWithRoPE(embed_dim, heads)\n",
        "        self.hull_mlp = HullModule(embed_dim, [embed_dim * 2, embed_dim], petals)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.hull_mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class SkinnyLittleGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    NanoGPT-style model with recursive convex-hull ICNN modules and RoPE.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, depth, heads, petals):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            OmniHullBlock(embed_dim, heads, petals)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "\n",
        "        x = self.token_emb(idx)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        logits = self.head(self.ln_f(x))\n",
        "        return logits\n",
        "\n",
        "# shes got a flat butt but shes faster"
      ],
      "metadata": {
        "id": "iyGFBw_zivCi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#currently, skinnylittlegpt is in mode collapse.\n",
        "#maybe her butt just isnt big enough. maybe she isnt done being baked yet.\n",
        "#shes not really convex, but, she has some convexity.\n",
        "#bigfatmommy is very very convex. in many ways. rounded. Rounded lumps.\n",
        "#perfectly so? no. But there are tradeoffs involved.\n",
        "#mommy also collapses almost immediately, outputting nothing but spaces.\n",
        "#am i pushing her too hard?"
      ],
      "metadata": {
        "id": "7VmSqESFnENJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.optimizer import Optimizer\n",
        "device    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "@torch.jit.script\n",
        "def wolf_update(p: torch.Tensor,\n",
        "                g: torch.Tensor,\n",
        "                state_p: torch.Tensor,\n",
        "                lr: float):\n",
        "    # define your constants here instead of capturing them\n",
        "    etcerta: float = 0.367879441\n",
        "    et:      float = 1.0 - etcerta\n",
        "\n",
        "    # same logic as before\n",
        "    update    = state_p * et + g * etcerta\n",
        "    new_state = state_p * et + update * etcerta\n",
        "    sign_agree = torch.sign(update) * torch.sign(g)\n",
        "    update    = update + (torch.rand_like(update)*2 - 1) * etcerta * update\n",
        "    p_new     = torch.where(sign_agree > 0, p - lr * update, p)\n",
        "    return p_new, new_state\n",
        "\n",
        "class Wolf(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3):\n",
        "        defaults = dict(lr=lr)\n",
        "        super().__init__(params, defaults)\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                self.state[p]['p'] = torch.zeros_like(p)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = closure() if closure is not None else None\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state_p = self.state[p]['p']\n",
        "                p_new, new_state = wolf_update(p.data, p.grad, state_p, lr)\n",
        "                p.data.copy_(p_new)\n",
        "                state_p.copy_(new_state)\n",
        "        return loss\n",
        "\n",
        "# 1) Load data and meta as before\n",
        "data_dir  = os.path.dirname(base_dir)\n",
        "train_ids = np.fromfile(os.path.join(data_dir, 'train.bin'), dtype=np.uint16)\n",
        "val_ids   = np.fromfile(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16)\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# 2) Compute data‐marginal q[v]\n",
        "counts = np.bincount(train_ids, minlength=vocab_size).astype(float)\n",
        "q = torch.tensor(counts / counts.sum(), dtype=torch.float32, device=device)  # [V]\n",
        "\n",
        "# 3) Dataset + DataLoader\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = torch.from_numpy(data).long()\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx : idx + self.block_size]\n",
        "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "block_size = 128\n",
        "train_loader = DataLoader(CharDataset(train_ids, block_size),\n",
        "                          batch_size=8, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(CharDataset(val_ids,   block_size),\n",
        "                          batch_size=8, shuffle=False, drop_last=True)\n",
        "\n",
        "# 4) Model, optimizer, loss\n",
        "virgin = BigFatMommaGPT(\n",
        "        vocab_size = vocab_size,\n",
        "        embed_dim  = 384,\n",
        "        depth      = 4,\n",
        "        heads      = 4,\n",
        "        petals     = 2\n",
        ")\n",
        "\n",
        "# (Optional) Re‑initialise *only* the PositiveLinear layers:\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in virgin.parameters()))\n",
        "model = torch.jit.script(virgin)\n",
        "model.to(device)\n",
        "optimizer = Wolf(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "λ_ent         = 0.5    # encourage weight diversification\n",
        "λ_kl          = 0.5     # discourage mode collapse\n",
        "\n",
        "# 6) Train / eval functions\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # Forward\n",
        "        logits = model(xb)                 # (B, T, V)\n",
        "        B, T, V = logits.shape\n",
        "        p = F.softmax(logits, dim=-1)      # (B, T, V)\n",
        "\n",
        "        # 1) Standard CE\n",
        "        ce_loss = criterion(logits.view(B*T, V),\n",
        "                            yb.view(B*T))\n",
        "\n",
        "        # 2) Entropy penalty\n",
        "        ent = -(p * (p + 1e-12).log()).sum(dim=-1)  # (B, T)\n",
        "        ent_loss = ent.mean()\n",
        "\n",
        "        p_m = p.mean(dim=(0,1))            # [V]\n",
        "        kl_loss = (p_m * (p_m + 1e-12).log() - p_m * q.log()).sum()\n",
        "\n",
        "        # 4) Combined loss\n",
        "        loss = ce_loss + λ_ent * ent_loss + λ_kl * kl_loss\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item())\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        total_loss += criterion(logits.view(B*T,V),\n",
        "                                yb.view(B*T)).item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# 7) Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = eval_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "pefLpevkNhPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01133cbb-ef23-496c-d309-fce265fdc5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters:  5799839\n",
            "7.089759349822998\n",
            "7.059626579284668\n",
            "7.024545192718506\n",
            "7.020734786987305\n",
            "6.986845016479492\n",
            "6.977483749389648\n",
            "6.955672740936279\n",
            "6.915209770202637\n",
            "6.830282211303711\n",
            "6.808751106262207\n",
            "6.788784027099609\n",
            "6.764035701751709\n",
            "6.756332874298096\n",
            "6.671619415283203\n",
            "6.662999629974365\n",
            "6.598953723907471\n",
            "6.588779926300049\n",
            "6.520735263824463\n",
            "6.512465953826904\n",
            "6.502041339874268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Wolf(model.parameters(), lr=0.5)#adam would explode at this training rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "λ_ent         = 0.5    # encourage weight diversification\n",
        "λ_kl          = 0.5     # discourage mode collapse\n",
        "\n",
        "# 6) Train / eval functions\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # Forward\n",
        "        logits = model(xb)                 # (B, T, V)\n",
        "        B, T, V = logits.shape\n",
        "        p = F.softmax(logits, dim=-1)      # (B, T, V)\n",
        "\n",
        "        # 1) Standard CE\n",
        "        ce_loss = criterion(logits.view(B*T, V),\n",
        "                            yb.view(B*T))\n",
        "\n",
        "        # 2) Entropy penalty\n",
        "        ent = -(p * (p + 1e-12).log()).sum(dim=-1)  # (B, T)\n",
        "        ent_loss = ent.mean()\n",
        "\n",
        "        p_m = p.mean(dim=(0,1))            # [V]\n",
        "        kl_loss = (p_m * (p_m + 1e-12).log() - p_m * q.log()).sum()\n",
        "\n",
        "        # 4) Combined loss\n",
        "        loss = ce_loss + λ_ent * ent_loss + λ_kl * kl_loss\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item())\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        total_loss += criterion(logits.view(B*T,V),\n",
        "                                yb.view(B*T)).item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# 7) Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = eval_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "vdbMQgrinb_8",
        "outputId": "1338ee9e-4398-43aa-e0fb-71edce9a5265"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14970003068447113\n",
            "0.10967803746461868\n",
            "0.116947703063488\n",
            "0.1101749986410141\n",
            "0.10924039781093597\n",
            "0.1043446809053421\n",
            "0.11175236850976944\n",
            "0.09217088669538498\n",
            "0.09896231442689896\n",
            "0.09130249172449112\n",
            "0.09938418865203857\n",
            "0.09552960842847824\n",
            "0.09382826834917068\n",
            "0.10704723000526428\n",
            "0.12667648494243622\n",
            "0.08107704669237137\n",
            "0.08900757879018784\n",
            "0.10800237953662872\n",
            "0.08867326378822327\n",
            "0.10503402352333069\n",
            "0.09006978571414948\n",
            "0.09783803671598434\n",
            "0.10546399652957916\n",
            "0.1254078894853592\n",
            "0.0985318124294281\n",
            "0.09559228271245956\n",
            "0.08799614757299423\n",
            "0.07349701970815659\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f3dabea853b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mval_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f3dabea853b7>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- helpers ---------------------------------------------------------\n",
        "def fenchel_decode(logits, tau=1.0, iters=3):\n",
        "    \"\"\"Fenchel‑dual KL‑regularised projection of -logits (energy).\"\"\"\n",
        "    energy = -logits                        # (B,V)\n",
        "    p = torch.full_like(energy, 1.0 / energy.size(-1))  # uniform start\n",
        "    for _ in range(iters):\n",
        "        p = torch.softmax((-energy / tau) + p.log(), dim=-1)\n",
        "    return p\n",
        "\n",
        "# --- generation ------------------------------------------------------\n",
        "use_fenchel   = False          # flip to compare\n",
        "tau           = 1.0           # λ  (temperature analogue)\n",
        "max_new_tokens = 200\n",
        "top_k          = 25\n",
        "block_size     = 128\n",
        "temperature    = 1.0\n",
        "\n",
        "bcontext_str = \"To be, or not to be,\"\n",
        "context_ids = torch.tensor([[ stoi[c] for c in bcontext_str ]],\n",
        "                           dtype=torch.long)\n",
        "context_ids = context_ids.to(device)\n",
        "\n",
        "generated = context_ids.clone()  # (1,T0)\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    input_ids = generated[:, -block_size:]        # casual block\n",
        "    logits = model(input_ids)                     # (1,cur_T,V)\n",
        "    logits = logits[:, -1, :] / temperature       # (1,V)\n",
        "\n",
        "    # top‑k mask\n",
        "    if top_k is not None:\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        logits[logits < v[:, [-1]]] = -1e10\n",
        "\n",
        "    if use_fenchel:\n",
        "        probs = fenchel_decode(logits, tau=tau, iters=3)\n",
        "    else:\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    next_id = torch.multinomial(probs, num_samples=1)   # (1,1)\n",
        "    generated = torch.cat([generated, next_id], dim=1)\n",
        "\n",
        "print('> ', ''.join(itos[i] for i in generated[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9GiSvYAnJnv",
        "outputId": "e0ed1fce-7ec3-4d9c-d2a6-62138912ef62"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">  To be, or not to be,We,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, d,,                                          \n"
          ]
        }
      ]
    }
  ]
}