{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/PyITD/blob/main/Artificial_General_Intelligence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p4-dcQlo_qoG"
      },
      "outputs": [],
      "source": [
        "#Ashem vohu vahishtem asti ushta asti ushta ahmai yad ashai vahishtai ashem\n",
        "#copyright joshuah.rainstar@gmail.com 2025\n",
        "#some concepts borrowed from various papers.\n",
        "#add_hypersphere_phase_heads by me.\n",
        "#use my code and my ideas- but if you have money, I want some. I need a new car and a house.\n",
        "\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "\n",
        "def add_hypersphere_phase_heads(x: torch.Tensor, num_segs: int, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x: [B,T,C]\n",
        "    num_segs: number of segments to divide C dimension into\n",
        "    First segment gets no phase info, subsequent segments get lag 1, 2, 3, etc.\n",
        "    \"\"\"\n",
        "    B, T, C = x.shape\n",
        "    if T == 0 or num_segs <= 0:\n",
        "        return x\n",
        "\n",
        "    assert C % num_segs == 0, f\"C ({C}) must be divisible by num_segs ({num_segs})\"\n",
        "    seg_size = C // num_segs\n",
        "\n",
        "    # Reshape to treat segments as separate heads: [B,T,C] -> [B,num_segs,T,seg_size]\n",
        "    x_reshaped = x.view(B, T, num_segs, seg_size).transpose(1, 2)  # [B,num_segs,T,seg_size]\n",
        "\n",
        "    # Skip first segment, process segments 1 onwards with lags 1, 2, 3, ...\n",
        "    if num_segs > 1:\n",
        "        # Create lags: [1, 2, 3, ..., num_segs-1] for segments 1, 2, 3, ..., num_segs-1\n",
        "        L_h = torch.arange(1, num_segs, device=x.device, dtype=torch.long)  # [num_segs-1]\n",
        "\n",
        "        # Process only segments 1 onwards\n",
        "        x_to_process = x_reshaped[:, 1:]  # [B, num_segs-1, T, seg_size]\n",
        "\n",
        "        v = F.normalize(x_to_process, dim=-1, eps=eps)  # [B,num_segs-1,T,seg_size]\n",
        "\n",
        "        t = torch.arange(T, device=x.device)  # [T]\n",
        "        src = (t.unsqueeze(0) - L_h.view(-1, 1)).clamp_min(0)  # [num_segs-1,T]\n",
        "\n",
        "        # gather anchors per segment and time\n",
        "        anchor = v.gather(\n",
        "            dim=2,\n",
        "            index=src.view(1, num_segs-1, T, 1).expand(B, num_segs-1, T, seg_size)\n",
        "        )\n",
        "        cosA = (v * anchor).sum(dim=-1).clamp(-1.0 + eps, 1.0 - eps)  # [B,num_segs-1,T]\n",
        "        scalar = (cosA / max(seg_size, 1)).unsqueeze(-1)  # [B,num_segs-1,T,1]\n",
        "\n",
        "        # Add scalar to processed segments\n",
        "        x_modified = x_to_process + scalar  # [B,num_segs-1,T,seg_size]\n",
        "\n",
        "        # Combine unchanged first segment with modified segments\n",
        "        x_reshaped = torch.cat([x_reshaped[:, :1], x_modified], dim=1)  # [B,num_segs,T,seg_size]\n",
        "\n",
        "    # Reshape back to original format: [B,num_segs,T,seg_size] -> [B,T,C]\n",
        "    return x_reshaped.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "\n",
        "class KalmanSweepMHGainsOptimal(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimal form with:\n",
        "    - Sink mechanism via adaptive R\n",
        "    - Shared V: first chunk broadcast to all heads\n",
        "    - Fused operations for efficiency\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg, n_passes: int = 12, init_logQ: float = -2.0, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.n_head = cfg.n_head\n",
        "        self.dh = cfg.n_embd // cfg.n_head\n",
        "        self.n_passes = n_passes\n",
        "        self.eps = eps\n",
        "\n",
        "        # Single fused projection for all operations\n",
        "        # Now outputs 5 components: [H_diag, y, R_diag, A_modulation, sink_gate]\n",
        "        self.fused_proj = nn.Linear(3 * self.dh, 5 * self.dh)\n",
        "\n",
        "        # Base transition (can be modulated)\n",
        "        self.A_base = nn.Parameter(torch.eye(self.dh))\n",
        "\n",
        "        # Process noise\n",
        "        self.logQ = nn.Parameter(torch.full((self.n_head, self.dh), init_logQ))\n",
        "\n",
        "        # Scales\n",
        "        self.scales = nn.Parameter(torch.ones(3, self.n_head))  # H_scale, R_scale, sink_scale\n",
        "\n",
        "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, D = Q.shape\n",
        "        H, dh = self.n_head, self.dh\n",
        "\n",
        "        # Extract first chunk of V and broadcast to all heads\n",
        "        V_reshaped = V.view(B, N, H, dh)  # [B, N, H, dh]\n",
        "        V_shared = V_reshaped[:, :, 0:1, :].expand(B, N, H, dh)  # Broadcast first head to all\n",
        "        V_broadcast = V_shared.contiguous().view(B, N, D)  # [B, N, D]\n",
        "\n",
        "        # Use original Q, K but shared V for projection\n",
        "        QKV = torch.cat([Q, K, V_broadcast], dim=-1).view(B * N * H, 3 * dh)\n",
        "\n",
        "        # Fused projection and activation\n",
        "        out = self.fused_proj(QKV).view(B, N, H, 5 * dh)\n",
        "        H_raw, y, R_raw, A_mod, sink_raw = out.chunk(5, dim=-1)\n",
        "\n",
        "        # Apply activations and scales\n",
        "        H_diag = torch.sigmoid(H_raw) * self.scales[0].view(1, 1, H, 1)\n",
        "\n",
        "        # Sink mechanism: when sink_gate → 0, R → ∞ (ignore input)\n",
        "        sink_gate = torch.sigmoid(sink_raw) * self.scales[2].view(1, 1, H, 1)\n",
        "        R_base = F.softplus(R_raw) * self.scales[1].view(1, 1, H, 1) + self.eps\n",
        "        R_diag = R_base / (sink_gate + 0.01)  # When sink_gate→0, R becomes very large\n",
        "\n",
        "        # Modulated transition matrix\n",
        "        A_mod_sigmoid = torch.sigmoid(A_mod).view(B * N * H, dh, 1)\n",
        "        A = self.A_base.unsqueeze(0) * A_mod_sigmoid  # (B*N*H, dh, dh)\n",
        "\n",
        "        # Pre-compute constants\n",
        "        Q_diag = torch.exp(self.logQ).clamp(min=self.eps).view(1, 1, H, dh)\n",
        "\n",
        "        # Initialize with cold start\n",
        "        P = torch.ones(B, N, H, dh, device=Q.device)\n",
        "        HP = H_diag * P\n",
        "        S = HP * H_diag + R_diag\n",
        "        K_gain = HP / S\n",
        "\n",
        "        if self.n_passes == 1:\n",
        "            return K_gain.reshape(B, N, D)\n",
        "\n",
        "        # State for multi-pass\n",
        "        x = K_gain * y\n",
        "        P = P - K_gain * HP\n",
        "\n",
        "        # Parallel passes\n",
        "        for pass_idx in range(1, self.n_passes):\n",
        "            # Shift and transform in one operation\n",
        "            x_flat = x[:, :-1].reshape(B * (N-1) * H, dh)\n",
        "            x_pred_flat = torch.bmm(\n",
        "                A[:B*(N-1)*H].view(B*(N-1)*H, dh, dh),\n",
        "                x_flat.unsqueeze(-1)\n",
        "            ).squeeze(-1).view(B, N-1, H, dh)\n",
        "\n",
        "            x_prev = torch.cat([\n",
        "                torch.zeros(B, 1, H, dh, device=x.device),\n",
        "                x_pred_flat\n",
        "            ], dim=1)\n",
        "\n",
        "            P_prev = torch.cat([\n",
        "                torch.ones(B, 1, H, dh, device=P.device),\n",
        "                P[:, :-1] + Q_diag\n",
        "            ], dim=1)\n",
        "\n",
        "            # Parallel Kalman update\n",
        "            HP = H_diag * P_prev\n",
        "            S = HP * H_diag + R_diag\n",
        "            K_gain = HP / S\n",
        "\n",
        "            innov = y - H_diag * x_prev\n",
        "            x = x_prev + K_gain * innov\n",
        "            P = P_prev - K_gain * HP\n",
        "\n",
        "        return K_gain.reshape(B, N, D)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------- DynMix --\n",
        "class DynMix(nn.Module):\n",
        "    \"\"\"\n",
        "    Symplectic mixer for ≥3 tensors (B,T,E).  See original author for details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, step: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.h = float(step)\n",
        "\n",
        "    @staticmethod\n",
        "    def _coop(R: torch.Tensor, C: torch.Tensor, h: float) -> torch.Tensor:\n",
        "        w = torch.sigmoid((R * C).sum(dim=-1, keepdim=True) / (2 * R.size(-1) ** 0.5))\n",
        "        k1 = w * (C - R)\n",
        "        k2 = w * (C - (R + h * k1))\n",
        "        return R + 0.5 * h * (k1 + k2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _mix_list(xs: List[torch.Tensor], h: float) -> List[torch.Tensor]:\n",
        "        n = len(xs)\n",
        "        if n < 3:\n",
        "            raise ValueError(\"Need at least 3 components\")\n",
        "        stacked = torch.stack(xs, 0)\n",
        "        total = stacked.sum(0, keepdim=False)\n",
        "        out = []\n",
        "        for i in range(n):\n",
        "            others_mean = (total - stacked[i]) / (n - 1)\n",
        "            out.append(DynMix._coop(stacked[i], others_mean, h))\n",
        "        return out\n",
        "\n",
        "    def forward(self, comps: List[torch.Tensor], loop_iters: int = 2) -> List[torch.Tensor]:\n",
        "        for _ in range(loop_iters):\n",
        "            comps = DynMix._mix_list(comps, self.h)\n",
        "        return comps\n",
        "\n",
        "\n",
        "\n",
        "class CausalKalman(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.cal = KalmanSweepMHGainsOptimal(cfg=config)\n",
        "        self.dynmix = DynMix()\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # QKV projection\n",
        "        qkv = self.c_attn(x)  # [B, T, 3*C]\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        s = [q,k,v]\n",
        "        s = self.dynmix(s) #apply mixing\n",
        "        q, k, v = s\n",
        "        s = self.cal(q,k,v)\n",
        "\n",
        "        return x*s\n",
        "\n",
        "\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"RoPE implementation for better positional encoding\"\"\"\n",
        "    def __init__(self, dim, max_seq_len=8192, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.base = base\n",
        "\n",
        "        # Precompute frequencies\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, q, k):\n",
        "        seq_len = q.size(-2)\n",
        "        device = q.device\n",
        "\n",
        "        # Generate position indices\n",
        "        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.outer(t, self.inv_freq)  # [seq_len, dim//2]\n",
        "\n",
        "        # Create rotation matrix\n",
        "        cos = freqs.cos().to(q.dtype)\n",
        "        sin = freqs.sin().to(q.dtype)\n",
        "\n",
        "        # Apply rotation\n",
        "        q_rot = self._apply_rope(q, cos, sin)\n",
        "        k_rot = self._apply_rope(k, cos, sin)\n",
        "\n",
        "        return q_rot, k_rot\n",
        "\n",
        "    def _apply_rope(self, x, cos, sin):\n",
        "        # x: [B, H, T, head_dim]\n",
        "        # cos, sin: [T, head_dim//2]\n",
        "\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]  # Split even/odd dimensions\n",
        "        cos = cos.view(1, 1, cos.size(0), cos.size(1))\n",
        "        sin = sin.view(1, 1, sin.size(0), sin.size(1))\n",
        "\n",
        "        # Apply rotation\n",
        "        rotated = torch.stack([\n",
        "            x1 * cos - x2 * sin,\n",
        "            x1 * sin + x2 * cos\n",
        "        ], dim=-1).flatten(-2)\n",
        "\n",
        "        return rotated\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "        # Sink tokens - learnable parameters for attention stabilization\n",
        "        self.sinks = nn.Parameter(torch.randn(config.n_head) * 0.02)\n",
        "\n",
        "        # Use flash attention when available, but handle sinks manually\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "\n",
        "        # RoPE embeddings (optional improvement)\n",
        "        self.use_rope = True\n",
        "        if self.use_rope:\n",
        "            self.rope = RotaryPositionalEmbedding(self.head_dim)\n",
        "\n",
        "        # For non-flash attention fallback\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                               .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def _create_causal_mask(self, T, device, dtype):\n",
        "        \"\"\"Create causal mask avoiding -inf to prevent NaNs\"\"\"\n",
        "        mask = torch.tril(torch.ones(T, T, device=device, dtype=dtype))\n",
        "        # Use large negative value instead of -inf to avoid NaN issues\n",
        "        large_neg = torch.finfo(dtype).min / 2\n",
        "        return mask.masked_fill(mask == 0, large_neg)\n",
        "\n",
        "    def _sink_stabilized_softmax(self, attn, sinks, dim=-1):\n",
        "        \"\"\"Softmax with sink stabilization to prevent attention collapse\"\"\"\n",
        "        # attn: [B, H, T, T]\n",
        "        # sinks: [H] -> [1, H, 1, 1]\n",
        "\n",
        "        max_logits = torch.amax(attn, dim=dim, keepdim=True)  # [B, H, T, 1]\n",
        "        sinks_expanded = sinks.view(1, -1, 1, 1)  # [1, H, 1, 1]\n",
        "\n",
        "        # Stabilizer is max of attention logits and sink values\n",
        "        stabilizer = torch.maximum(max_logits, sinks_expanded)\n",
        "\n",
        "        # Compute exp scores\n",
        "        exp_attn = torch.exp(attn - stabilizer)\n",
        "        exp_sinks = torch.exp(sinks_expanded - stabilizer)\n",
        "\n",
        "        # Normalize with sink contribution\n",
        "        normalizer = exp_attn.sum(dim=dim, keepdim=True) + exp_sinks\n",
        "        probs = exp_attn / normalizer\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # QKV projection\n",
        "        qkv = self.c_attn(x)  # [B, T, 3*C]\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B, H, T, head_dim]\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B, H, T, head_dim]\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B, H, T, head_dim]\n",
        "\n",
        "        # Apply RoPE if enabled\n",
        "        q, k = self.rope(q, k)\n",
        "\n",
        "        # Attention computation\n",
        "        if self.flash and not self.training:  # Use flash attention for inference only\n",
        "            # For flash attention, we need to handle sinks differently\n",
        "            # This is a simplified approach - you might need custom CUDA kernels for full optimization\n",
        "            y = F.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=0.0,  # Flash attention handles dropout internally\n",
        "                is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            # Manual attention with sink stabilization\n",
        "            attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, H, T, T]\n",
        "\n",
        "            # Apply causal mask (avoiding -inf)\n",
        "            causal_mask = self._create_causal_mask(T, x.device, attn.dtype)\n",
        "            attn = attn + causal_mask.view(1, 1, T, T)\n",
        "\n",
        "            # Apply sink-stabilized softmax\n",
        "            probs = self._sink_stabilized_softmax(attn, self.sinks)\n",
        "            probs = self.attn_dropout(probs)\n",
        "\n",
        "            # Apply attention to values\n",
        "            y = torch.matmul(probs, v)  # [B, H, T, head_dim]\n",
        "\n",
        "        # Reshape back to original format\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)  # Replace MLP with MMMLP\n",
        "        self.n_head = config.n_head\n",
        "        # output projection\n",
        "    def forward(self, x):\n",
        "        x1 = self.ln_1(x)\n",
        "        x1 = add_hypersphere_phase_heads(x1, self.n_head)\n",
        "        x1 = self.attn(x1)\n",
        "        x = x + x1\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class BlockKal(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.kal = CausalKalman(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)  # Replace MLP with MMMLP\n",
        "        self.n_head = config.n_head\n",
        "        # output projection\n",
        "    def forward(self, x):\n",
        "        x1 = self.ln_1(x)\n",
        "        x1 = x + self.kal(x1)\n",
        "        x1 = self.ln_1(x)\n",
        "        x = x + x1\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class ExplorerEngineerStage(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.explorer= BlockKal(config)   # H-module\n",
        "        self.engineer = Block(config)   # H-module\n",
        "        self.n_head = config.n_head\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        residual: torch.Tensor,\n",
        "    ):\n",
        "\n",
        "        landmarks = self.explorer(x)\n",
        "        mapping = self.engineer(landmarks)\n",
        "        return mapping\n",
        "\n",
        "# ---------------- PATHFINDER (end-to-end) ----------------\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 2\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.ModuleList([nn.Embedding(config.vocab_size, config.n_embd//config.n_head) for _ in range(config.n_head)]),\n",
        "\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([ExplorerEngineerStage(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "        # forward the GPT model itself\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        embeddings = torch.cat([self.transformer.wte[i](idx)  for i in range(self.n_head)], dim=-1)\n",
        "        embeddings = embeddings + pos_emb\n",
        "        x = self.transformer.drop(embeddings)\n",
        "        x_orig = x.clone()\n",
        "        for stage in self.transformer.h:  # stages are ExplorerEngineerStage\n",
        "            x = stage(x, x_orig)\n",
        "\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "\n",
        "            # Total loss = main loss + weighted load balance loss\n",
        "\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFGVJvlN_yfW",
        "outputId": "0346e42e-e768-4782-82c8-fa9f7f940295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Downloading aochildes.txt...\n",
            "📥 Downloading cbt.txt...\n",
            "📥 Downloading children_stories.txt...\n",
            "📥 Downloading gutenberg.txt...\n",
            "📥 Downloading qed.txt...\n",
            "📥 Downloading simple_wikipedia.txt...\n",
            "📥 Downloading switchboard.txt...\n",
            "📥 Downloading wikipedia.txt...\n",
            "📥 Downloading shakespeare.txt...\n",
            "✅ Done. Files saved to ./babylm_10m_cleaned\n"
          ]
        }
      ],
      "source": [
        "import requests, os\n",
        "\n",
        "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
        "target_dir = \"./babylm_10m_cleaned\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "file_names = [\n",
        "    \"aochildes.txt\",\n",
        "    \"cbt.txt\",\n",
        "    \"children_stories.txt\",\n",
        "    \"gutenberg.txt\",\n",
        "    \"qed.txt\",\n",
        "    \"simple_wikipedia.txt\",\n",
        "    \"switchboard.txt\",\n",
        "    \"wikipedia.txt\"\n",
        "]\n",
        "\n",
        "# Optional addition: Shakespeare from another dataset\n",
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
        "shakespeare_fname = \"shakespeare.txt\"\n",
        "\n",
        "# Combined download logic\n",
        "all_files = [(base_url + fname, fname) for fname in file_names]\n",
        "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
        "\n",
        "\n",
        "# Download loop\n",
        "for url, fname in all_files:\n",
        "    out_path = os.path.join(target_dir, fname)\n",
        "    print(f\"📥 Downloading {fname}...\")\n",
        "    resp = requests.get(url)\n",
        "    if resp.status_code == 200:\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(resp.text)\n",
        "    else:\n",
        "        print(f\"❌ Failed to download {fname} ({resp.status_code})\")\n",
        "\n",
        "print(f\"✅ Done. Files saved to {target_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0fFuL2a_sAF",
        "outputId": "6f4308d5-6efd-4dcc-8c17-55433a23e968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Char tokenizer finalized.\n",
            "🧾 Train tokens: 1016242 | Val tokens: 99152\n",
            "🔤 Vocab size: 66\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# === Paths ===\n",
        "source_dir = \"./babylm_10m_cleaned\"\n",
        "out_dir    = \"./babylm_char_tokenized\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "file_names = [\n",
        "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
        "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\",\"shakespeare.txt\"\n",
        "]\n",
        "\n",
        "# === Load and split ===\n",
        "train_texts, val_texts = [], []\n",
        "char_set = set()\n",
        "\n",
        "for fname in file_names:\n",
        "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        n = len(lines)\n",
        "        split = int(0.9 * n)\n",
        "        train_part = \"\".join(lines[:split])\n",
        "        val_part   = \"\".join(lines[split:])\n",
        "        train_texts.append(train_part)\n",
        "        val_texts.append(val_part)\n",
        "        char_set.update(train_part)\n",
        "        char_set.update(val_part)\n",
        "\n",
        "full_train = \"\\n\".join(train_texts)\n",
        "full_val   = \"\\n\".join(val_texts)\n",
        "\n",
        "# === Final vocab ===\n",
        "char_set = sorted(set(char_set))\n",
        "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "# === Encode function ===\n",
        "def encode(text):\n",
        "    return [stoi.get(c, 0) for c in text]\n",
        "\n",
        "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
        "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
        "\n",
        "# === Save ===\n",
        "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
        "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
        "\n",
        "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"vocab_size\": len(stoi),\n",
        "        \"stoi\": stoi,\n",
        "        \"itos\": itos\n",
        "    }, f)\n",
        "\n",
        "print(f\"✅ Char tokenizer finalized.\")\n",
        "print(f\"🧾 Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
        "print(f\"🔤 Vocab size: {len(stoi)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42l_Fa8_v9z",
        "outputId": "d7216052-de4a-442a-b53a-711a27549412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 6.11M\n",
            "6369344\n",
            "4.229323863983154\n",
            "3.587635040283203\n",
            "3.2607264518737793\n",
            "3.160344123840332\n",
            "2.9825143814086914\n",
            "2.8905162811279297\n",
            "2.7934513092041016\n",
            "2.821227550506592\n",
            "2.7585995197296143\n",
            "2.7133898735046387\n",
            "2.6475701332092285\n",
            "2.6605639457702637\n",
            "2.626351833343506\n",
            "2.6464946269989014\n",
            "2.5896246433258057\n",
            "2.6607542037963867\n",
            "2.5939559936523438\n",
            "2.5683486461639404\n",
            "2.583636999130249\n",
            "2.5819802284240723\n",
            "2.598752975463867\n",
            "2.6027097702026367\n",
            "2.524176597595215\n",
            "2.557281494140625\n",
            "2.5649971961975098\n",
            "2.574275255203247\n",
            "2.550318956375122\n",
            "2.5256781578063965\n",
            "2.541027545928955\n",
            "2.520171880722046\n",
            "2.5418219566345215\n",
            "2.5121800899505615\n",
            "2.4932773113250732\n",
            "2.5091464519500732\n",
            "2.5037894248962402\n",
            "2.5038654804229736\n",
            "2.490067481994629\n",
            "2.453451633453369\n",
            "2.451658010482788\n",
            "2.4378390312194824\n",
            "2.391697406768799\n",
            "2.4202349185943604\n",
            "2.402888536453247\n",
            "2.3991470336914062\n",
            "2.3965632915496826\n",
            "2.421313762664795\n",
            "2.3997669219970703\n",
            "2.3691632747650146\n",
            "2.3574719429016113\n",
            "2.3477115631103516\n",
            "2.337822675704956\n",
            "2.3605308532714844\n",
            "2.347334146499634\n",
            "2.296182632446289\n",
            "2.323864698410034\n",
            "2.3160147666931152\n",
            "2.3475377559661865\n",
            "2.358111619949341\n",
            "2.287210702896118\n",
            "2.293682336807251\n",
            "2.3282175064086914\n",
            "2.3079617023468018\n",
            "2.322084665298462\n",
            "2.2982046604156494\n",
            "2.300416946411133\n",
            "2.284475564956665\n",
            "2.2833149433135986\n",
            "2.2948672771453857\n",
            "2.2731709480285645\n",
            "2.2949421405792236\n",
            "2.2904062271118164\n",
            "2.284698724746704\n",
            "2.285264253616333\n",
            "2.2290468215942383\n",
            "2.228583574295044\n",
            "2.2328927516937256\n",
            "2.224152088165283\n",
            "2.2222256660461426\n",
            "2.2441396713256836\n",
            "2.266740560531616\n",
            "2.2540369033813477\n",
            "2.2792036533355713\n",
            "2.210421562194824\n",
            "2.207019805908203\n",
            "2.2316861152648926\n",
            "2.266660213470459\n",
            "2.265002489089966\n",
            "2.1815640926361084\n",
            "2.213115930557251\n",
            "2.22111439704895\n",
            "2.2072904109954834\n",
            "2.172983169555664\n",
            "2.1854288578033447\n",
            "2.2013039588928223\n",
            "2.1352405548095703\n",
            "2.204119920730591\n",
            "2.205451488494873\n",
            "2.1945486068725586\n",
            "2.1797585487365723\n",
            "2.207019805908203\n",
            "2.1272616386413574\n",
            "2.158815622329712\n",
            "2.1819522380828857\n",
            "2.1573307514190674\n",
            "2.152613878250122\n",
            "2.151482105255127\n",
            "2.162433385848999\n",
            "2.1467039585113525\n",
            "2.1122610569000244\n",
            "2.125347852706909\n",
            "2.110297679901123\n",
            "2.1443533897399902\n",
            "2.080756187438965\n",
            "2.1137406826019287\n",
            "2.1440742015838623\n",
            "2.1462132930755615\n",
            "2.1005382537841797\n",
            "2.0800673961639404\n",
            "2.1095077991485596\n",
            "2.04723858833313\n",
            "2.116401195526123\n",
            "2.1111342906951904\n",
            "2.092489719390869\n",
            "2.073943614959717\n",
            "2.0428361892700195\n",
            "2.0752813816070557\n",
            "2.1035706996917725\n",
            "2.1055967807769775\n",
            "2.067955732345581\n",
            "2.060413122177124\n",
            "2.0391414165496826\n",
            "2.0382297039031982\n",
            "2.035533905029297\n",
            "2.0488016605377197\n",
            "2.0060935020446777\n",
            "2.0039281845092773\n",
            "2.0396738052368164\n",
            "2.0777571201324463\n",
            "2.0426158905029297\n",
            "2.07241153717041\n",
            "2.110326051712036\n",
            "2.0630650520324707\n",
            "2.056401014328003\n",
            "2.057640790939331\n",
            "2.048917055130005\n",
            "2.023965358734131\n",
            "2.0019478797912598\n",
            "2.043578863143921\n",
            "1.9402865171432495\n",
            "2.007331609725952\n",
            "2.0512964725494385\n",
            "1.998187780380249\n",
            "1.9277541637420654\n",
            "2.024364709854126\n",
            "1.996128797531128\n",
            "1.9840413331985474\n",
            "1.99332857131958\n",
            "1.965153455734253\n",
            "1.9329051971435547\n",
            "1.9694504737854004\n",
            "1.9404096603393555\n",
            "1.922093391418457\n",
            "1.9251035451889038\n",
            "1.9795153141021729\n",
            "2.0173251628875732\n",
            "1.9163898229599\n",
            "1.929443120956421\n",
            "1.9346915483474731\n",
            "1.9455151557922363\n",
            "1.9510823488235474\n",
            "1.9392353296279907\n",
            "1.9563782215118408\n",
            "1.9424221515655518\n",
            "1.9188027381896973\n",
            "1.8936342000961304\n",
            "1.9643042087554932\n",
            "1.9263944625854492\n",
            "1.9001377820968628\n",
            "1.9003593921661377\n",
            "1.9302021265029907\n",
            "1.84987473487854\n",
            "1.928022861480713\n",
            "1.8839130401611328\n",
            "1.902889370918274\n",
            "1.9103128910064697\n",
            "1.8589750528335571\n",
            "1.9016820192337036\n",
            "1.8613736629486084\n",
            "1.8774782419204712\n",
            "1.8388267755508423\n",
            "1.8413751125335693\n",
            "1.847540259361267\n",
            "1.8848222494125366\n",
            "1.8958476781845093\n",
            "1.8740262985229492\n",
            "1.8724807500839233\n",
            "1.8425647020339966\n",
            "1.8246793746948242\n",
            "1.8619855642318726\n",
            "1.9656208753585815\n",
            "1.8686286211013794\n",
            "1.8253599405288696\n",
            "1.8673757314682007\n",
            "1.79298996925354\n",
            "1.843766450881958\n",
            "1.8836596012115479\n",
            "1.8874549865722656\n",
            "1.8397122621536255\n",
            "1.8952866792678833\n",
            "1.8107954263687134\n",
            "1.8411552906036377\n",
            "1.8506274223327637\n",
            "1.7441291809082031\n",
            "1.8109703063964844\n",
            "1.806999921798706\n",
            "1.8018074035644531\n",
            "1.8609564304351807\n",
            "1.8409613370895386\n",
            "1.7908517122268677\n",
            "1.8272172212600708\n",
            "1.8111470937728882\n",
            "1.748219609260559\n",
            "1.7665314674377441\n",
            "1.7779618501663208\n",
            "1.8007664680480957\n",
            "1.7738145589828491\n",
            "1.8539912700653076\n",
            "1.7806479930877686\n",
            "1.8186185359954834\n",
            "1.799573302268982\n",
            "1.7621663808822632\n",
            "1.7496775388717651\n",
            "1.8114899396896362\n",
            "1.7445532083511353\n",
            "1.847230315208435\n",
            "1.7496881484985352\n",
            "1.7887678146362305\n",
            "1.7784258127212524\n",
            "1.782930850982666\n",
            "1.7891857624053955\n",
            "1.7863482236862183\n",
            "1.814135193824768\n",
            "1.7377517223358154\n",
            "1.8126674890518188\n",
            "1.7772465944290161\n",
            "1.7953190803527832\n",
            "1.6625255346298218\n",
            "1.761513352394104\n",
            "1.7787885665893555\n",
            "1.778733253479004\n",
            "1.7348275184631348\n",
            "1.7435963153839111\n",
            "1.7361993789672852\n",
            "1.7235281467437744\n",
            "1.7359791994094849\n",
            "1.7403438091278076\n",
            "1.7340859174728394\n",
            "1.8137365579605103\n",
            "1.7169872522354126\n",
            "1.7645471096038818\n",
            "1.714853048324585\n",
            "1.6995518207550049\n",
            "1.7314071655273438\n",
            "1.7289924621582031\n",
            "1.6702157258987427\n",
            "1.749976396560669\n",
            "1.7972649335861206\n",
            "1.7424346208572388\n",
            "1.7606502771377563\n",
            "1.6714463233947754\n",
            "1.7101109027862549\n",
            "1.6582351922988892\n",
            "1.7500739097595215\n",
            "1.7279685735702515\n",
            "1.7235974073410034\n",
            "1.7448484897613525\n",
            "1.7243010997772217\n",
            "1.7530920505523682\n",
            "1.711027979850769\n",
            "1.7067439556121826\n",
            "1.6742497682571411\n",
            "1.6496697664260864\n",
            "1.6860812902450562\n",
            "1.6526001691818237\n",
            "1.678033709526062\n",
            "1.7171220779418945\n",
            "1.6957796812057495\n",
            "1.743607521057129\n",
            "1.6912693977355957\n",
            "1.65968656539917\n",
            "1.7143957614898682\n",
            "1.7045323848724365\n",
            "1.6764459609985352\n",
            "1.7405482530593872\n",
            "1.6932960748672485\n",
            "1.6518605947494507\n",
            "1.6785216331481934\n",
            "1.6604737043380737\n",
            "1.7146480083465576\n",
            "1.678422212600708\n",
            "1.7172605991363525\n",
            "1.6802587509155273\n",
            "1.6702938079833984\n",
            "1.6145734786987305\n",
            "1.7069032192230225\n",
            "1.6620820760726929\n",
            "1.6193187236785889\n",
            "1.6804251670837402\n",
            "1.6559590101242065\n",
            "1.633835792541504\n",
            "1.5952948331832886\n",
            "1.6430710554122925\n",
            "1.739133358001709\n",
            "1.637792706489563\n",
            "1.5921331644058228\n",
            "1.674301266670227\n",
            "1.654505729675293\n",
            "1.6440987586975098\n",
            "1.6462095975875854\n",
            "1.658161997795105\n",
            "1.6233460903167725\n",
            "1.6253629922866821\n",
            "1.596361756324768\n",
            "1.6077114343643188\n",
            "1.6687965393066406\n",
            "1.6649478673934937\n",
            "1.6517828702926636\n",
            "1.6102168560028076\n",
            "1.6354362964630127\n",
            "1.6249135732650757\n",
            "1.5886160135269165\n",
            "1.609655499458313\n",
            "1.5709871053695679\n",
            "1.643970251083374\n",
            "1.6437053680419922\n",
            "1.6182305812835693\n",
            "1.678368091583252\n",
            "1.6129193305969238\n",
            "1.6006715297698975\n",
            "1.5936821699142456\n",
            "1.6401746273040771\n",
            "1.6111538410186768\n",
            "1.5846034288406372\n",
            "1.6177687644958496\n",
            "1.63275945186615\n",
            "1.6519317626953125\n",
            "1.630489468574524\n",
            "1.51658296585083\n",
            "1.6446738243103027\n",
            "1.6187716722488403\n",
            "1.5920898914337158\n",
            "1.5822752714157104\n",
            "1.6137079000473022\n",
            "1.56268310546875\n",
            "1.5046963691711426\n",
            "1.6291139125823975\n",
            "1.5834919214248657\n",
            "1.5995944738388062\n",
            "1.6448813676834106\n",
            "1.6192704439163208\n",
            "1.6098169088363647\n",
            "1.520613193511963\n",
            "1.6376492977142334\n",
            "1.5892220735549927\n",
            "1.6700925827026367\n",
            "1.5639050006866455\n",
            "1.637216329574585\n",
            "1.5724589824676514\n",
            "1.6235294342041016\n",
            "1.5459362268447876\n",
            "1.582725167274475\n",
            "1.611484408378601\n",
            "1.5709028244018555\n",
            "1.5930510759353638\n",
            "1.5710031986236572\n",
            "1.5877888202667236\n",
            "1.4926105737686157\n",
            "1.5535956621170044\n",
            "1.5036287307739258\n",
            "1.6431552171707153\n",
            "1.5705899000167847\n",
            "1.5476456880569458\n",
            "1.5091164112091064\n",
            "1.5841130018234253\n",
            "1.5603365898132324\n",
            "1.563088297843933\n",
            "1.5884623527526855\n",
            "1.535399317741394\n",
            "1.5349797010421753\n",
            "1.5032141208648682\n",
            "1.6592055559158325\n",
            "1.54120671749115\n",
            "1.5891329050064087\n",
            "1.5021346807479858\n",
            "1.4696192741394043\n",
            "1.5369206666946411\n",
            "1.588976502418518\n",
            "1.5426268577575684\n",
            "1.5161614418029785\n",
            "1.5676246881484985\n",
            "1.4976154565811157\n",
            "1.5569725036621094\n",
            "1.533905267715454\n",
            "1.5920748710632324\n",
            "1.520559549331665\n",
            "1.5371270179748535\n",
            "1.52937650680542\n",
            "1.5341862440109253\n",
            "1.567664623260498\n",
            "1.5768210887908936\n",
            "1.5680171251296997\n",
            "1.5483440160751343\n",
            "1.5117509365081787\n",
            "1.5305427312850952\n",
            "1.4727776050567627\n",
            "1.584748387336731\n",
            "1.5070072412490845\n",
            "1.524827241897583\n",
            "1.5751045942306519\n",
            "1.5131748914718628\n",
            "1.5221364498138428\n",
            "1.4676103591918945\n",
            "1.5532861948013306\n",
            "1.48331880569458\n",
            "1.5760549306869507\n",
            "1.5175739526748657\n",
            "1.5318775177001953\n",
            "1.5050071477890015\n",
            "1.4917100667953491\n",
            "1.4523085355758667\n",
            "1.5147331953048706\n",
            "1.492653250694275\n",
            "1.5345183610916138\n",
            "1.5308175086975098\n",
            "1.551419734954834\n",
            "1.5972295999526978\n",
            "1.451439619064331\n",
            "1.5697975158691406\n",
            "1.4929543733596802\n",
            "1.5128215551376343\n",
            "1.4926636219024658\n",
            "1.4944602251052856\n",
            "1.4658585786819458\n",
            "1.466281771659851\n",
            "1.5095155239105225\n",
            "1.4815304279327393\n",
            "1.4877183437347412\n",
            "1.4724689722061157\n",
            "1.5179557800292969\n",
            "1.4635645151138306\n",
            "1.4990148544311523\n",
            "1.5294406414031982\n",
            "1.5221762657165527\n",
            "1.4942240715026855\n",
            "1.5306953191757202\n",
            "1.4355965852737427\n",
            "1.5067152976989746\n",
            "1.529903531074524\n",
            "1.5189757347106934\n",
            "1.5051937103271484\n",
            "1.5096591711044312\n",
            "1.4845486879348755\n",
            "1.5199590921401978\n",
            "1.4571198225021362\n",
            "1.464687466621399\n",
            "1.477781891822815\n",
            "1.4325051307678223\n",
            "1.4860681295394897\n",
            "1.5033420324325562\n",
            "1.4848345518112183\n",
            "1.502720594406128\n",
            "1.4269170761108398\n",
            "1.4477360248565674\n",
            "1.4867008924484253\n",
            "1.580963373184204\n",
            "1.3910274505615234\n",
            "1.5292092561721802\n",
            "1.5008690357208252\n",
            "1.4762427806854248\n",
            "1.417919397354126\n",
            "1.4765855073928833\n",
            "1.4094444513320923\n",
            "1.4536274671554565\n",
            "1.505214810371399\n",
            "1.4177844524383545\n",
            "1.545883059501648\n",
            "1.5206925868988037\n",
            "1.471301794052124\n",
            "1.4775530099868774\n",
            "1.4701852798461914\n",
            "1.4197686910629272\n",
            "1.4933757781982422\n",
            "1.4836385250091553\n",
            "1.467315673828125\n",
            "1.4587160348892212\n",
            "1.5176091194152832\n",
            "1.5565056800842285\n",
            "1.5193828344345093\n",
            "1.4694664478302002\n",
            "1.4841442108154297\n",
            "1.4637694358825684\n",
            "1.471052646636963\n",
            "1.419655442237854\n",
            "1.4556493759155273\n",
            "1.5564155578613281\n",
            "1.4492303133010864\n",
            "1.37879478931427\n",
            "1.437040090560913\n",
            "1.4195506572723389\n",
            "1.4170432090759277\n",
            "1.4530932903289795\n",
            "1.4791843891143799\n",
            "1.4902070760726929\n",
            "1.3829009532928467\n",
            "1.416917324066162\n",
            "1.4904959201812744\n",
            "1.4689933061599731\n",
            "1.5182428359985352\n",
            "1.4939614534378052\n",
            "1.3967881202697754\n",
            "1.4583319425582886\n",
            "1.396610140800476\n",
            "1.415553092956543\n",
            "1.4478052854537964\n",
            "1.422018051147461\n",
            "1.3815932273864746\n",
            "1.4323095083236694\n",
            "1.453977346420288\n",
            "1.4019598960876465\n",
            "1.4323198795318604\n",
            "1.4426813125610352\n",
            "1.4828705787658691\n",
            "1.4424762725830078\n",
            "1.4539955854415894\n",
            "1.4692445993423462\n",
            "1.451528549194336\n",
            "1.4504950046539307\n",
            "1.424278736114502\n",
            "1.4491252899169922\n",
            "1.4444849491119385\n",
            "1.4216532707214355\n",
            "1.3883988857269287\n",
            "1.3687222003936768\n",
            "1.4648417234420776\n",
            "1.4075273275375366\n",
            "1.4274622201919556\n",
            "1.4089629650115967\n",
            "1.4565943479537964\n",
            "1.4392688274383545\n",
            "1.4316706657409668\n",
            "1.4259799718856812\n",
            "1.4205896854400635\n",
            "1.519183874130249\n",
            "1.4239368438720703\n",
            "1.4117318391799927\n",
            "1.4499475955963135\n",
            "1.463692307472229\n",
            "1.408766508102417\n",
            "1.43222177028656\n",
            "1.4376177787780762\n",
            "1.4258387088775635\n",
            "1.4269695281982422\n",
            "1.491819143295288\n",
            "1.3811286687850952\n",
            "1.412745475769043\n",
            "1.3818936347961426\n",
            "1.3842936754226685\n",
            "1.4874252080917358\n",
            "1.4401755332946777\n",
            "1.4632245302200317\n",
            "1.375558853149414\n",
            "1.395673394203186\n",
            "1.3903967142105103\n",
            "1.4691108465194702\n",
            "1.415892243385315\n",
            "1.3926054239273071\n",
            "1.376366138458252\n",
            "1.4287153482437134\n",
            "1.4256553649902344\n",
            "1.436995506286621\n",
            "1.4051793813705444\n",
            "1.3925427198410034\n",
            "1.4473786354064941\n",
            "1.4386917352676392\n",
            "1.4345767498016357\n",
            "1.4752634763717651\n",
            "1.4541559219360352\n",
            "1.3834017515182495\n",
            "1.4000545740127563\n",
            "1.434287667274475\n",
            "1.4426524639129639\n",
            "1.451900839805603\n",
            "1.4435242414474487\n",
            "1.3759067058563232\n",
            "1.359660029411316\n",
            "1.4593287706375122\n",
            "1.378986120223999\n",
            "1.3815935850143433\n",
            "1.375070333480835\n",
            "1.4495471715927124\n",
            "1.390845775604248\n",
            "1.348814606666565\n",
            "1.4318242073059082\n",
            "1.3788509368896484\n",
            "1.4221347570419312\n",
            "1.5002561807632446\n",
            "1.373076319694519\n",
            "1.3798365592956543\n",
            "1.422900676727295\n",
            "1.476715326309204\n",
            "1.4002983570098877\n",
            "1.4279794692993164\n",
            "1.3867584466934204\n",
            "1.3800846338272095\n",
            "1.377296805381775\n",
            "1.38287353515625\n",
            "1.3821215629577637\n",
            "1.3760051727294922\n",
            "1.3552252054214478\n",
            "1.3893439769744873\n",
            "1.37814462184906\n",
            "1.385656476020813\n",
            "1.470923900604248\n",
            "1.3304789066314697\n",
            "1.387778401374817\n",
            "1.338225245475769\n",
            "1.3859429359436035\n",
            "1.3739125728607178\n",
            "1.3281919956207275\n",
            "1.4378407001495361\n",
            "1.415895700454712\n",
            "1.4028767347335815\n",
            "1.3375509977340698\n",
            "1.3200702667236328\n",
            "1.380110740661621\n",
            "1.394822359085083\n",
            "1.4282078742980957\n",
            "1.3776400089263916\n",
            "1.3802400827407837\n",
            "1.340199589729309\n",
            "1.3853029012680054\n",
            "1.3261823654174805\n",
            "1.3719274997711182\n",
            "1.4003311395645142\n",
            "1.4303690195083618\n",
            "1.357749342918396\n",
            "1.3663477897644043\n",
            "1.4517691135406494\n",
            "1.353312611579895\n",
            "1.3585151433944702\n",
            "1.3945010900497437\n",
            "1.3648309707641602\n",
            "1.4017709493637085\n",
            "1.4141465425491333\n",
            "1.4072195291519165\n",
            "1.3959099054336548\n",
            "1.4179794788360596\n",
            "1.3273091316223145\n",
            "1.3391848802566528\n",
            "1.3567495346069336\n",
            "1.313493251800537\n",
            "1.4410688877105713\n",
            "1.3897955417633057\n",
            "1.364030122756958\n",
            "1.368847370147705\n",
            "1.3894813060760498\n",
            "1.4528645277023315\n",
            "1.429161548614502\n",
            "1.3565418720245361\n",
            "1.4471899271011353\n",
            "1.3805190324783325\n",
            "1.3433027267456055\n",
            "1.3666651248931885\n",
            "1.4102915525436401\n",
            "1.3189712762832642\n",
            "1.3468838930130005\n",
            "1.382775068283081\n",
            "1.4104152917861938\n",
            "1.3939471244812012\n",
            "1.3723222017288208\n",
            "1.3611664772033691\n",
            "1.3415699005126953\n",
            "1.3570679426193237\n",
            "1.4054614305496216\n",
            "1.3272446393966675\n",
            "1.3915705680847168\n",
            "1.3789832592010498\n",
            "1.3472120761871338\n",
            "1.3815653324127197\n",
            "1.3709136247634888\n",
            "1.3649924993515015\n",
            "1.3647375106811523\n",
            "1.393559217453003\n",
            "1.3783822059631348\n",
            "1.3578633069992065\n",
            "1.3632746934890747\n",
            "1.345399022102356\n",
            "1.3854119777679443\n",
            "1.3651801347732544\n",
            "1.3416153192520142\n",
            "1.383557677268982\n",
            "1.3464752435684204\n",
            "1.3763694763183594\n",
            "1.3435561656951904\n",
            "1.3493328094482422\n",
            "1.3872134685516357\n",
            "1.3484501838684082\n",
            "1.37054443359375\n",
            "1.3261886835098267\n",
            "1.2393300533294678\n",
            "1.3205512762069702\n",
            "1.345531940460205\n",
            "1.3846187591552734\n",
            "1.3701612949371338\n",
            "1.3599975109100342\n",
            "1.3663126230239868\n",
            "1.3389850854873657\n",
            "1.4029998779296875\n",
            "1.3496638536453247\n",
            "1.2866549491882324\n",
            "1.4185872077941895\n",
            "1.3311502933502197\n",
            "1.4346013069152832\n",
            "1.3418681621551514\n",
            "1.3496203422546387\n",
            "1.3226993083953857\n",
            "1.2726396322250366\n",
            "1.358190655708313\n",
            "1.4048656225204468\n",
            "1.3385697603225708\n",
            "1.3601059913635254\n",
            "1.303464412689209\n",
            "1.2705841064453125\n",
            "1.392987608909607\n",
            "1.366584300994873\n",
            "1.3292869329452515\n",
            "1.3827182054519653\n",
            "1.3837299346923828\n",
            "1.335763692855835\n",
            "1.3451780080795288\n",
            "1.3397942781448364\n",
            "1.3188502788543701\n",
            "1.3375811576843262\n",
            "1.3232145309448242\n",
            "1.337100625038147\n",
            "1.2707425355911255\n",
            "1.2629950046539307\n",
            "1.3695980310440063\n",
            "1.2836068868637085\n",
            "1.3275182247161865\n",
            "1.2901191711425781\n",
            "1.306581974029541\n",
            "1.3646224737167358\n",
            "1.3817110061645508\n",
            "1.3577545881271362\n",
            "1.2961381673812866\n",
            "1.3412606716156006\n",
            "1.2860263586044312\n",
            "1.28651762008667\n",
            "1.268294334411621\n",
            "1.3336057662963867\n",
            "1.3490216732025146\n",
            "1.2871854305267334\n",
            "1.4087692499160767\n",
            "1.3492450714111328\n",
            "1.3914024829864502\n",
            "1.3397551774978638\n",
            "1.318971037864685\n",
            "1.3747202157974243\n",
            "1.3832635879516602\n",
            "1.295927882194519\n",
            "1.3333961963653564\n",
            "1.4168028831481934\n",
            "1.3301782608032227\n",
            "1.3358845710754395\n",
            "1.3729147911071777\n",
            "1.3273255825042725\n",
            "1.3891127109527588\n",
            "1.385158896446228\n",
            "1.3053369522094727\n",
            "1.3451343774795532\n",
            "1.34662663936615\n",
            "1.3252100944519043\n",
            "1.338402271270752\n",
            "1.3720768690109253\n",
            "1.3166495561599731\n",
            "1.3169972896575928\n",
            "1.3207523822784424\n",
            "1.3202717304229736\n",
            "1.2918264865875244\n",
            "1.3404176235198975\n",
            "1.3180909156799316\n",
            "1.2728694677352905\n",
            "1.2828712463378906\n",
            "1.3086590766906738\n",
            "1.3704123497009277\n",
            "1.3073482513427734\n",
            "1.3396315574645996\n",
            "1.3756916522979736\n",
            "1.2568578720092773\n",
            "1.336854338645935\n",
            "1.2973028421401978\n",
            "1.3408845663070679\n",
            "1.2419712543487549\n",
            "1.3579353094100952\n",
            "1.311715006828308\n",
            "1.3441888093948364\n",
            "1.289707064628601\n",
            "1.3247061967849731\n",
            "1.3509660959243774\n",
            "1.3142353296279907\n",
            "1.2923400402069092\n",
            "1.2802999019622803\n",
            "1.2613779306411743\n",
            "1.2889211177825928\n",
            "1.3480675220489502\n",
            "1.320908546447754\n",
            "1.2672560214996338\n",
            "1.271864891052246\n",
            "1.3532350063323975\n",
            "1.295665979385376\n",
            "1.3363076448440552\n",
            "1.3545712232589722\n",
            "1.2491672039031982\n",
            "1.310400128364563\n",
            "1.2108781337738037\n",
            "1.3416064977645874\n",
            "1.308058738708496\n",
            "1.2925294637680054\n",
            "1.3608894348144531\n",
            "1.3327001333236694\n",
            "1.3194621801376343\n",
            "1.3063349723815918\n",
            "1.2960660457611084\n",
            "1.3433589935302734\n",
            "1.28505539894104\n",
            "1.303787112236023\n",
            "1.3881367444992065\n",
            "1.2924394607543945\n",
            "1.3104757070541382\n",
            "1.3355660438537598\n",
            "1.3787646293640137\n",
            "1.294238567352295\n",
            "1.2580419778823853\n",
            "1.2950760126113892\n",
            "1.296731948852539\n",
            "1.3731590509414673\n",
            "1.2912451028823853\n",
            "1.2925400733947754\n",
            "1.349462628364563\n",
            "1.2789171934127808\n",
            "1.3196887969970703\n",
            "1.3316909074783325\n",
            "1.2857393026351929\n",
            "1.4066487550735474\n",
            "1.3487695455551147\n",
            "1.2914650440216064\n",
            "1.3409457206726074\n",
            "1.2912324666976929\n",
            "1.267254114151001\n",
            "1.2548288106918335\n",
            "1.2534582614898682\n",
            "1.3821499347686768\n",
            "1.2175973653793335\n",
            "1.351401925086975\n",
            "1.333755373954773\n",
            "1.3269150257110596\n",
            "1.2747292518615723\n",
            "1.3010469675064087\n",
            "1.2769649028778076\n",
            "1.247120976448059\n",
            "1.3066792488098145\n",
            "1.355624794960022\n",
            "1.3062176704406738\n",
            "1.311141014099121\n",
            "1.431656837463379\n",
            "1.3138576745986938\n",
            "1.3164030313491821\n",
            "1.2417585849761963\n",
            "1.3596255779266357\n",
            "1.3225128650665283\n",
            "1.2825034856796265\n",
            "1.2867921590805054\n",
            "1.2739343643188477\n",
            "1.333938479423523\n",
            "1.283795952796936\n",
            "1.251458764076233\n",
            "1.3054234981536865\n",
            "1.2876675128936768\n",
            "1.3374749422073364\n",
            "1.350193977355957\n",
            "1.276551604270935\n",
            "1.3050206899642944\n",
            "1.241315245628357\n",
            "1.2898149490356445\n",
            "1.2945032119750977\n",
            "1.3197206258773804\n",
            "1.2970178127288818\n",
            "1.279360055923462\n",
            "1.2609230279922485\n",
            "1.2704837322235107\n",
            "1.3000515699386597\n",
            "1.286268949508667\n",
            "1.2830661535263062\n",
            "1.2845184803009033\n",
            "1.260230302810669\n",
            "1.2930593490600586\n",
            "1.254888653755188\n",
            "1.3354138135910034\n",
            "1.3596409559249878\n",
            "1.3309732675552368\n",
            "1.3540174961090088\n",
            "1.2671395540237427\n",
            "1.2907179594039917\n",
            "1.3117282390594482\n",
            "1.2511671781539917\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# === Config ===\n",
        "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
        "block_size = 1024\n",
        "batch_size = 8\n",
        "\n",
        "# === Load tokenizer metadata ===\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# === Load mmap data (char-level tokens, uint16) ===\n",
        "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
        "\n",
        "# === Efficient GPU Batch Sampler ===\n",
        "class GPUBatchDataset(Dataset):\n",
        "    def __init__(self, mmap_file, block_size, batch_size, device, jitter=63, p_aligned=0.5):\n",
        "        self.data = mmap_file\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.total = len(self.data) - block_size - 1\n",
        "        self.n_blocks = self.total // self.block_size\n",
        "        self.jitter = int(jitter)          # small random offset added to aligned start\n",
        "        self.p_aligned = float(p_aligned)  # mix aligned and jittered\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # choose a base aligned block\n",
        "            base_block = np.random.randint(0, self.n_blocks)\n",
        "            start = base_block * self.block_size\n",
        "\n",
        "            # with probability, add a small jitter (keeps cache-friendly contiguous reads)\n",
        "            if np.random.rand() > self.p_aligned:\n",
        "                j = np.random.randint(0, self.jitter + 1)\n",
        "                start = min(start + j, self.total)  # stay in range\n",
        "\n",
        "            X[i] = self.data[start : start + self.block_size]\n",
        "            Y[i] = self.data[start + 1 : start + 1 + self.block_size]\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
        "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
        "        )\n",
        "\n",
        "# === DataLoader ===\n",
        "train_dataset = GPUBatchDataset(train_ids, block_size, batch_size, device)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "# === Model ===\n",
        "config = GPTConfig(\n",
        "    vocab_size=len(stoi),\n",
        "    n_layer=4,\n",
        "    n_embd=256,\n",
        "    n_head=16,\n",
        "    dropout=0.0,\n",
        "    block_size=block_size\n",
        ")\n",
        "model = GPT(config)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "print(len(torch.nn.utils.parameters_to_vector(model.parameters())))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "losses = []\n",
        "\n",
        "# === Training Loop ===\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "          xb, yb = xb[0], yb[0]  # unwrap batch dimension\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits, loss = model(xb, yb)\n",
        "          loss = loss\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "          losses.append(loss.item())\n",
        "          print(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# === Run Training ===\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval_ce(mmap_file, n_batches=200):\n",
        "    model.eval()\n",
        "    T = block_size\n",
        "    losses = []\n",
        "    for _ in range(n_batches):\n",
        "        start = np.random.randint(0, len(mmap_file) - T - 2)  # any position, not aligned\n",
        "        X = torch.from_numpy(np.array(mmap_file[start:start+T], dtype=np.int64)).unsqueeze(0).to(device)\n",
        "        Y = torch.from_numpy(np.array(mmap_file[start+1:start+1+T], dtype=np.int64)).unsqueeze(0).to(device)\n",
        "        logits, loss = model(X, Y)\n",
        "        losses.append(loss.item())\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "print(\"train CE:\", eval_ce(train_ids, n_batches=200))\n",
        "print(\"val   CE:\", eval_ce(val_ids,   n_batches=200))\n",
        "print(\"train PPL:\", math.exp(eval_ce(train_ids, 200)))\n",
        "print(\"val   PPL:\", math.exp(eval_ce(val_ids,   200)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUAzrYGZVwmj",
        "outputId": "b5699804-356f-4f6b-9db2-02e5715f2543"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train CE: 3.630407050848007\n",
            "val   CE: 5.085663580894471\n",
            "train PPL: 37.29116680753269\n",
            "val   PPL: 161.9506689074359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8YZsTPrGnLm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def expected_rank_of_token(scores: torch.Tensor,\n",
        "                           token_ids: torch.Tensor,\n",
        "                           temperature: float = 1.0) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the expected rank of the given token at each position, without full V x V matrix.\n",
        "    \"\"\"\n",
        "    # scores: (..., V), token_ids: (...,)\n",
        "\n",
        "    # Gather score of the target token\n",
        "    score_i = scores.gather(-1, token_ids.unsqueeze(-1))  # (..., 1)\n",
        "\n",
        "    # Δ_j = score_j - score_i\n",
        "    diff = scores - score_i  # (..., V)\n",
        "\n",
        "    # P(j beats i)\n",
        "    p = torch.sigmoid(diff / temperature)\n",
        "\n",
        "    # Expected rank = 1 + sum_j P(j > i)\n",
        "    return 1.0 + p.sum(dim=-1)  # (...,)\n",
        "\n",
        "\n",
        "def rank_future_sequence_loss_soft(\n",
        "    logits: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    max_future_steps: int = 15,\n",
        "    decay: float = 0.5,\n",
        "    temperature: float = 1.0,\n",
        "    reduction: str = \"mean\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Memory-efficient smooth rank loss. For each t, matches rank of x_{t+Δ} to Δ.\n",
        "    logits  … (B, T, V) – model scores\n",
        "    targets … (B, T)    – token ids\n",
        "    \"\"\"\n",
        "    B, T, V = logits.shape\n",
        "    device = logits.device\n",
        "    total_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    for Δ in range(2, max_future_steps + 1):\n",
        "        if Δ >= T:\n",
        "            break\n",
        "\n",
        "        # Current time-step logits (for rank eval)\n",
        "        cur_logits  = logits[:, :-Δ, :]          # (B, T−Δ, V)\n",
        "        fut_targets = targets[:, Δ:]             # (B, T−Δ)\n",
        "\n",
        "        # Efficient rank of ground-truth future token\n",
        "        tgt_exp_rank = expected_rank_of_token(cur_logits, fut_targets, temperature)  # (B, T−Δ)\n",
        "\n",
        "        # Penalize distance from desired rank Δ\n",
        "        step_loss = F.l1_loss(\n",
        "            tgt_exp_rank,\n",
        "            torch.full_like(tgt_exp_rank, float(Δ)),\n",
        "            reduction=reduction\n",
        "        )\n",
        "\n",
        "        # Apply decay for further future steps\n",
        "        total_loss = total_loss + step_loss * (decay ** (Δ - 1))\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def ordered_future_loss(logits: torch.Tensor,\n",
        "                        targets: torch.Tensor,\n",
        "                        N: int = 15,\n",
        "                        decay: float = 0.7,\n",
        "                        tau: float = 1.0,\n",
        "                        reduction: str = \"mean\"):\n",
        "    \"\"\"\n",
        "    Penalise when the logits at step t do *not* respect the order of the next N tokens.\n",
        "\n",
        "        top-1 logit should match token t+1\n",
        "        top-2 logit should match token t+2\n",
        "        ...\n",
        "        top-N logit should match token t+N\n",
        "\n",
        "    logits  – (B, T, V)\n",
        "    targets – (B, T)\n",
        "    \"\"\"\n",
        "    B, T, V = logits.shape\n",
        "    device  = logits.device\n",
        "\n",
        "    if N < 2:\n",
        "        return torch.tensor(0., device=device)\n",
        "\n",
        "    # windows where t+N fits in sequence\n",
        "    valid_T = T - (N + 1)\n",
        "    if valid_T <= 0:\n",
        "        return torch.tensor(0., device=device)\n",
        "\n",
        "    # (B, valid_T, N) → future token ids for each offset 2..N\n",
        "    future_ids = torch.stack([targets[:, 2+k : 2+k+valid_T] for k in range(N)],\n",
        "                         dim=-1)\n",
        "\n",
        "    # (B, valid_T, N) → gather logits of those future tokens *now* (at step t)\n",
        "    step_logits = logits[:, :valid_T, :].gather(\n",
        "        -1, future_ids)                       # logit(x_{t+k})\n",
        "\n",
        "    # pair-wise differences  Δ_{k,j} = logit_k − logit_j, shape (B, valid_T, N, N)\n",
        "    diff = step_logits.unsqueeze(-1) - step_logits.unsqueeze(-2)\n",
        "\n",
        "    # upper-triangular mask k<j (ignore diag & lower triangle)\n",
        "    k_lt_j = torch.triu(torch.ones(N, N, device=device, dtype=torch.bool), 1)\n",
        "\n",
        "    # logistic ranking loss\n",
        "    pair_loss = F.softplus(-diff / tau)       # log(1+e^{-Δ/τ})\n",
        "    pair_loss = pair_loss[..., k_lt_j]        # keep k<j entries, now shape (B, valid_T, M)\n",
        "\n",
        "    # geometric weights per k (distance from current step)\n",
        "    k_idx = torch.arange(N, device=device)\n",
        "    weight = decay ** k_idx                   # shape (N,)\n",
        "    # broadcast to pair-wise (k<j) selector\n",
        "    weight_pair = weight.unsqueeze(-1).expand(N, N)[k_lt_j]  # (M,)\n",
        "\n",
        "    pair_loss = pair_loss * weight_pair       # (B, valid_T, M)\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        return pair_loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        return pair_loss.sum()\n",
        "    else:                                     # 'none'\n",
        "        return pair_loss                      # (B, valid_T, M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O0JnDKAyKlb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEljjIcEGnLn",
        "outputId": "6d2854c9-c0dd-4df7-be42-fcf3d8d56c4a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " isotonic cone loss = 0.0162\n",
            "0.6065202951431274\n",
            " isotonic cone loss = 0.0165\n",
            "0.6646222472190857\n",
            " isotonic cone loss = 0.0164\n",
            "0.6292304396629333\n",
            " isotonic cone loss = 0.0165\n",
            "0.6164491772651672\n",
            " isotonic cone loss = 0.0164\n",
            "0.6164873242378235\n",
            " isotonic cone loss = 0.0159\n",
            "0.6241068840026855\n",
            " isotonic cone loss = 0.0163\n",
            "0.5703762173652649\n",
            " isotonic cone loss = 0.0166\n",
            "0.4838354289531708\n",
            " isotonic cone loss = 0.0164\n",
            "0.5587405562400818\n",
            " isotonic cone loss = 0.0170\n",
            "0.4840662479400635\n",
            " isotonic cone loss = 0.0167\n",
            "0.5050009489059448\n",
            " isotonic cone loss = 0.0161\n",
            "0.6710383892059326\n",
            " isotonic cone loss = 0.0169\n",
            "0.6347062587738037\n",
            " isotonic cone loss = 0.0164\n",
            "0.6994177103042603\n",
            " isotonic cone loss = 0.0162\n",
            "0.6283939480781555\n",
            " isotonic cone loss = 0.0157\n",
            "0.5675783157348633\n",
            " isotonic cone loss = 0.0166\n",
            "0.6659209132194519\n",
            " isotonic cone loss = 0.0162\n",
            "0.5617184638977051\n",
            " isotonic cone loss = 0.0167\n",
            "0.5934674143791199\n",
            " isotonic cone loss = 0.0164\n",
            "0.4353094696998596\n",
            " isotonic cone loss = 0.0168\n",
            "0.5479791164398193\n",
            " isotonic cone loss = 0.0160\n",
            "0.6121336817741394\n",
            " isotonic cone loss = 0.0166\n",
            "0.505158007144928\n",
            " isotonic cone loss = 0.0163\n",
            "0.4993322491645813\n",
            " isotonic cone loss = 0.0165\n",
            "0.5678287744522095\n",
            " isotonic cone loss = 0.0168\n",
            "0.6469599008560181\n",
            " isotonic cone loss = 0.0169\n",
            "0.5415866374969482\n",
            " isotonic cone loss = 0.0169\n",
            "0.5696319341659546\n",
            " isotonic cone loss = 0.0167\n",
            "0.5341596603393555\n",
            " isotonic cone loss = 0.0164\n",
            "0.5248677730560303\n",
            " isotonic cone loss = 0.0165\n",
            "0.4821471869945526\n",
            " isotonic cone loss = 0.0166\n",
            "0.5524095892906189\n",
            " isotonic cone loss = 0.0168\n",
            "0.6399884819984436\n",
            " isotonic cone loss = 0.0165\n",
            "0.49043163657188416\n",
            " isotonic cone loss = 0.0164\n",
            "0.5795419216156006\n",
            " isotonic cone loss = 0.0167\n",
            "0.4806012809276581\n",
            " isotonic cone loss = 0.0170\n",
            "0.5298562049865723\n",
            " isotonic cone loss = 0.0167\n",
            "0.542778730392456\n",
            " isotonic cone loss = 0.0164\n",
            "0.6285346150398254\n",
            " isotonic cone loss = 0.0166\n",
            "0.4328097999095917\n",
            " isotonic cone loss = 0.0163\n",
            "0.6834514141082764\n",
            " isotonic cone loss = 0.0163\n",
            "0.5389702320098877\n",
            " isotonic cone loss = 0.0166\n",
            "0.5148103833198547\n",
            " isotonic cone loss = 0.0166\n",
            "0.5971315503120422\n",
            " isotonic cone loss = 0.0163\n",
            "0.5922890901565552\n",
            " isotonic cone loss = 0.0161\n",
            "0.6469639539718628\n",
            " isotonic cone loss = 0.0166\n",
            "0.6228707432746887\n",
            " isotonic cone loss = 0.0168\n",
            "0.7103928327560425\n",
            " isotonic cone loss = 0.0162\n",
            "0.6043503880500793\n",
            " isotonic cone loss = 0.0162\n",
            "0.6497825384140015\n",
            " isotonic cone loss = 0.0162\n",
            "0.5641348361968994\n",
            " isotonic cone loss = 0.0163\n",
            "0.5012904405593872\n",
            " isotonic cone loss = 0.0161\n",
            "0.5486714243888855\n",
            " isotonic cone loss = 0.0162\n",
            "0.48943543434143066\n",
            " isotonic cone loss = 0.0165\n",
            "0.5520421862602234\n",
            " isotonic cone loss = 0.0164\n",
            "0.528758704662323\n",
            " isotonic cone loss = 0.0165\n",
            "0.5265023112297058\n",
            " isotonic cone loss = 0.0167\n",
            "0.5676419138908386\n",
            " isotonic cone loss = 0.0164\n",
            "0.5790499448776245\n",
            " isotonic cone loss = 0.0166\n",
            "0.40784206986427307\n",
            " isotonic cone loss = 0.0168\n",
            "0.49219441413879395\n",
            " isotonic cone loss = 0.0166\n",
            "0.5031369924545288\n",
            " isotonic cone loss = 0.0166\n",
            "0.43415093421936035\n",
            " isotonic cone loss = 0.0162\n",
            "0.5653409957885742\n",
            " isotonic cone loss = 0.0164\n",
            "0.4518902599811554\n",
            " isotonic cone loss = 0.0161\n",
            "0.5547647476196289\n",
            " isotonic cone loss = 0.0164\n",
            "0.49064791202545166\n",
            " isotonic cone loss = 0.0167\n",
            "0.48571136593818665\n",
            " isotonic cone loss = 0.0165\n",
            "0.518686830997467\n",
            " isotonic cone loss = 0.0162\n",
            "0.6826419234275818\n",
            " isotonic cone loss = 0.0162\n",
            "0.591618001461029\n",
            " isotonic cone loss = 0.0162\n",
            "0.5346646308898926\n",
            " isotonic cone loss = 0.0161\n",
            "0.525415301322937\n",
            " isotonic cone loss = 0.0165\n",
            "0.6059473752975464\n",
            " isotonic cone loss = 0.0169\n",
            "0.6266729831695557\n",
            " isotonic cone loss = 0.0164\n",
            "0.6828813552856445\n",
            " isotonic cone loss = 0.0165\n",
            "0.6107520461082458\n",
            " isotonic cone loss = 0.0163\n",
            "0.5845863223075867\n",
            " isotonic cone loss = 0.0161\n",
            "0.5986936688423157\n",
            " isotonic cone loss = 0.0162\n",
            "0.5296688079833984\n",
            " isotonic cone loss = 0.0161\n",
            "0.48283445835113525\n",
            " isotonic cone loss = 0.0164\n",
            "0.6961236596107483\n",
            " isotonic cone loss = 0.0166\n",
            "0.5667852759361267\n",
            " isotonic cone loss = 0.0166\n",
            "0.6156013607978821\n",
            " isotonic cone loss = 0.0160\n",
            "0.5188789963722229\n",
            " isotonic cone loss = 0.0164\n",
            "0.571137547492981\n",
            " isotonic cone loss = 0.0164\n",
            "0.5829795598983765\n",
            " isotonic cone loss = 0.0163\n",
            "0.6098074316978455\n",
            " isotonic cone loss = 0.0162\n",
            "0.4404250383377075\n",
            " isotonic cone loss = 0.0163\n",
            "0.5557309985160828\n",
            " isotonic cone loss = 0.0164\n",
            "0.6030735969543457\n",
            " isotonic cone loss = 0.0167\n",
            "0.554990828037262\n",
            " isotonic cone loss = 0.0165\n",
            "0.513907253742218\n",
            " isotonic cone loss = 0.0165\n",
            "0.43086346983909607\n",
            " isotonic cone loss = 0.0160\n",
            "0.522717297077179\n",
            " isotonic cone loss = 0.0166\n",
            "0.6243453025817871\n",
            " isotonic cone loss = 0.0165\n",
            "0.5419912338256836\n",
            " isotonic cone loss = 0.0162\n",
            "0.7389711141586304\n",
            " isotonic cone loss = 0.0165\n",
            "0.6630370616912842\n",
            " isotonic cone loss = 0.0161\n",
            "0.5989196300506592\n",
            " isotonic cone loss = 0.0158\n",
            "0.5361197590827942\n",
            " isotonic cone loss = 0.0160\n",
            "0.6713331937789917\n",
            " isotonic cone loss = 0.0159\n",
            "0.651534914970398\n",
            " isotonic cone loss = 0.0160\n",
            "0.6754385828971863\n",
            " isotonic cone loss = 0.0163\n",
            "0.6784975528717041\n",
            " isotonic cone loss = 0.0160\n",
            "0.6602683663368225\n",
            " isotonic cone loss = 0.0164\n",
            "0.47735217213630676\n",
            " isotonic cone loss = 0.0161\n",
            "0.5869523882865906\n",
            " isotonic cone loss = 0.0155\n",
            "0.6612783074378967\n",
            " isotonic cone loss = 0.0163\n",
            "0.5623131990432739\n",
            " isotonic cone loss = 0.0163\n",
            "0.5865635871887207\n",
            " isotonic cone loss = 0.0163\n",
            "0.6991913318634033\n",
            " isotonic cone loss = 0.0163\n",
            "0.6303715705871582\n",
            " isotonic cone loss = 0.0163\n",
            "0.5217846035957336\n",
            " isotonic cone loss = 0.0163\n",
            "0.6547737717628479\n",
            " isotonic cone loss = 0.0163\n",
            "0.5929052829742432\n",
            " isotonic cone loss = 0.0163\n",
            "0.5517741441726685\n",
            " isotonic cone loss = 0.0167\n",
            "0.5703626275062561\n",
            " isotonic cone loss = 0.0160\n",
            "0.47168558835983276\n",
            " isotonic cone loss = 0.0164\n",
            "0.5110870599746704\n",
            " isotonic cone loss = 0.0167\n",
            "0.49487921595573425\n",
            " isotonic cone loss = 0.0167\n",
            "0.5815904140472412\n",
            " isotonic cone loss = 0.0166\n",
            "0.5147442817687988\n",
            " isotonic cone loss = 0.0166\n",
            "0.534941554069519\n",
            " isotonic cone loss = 0.0166\n",
            "0.6030377745628357\n",
            " isotonic cone loss = 0.0165\n",
            "0.4587841033935547\n",
            " isotonic cone loss = 0.0163\n",
            "0.5918739438056946\n",
            " isotonic cone loss = 0.0165\n",
            "0.5049338936805725\n",
            " isotonic cone loss = 0.0163\n",
            "0.6172177791595459\n",
            " isotonic cone loss = 0.0164\n",
            "0.4133189022541046\n",
            " isotonic cone loss = 0.0160\n",
            "0.6069645881652832\n",
            " isotonic cone loss = 0.0170\n",
            "0.507426381111145\n",
            " isotonic cone loss = 0.0165\n",
            "0.6386064887046814\n",
            " isotonic cone loss = 0.0164\n",
            "0.5570549368858337\n",
            " isotonic cone loss = 0.0163\n",
            "0.4949589967727661\n",
            " isotonic cone loss = 0.0164\n",
            "0.4609590768814087\n",
            " isotonic cone loss = 0.0164\n",
            "0.5210112929344177\n",
            " isotonic cone loss = 0.0168\n",
            "0.5438862442970276\n",
            " isotonic cone loss = 0.0162\n",
            "0.5751581788063049\n",
            " isotonic cone loss = 0.0168\n",
            "0.40298470854759216\n",
            " isotonic cone loss = 0.0163\n",
            "0.6224708557128906\n",
            " isotonic cone loss = 0.0165\n",
            "0.5403310060501099\n",
            " isotonic cone loss = 0.0164\n",
            "0.5345563888549805\n",
            " isotonic cone loss = 0.0161\n",
            "0.5859639048576355\n",
            " isotonic cone loss = 0.0167\n",
            "0.45336267352104187\n",
            " isotonic cone loss = 0.0164\n",
            "0.47463929653167725\n",
            " isotonic cone loss = 0.0163\n",
            "0.5092010498046875\n",
            " isotonic cone loss = 0.0161\n",
            "0.6731495261192322\n",
            " isotonic cone loss = 0.0167\n",
            "0.5542175769805908\n",
            " isotonic cone loss = 0.0168\n",
            "0.4636227786540985\n",
            " isotonic cone loss = 0.0163\n",
            "0.47902750968933105\n",
            " isotonic cone loss = 0.0164\n",
            "0.4709765911102295\n",
            " isotonic cone loss = 0.0169\n",
            "0.40185344219207764\n",
            " isotonic cone loss = 0.0166\n",
            "0.4813145697116852\n",
            " isotonic cone loss = 0.0163\n",
            "0.5608616471290588\n",
            " isotonic cone loss = 0.0167\n",
            "0.5344946980476379\n",
            " isotonic cone loss = 0.0166\n",
            "0.46229344606399536\n",
            " isotonic cone loss = 0.0162\n",
            "0.5318601131439209\n",
            " isotonic cone loss = 0.0170\n",
            "0.5034191608428955\n",
            " isotonic cone loss = 0.0166\n",
            "0.5778499245643616\n",
            " isotonic cone loss = 0.0161\n",
            "0.4932672381401062\n",
            " isotonic cone loss = 0.0164\n",
            "0.4939802289009094\n",
            " isotonic cone loss = 0.0168\n",
            "0.5202406048774719\n",
            " isotonic cone loss = 0.0162\n",
            "0.4143783450126648\n",
            " isotonic cone loss = 0.0167\n",
            "0.4682047665119171\n",
            " isotonic cone loss = 0.0165\n",
            "0.47547435760498047\n",
            " isotonic cone loss = 0.0166\n",
            "0.4780672490596771\n",
            " isotonic cone loss = 0.0166\n",
            "0.44142645597457886\n",
            " isotonic cone loss = 0.0167\n",
            "0.4249875247478485\n",
            " isotonic cone loss = 0.0164\n",
            "0.6096307039260864\n",
            " isotonic cone loss = 0.0169\n",
            "0.5280563831329346\n",
            " isotonic cone loss = 0.0165\n",
            "0.5036795735359192\n",
            " isotonic cone loss = 0.0165\n",
            "0.5708635449409485\n",
            " isotonic cone loss = 0.0167\n",
            "0.4585610032081604\n",
            " isotonic cone loss = 0.0163\n",
            "0.4969595968723297\n",
            " isotonic cone loss = 0.0167\n",
            "0.45331957936286926\n",
            " isotonic cone loss = 0.0163\n",
            "0.4279644787311554\n",
            " isotonic cone loss = 0.0163\n",
            "0.48446646332740784\n",
            " isotonic cone loss = 0.0166\n",
            "0.5843120813369751\n",
            " isotonic cone loss = 0.0167\n",
            "0.4748358428478241\n",
            " isotonic cone loss = 0.0169\n",
            "0.4191640019416809\n",
            " isotonic cone loss = 0.0165\n",
            "0.5368252396583557\n",
            " isotonic cone loss = 0.0169\n",
            "0.49898916482925415\n",
            " isotonic cone loss = 0.0165\n",
            "0.5595635175704956\n",
            " isotonic cone loss = 0.0164\n",
            "0.48647624254226685\n",
            " isotonic cone loss = 0.0164\n",
            "0.3912649154663086\n",
            " isotonic cone loss = 0.0167\n",
            "0.31321510672569275\n",
            " isotonic cone loss = 0.0166\n",
            "0.477276086807251\n",
            " isotonic cone loss = 0.0169\n",
            "0.5160080194473267\n",
            " isotonic cone loss = 0.0175\n",
            "0.42822161316871643\n",
            " isotonic cone loss = 0.0174\n",
            "0.4932519495487213\n",
            " isotonic cone loss = 0.0168\n",
            "0.549044668674469\n",
            " isotonic cone loss = 0.0172\n",
            "0.4519835114479065\n",
            " isotonic cone loss = 0.0167\n",
            "0.3499484062194824\n",
            " isotonic cone loss = 0.0161\n",
            "0.4523843824863434\n",
            " isotonic cone loss = 0.0164\n",
            "0.3452255427837372\n",
            " isotonic cone loss = 0.0165\n",
            "0.5096901059150696\n",
            " isotonic cone loss = 0.0168\n",
            "0.5372697114944458\n",
            " isotonic cone loss = 0.0166\n",
            "0.43852031230926514\n",
            " isotonic cone loss = 0.0165\n",
            "0.34238776564598083\n",
            " isotonic cone loss = 0.0163\n",
            "0.4029533863067627\n",
            " isotonic cone loss = 0.0163\n",
            "0.5328992009162903\n",
            " isotonic cone loss = 0.0169\n",
            "0.463410884141922\n",
            " isotonic cone loss = 0.0166\n",
            "0.5427111387252808\n",
            " isotonic cone loss = 0.0164\n",
            "0.45317983627319336\n",
            " isotonic cone loss = 0.0168\n",
            "0.3532642126083374\n",
            " isotonic cone loss = 0.0165\n",
            "0.46874508261680603\n",
            " isotonic cone loss = 0.0166\n",
            "0.4583668112754822\n",
            " isotonic cone loss = 0.0169\n",
            "0.5557845830917358\n",
            " isotonic cone loss = 0.0164\n",
            "0.58104407787323\n",
            " isotonic cone loss = 0.0164\n",
            "0.41096335649490356\n",
            " isotonic cone loss = 0.0162\n",
            "0.5532659292221069\n",
            " isotonic cone loss = 0.0164\n",
            "0.4617069959640503\n",
            " isotonic cone loss = 0.0166\n",
            "0.4537349343299866\n",
            " isotonic cone loss = 0.0164\n",
            "0.49556365609169006\n",
            " isotonic cone loss = 0.0165\n",
            "0.5276097655296326\n",
            " isotonic cone loss = 0.0168\n",
            "0.4525769352912903\n",
            " isotonic cone loss = 0.0165\n",
            "0.3634576201438904\n",
            " isotonic cone loss = 0.0169\n",
            "0.392397403717041\n",
            " isotonic cone loss = 0.0166\n",
            "0.4947640895843506\n",
            " isotonic cone loss = 0.0167\n",
            "0.46307817101478577\n",
            " isotonic cone loss = 0.0164\n",
            "0.5148575901985168\n",
            " isotonic cone loss = 0.0165\n",
            "0.4542487561702728\n",
            " isotonic cone loss = 0.0166\n",
            "0.49421995878219604\n",
            " isotonic cone loss = 0.0167\n",
            "0.48795410990715027\n",
            " isotonic cone loss = 0.0166\n",
            "0.45413336157798767\n",
            " isotonic cone loss = 0.0171\n",
            "0.42820486426353455\n",
            " isotonic cone loss = 0.0164\n",
            "0.526714563369751\n",
            " isotonic cone loss = 0.0163\n",
            "0.5399953126907349\n",
            " isotonic cone loss = 0.0166\n",
            "0.507813036441803\n",
            " isotonic cone loss = 0.0163\n",
            "0.38182738423347473\n",
            " isotonic cone loss = 0.0166\n",
            "0.5077736377716064\n",
            " isotonic cone loss = 0.0171\n",
            "0.5290923118591309\n",
            " isotonic cone loss = 0.0167\n",
            "0.4476393163204193\n",
            " isotonic cone loss = 0.0169\n",
            "0.552304744720459\n",
            " isotonic cone loss = 0.0166\n",
            "0.4593426585197449\n",
            " isotonic cone loss = 0.0168\n",
            "0.4171506464481354\n",
            " isotonic cone loss = 0.0170\n",
            "0.4461705684661865\n",
            " isotonic cone loss = 0.0168\n",
            "0.4625299870967865\n",
            " isotonic cone loss = 0.0167\n",
            "0.5458962917327881\n",
            " isotonic cone loss = 0.0163\n",
            "0.5688420534133911\n",
            " isotonic cone loss = 0.0162\n",
            "0.4739108085632324\n",
            " isotonic cone loss = 0.0166\n",
            "0.4420216679573059\n",
            " isotonic cone loss = 0.0163\n",
            "0.5125740766525269\n",
            " isotonic cone loss = 0.0164\n",
            "0.4285849928855896\n",
            " isotonic cone loss = 0.0165\n",
            "0.49048128724098206\n",
            " isotonic cone loss = 0.0167\n",
            "0.42074722051620483\n",
            " isotonic cone loss = 0.0163\n",
            "0.6400682926177979\n",
            " isotonic cone loss = 0.0168\n",
            "0.5318991541862488\n",
            " isotonic cone loss = 0.0170\n",
            "0.5121744275093079\n",
            " isotonic cone loss = 0.0164\n",
            "0.5309723019599915\n",
            " isotonic cone loss = 0.0167\n",
            "0.5251562595367432\n",
            " isotonic cone loss = 0.0162\n",
            "0.5471189022064209\n",
            " isotonic cone loss = 0.0164\n",
            "0.49692201614379883\n",
            " isotonic cone loss = 0.0160\n",
            "0.5105064511299133\n",
            " isotonic cone loss = 0.0160\n",
            "0.47057417035102844\n",
            " isotonic cone loss = 0.0166\n",
            "0.5202852487564087\n",
            " isotonic cone loss = 0.0165\n",
            "0.4778614640235901\n",
            " isotonic cone loss = 0.0168\n",
            "0.4427221715450287\n",
            " isotonic cone loss = 0.0166\n",
            "0.33923283219337463\n",
            " isotonic cone loss = 0.0168\n",
            "0.5191559791564941\n",
            " isotonic cone loss = 0.0164\n",
            "0.5259146690368652\n",
            " isotonic cone loss = 0.0167\n",
            "0.5033637285232544\n",
            " isotonic cone loss = 0.0163\n",
            "0.5017023682594299\n",
            " isotonic cone loss = 0.0165\n",
            "0.4517551064491272\n",
            " isotonic cone loss = 0.0167\n",
            "0.5391196012496948\n",
            " isotonic cone loss = 0.0170\n",
            "0.4435880184173584\n",
            " isotonic cone loss = 0.0166\n",
            "0.5795813202857971\n",
            " isotonic cone loss = 0.0162\n",
            "0.4177629351615906\n",
            " isotonic cone loss = 0.0162\n",
            "0.47539016604423523\n",
            " isotonic cone loss = 0.0167\n",
            "0.3793034255504608\n",
            " isotonic cone loss = 0.0164\n",
            "0.45119449496269226\n",
            " isotonic cone loss = 0.0167\n",
            "0.49549517035484314\n",
            " isotonic cone loss = 0.0165\n",
            "0.4842386245727539\n",
            " isotonic cone loss = 0.0171\n",
            "0.5691510438919067\n",
            " isotonic cone loss = 0.0166\n",
            "0.5395821332931519\n",
            " isotonic cone loss = 0.0162\n",
            "0.5146212577819824\n",
            " isotonic cone loss = 0.0166\n",
            "0.59466952085495\n",
            " isotonic cone loss = 0.0162\n",
            "0.518344521522522\n",
            " isotonic cone loss = 0.0166\n",
            "0.5608370304107666\n",
            " isotonic cone loss = 0.0170\n",
            "0.44637027382850647\n",
            " isotonic cone loss = 0.0165\n",
            "0.5893173217773438\n",
            " isotonic cone loss = 0.0165\n",
            "0.5729718208312988\n",
            " isotonic cone loss = 0.0166\n",
            "0.5790455937385559\n",
            " isotonic cone loss = 0.0166\n",
            "0.39091140031814575\n",
            " isotonic cone loss = 0.0166\n",
            "0.5394489169120789\n",
            " isotonic cone loss = 0.0159\n",
            "0.6912499666213989\n",
            " isotonic cone loss = 0.0166\n",
            "0.653313398361206\n",
            " isotonic cone loss = 0.0164\n",
            "0.4810704290866852\n",
            " isotonic cone loss = 0.0164\n",
            "0.6134682893753052\n",
            " isotonic cone loss = 0.0160\n",
            "0.4936409890651703\n",
            " isotonic cone loss = 0.0163\n",
            "0.484925776720047\n",
            " isotonic cone loss = 0.0165\n",
            "0.4037472605705261\n",
            " isotonic cone loss = 0.0166\n",
            "0.43565356731414795\n",
            " isotonic cone loss = 0.0164\n",
            "0.5265384912490845\n",
            " isotonic cone loss = 0.0163\n",
            "0.4664067029953003\n",
            " isotonic cone loss = 0.0166\n",
            "0.5717518329620361\n",
            " isotonic cone loss = 0.0163\n",
            "0.6047879457473755\n",
            " isotonic cone loss = 0.0165\n",
            "0.5051543712615967\n",
            " isotonic cone loss = 0.0169\n",
            "0.3977077901363373\n",
            " isotonic cone loss = 0.0166\n",
            "0.5426751375198364\n",
            " isotonic cone loss = 0.0160\n",
            "0.43282946944236755\n",
            " isotonic cone loss = 0.0164\n",
            "0.5601085424423218\n",
            " isotonic cone loss = 0.0164\n",
            "0.500658392906189\n",
            " isotonic cone loss = 0.0168\n",
            "0.5137845873832703\n",
            " isotonic cone loss = 0.0167\n",
            "0.5267588496208191\n",
            " isotonic cone loss = 0.0163\n",
            "0.5983993411064148\n",
            " isotonic cone loss = 0.0163\n",
            "0.5403522253036499\n",
            " isotonic cone loss = 0.0163\n",
            "0.38603073358535767\n",
            " isotonic cone loss = 0.0165\n",
            "0.4402291774749756\n",
            " isotonic cone loss = 0.0160\n",
            "0.4112195670604706\n",
            " isotonic cone loss = 0.0166\n",
            "0.5084489583969116\n",
            " isotonic cone loss = 0.0164\n",
            "0.4993019104003906\n",
            " isotonic cone loss = 0.0170\n",
            "0.45408517122268677\n",
            " isotonic cone loss = 0.0166\n",
            "0.4351117014884949\n",
            " isotonic cone loss = 0.0162\n",
            "0.44640737771987915\n",
            " isotonic cone loss = 0.0161\n",
            "0.6952550411224365\n",
            " isotonic cone loss = 0.0164\n",
            "0.5365062952041626\n",
            " isotonic cone loss = 0.0157\n",
            "0.5314058065414429\n",
            " isotonic cone loss = 0.0161\n",
            "0.5491254329681396\n",
            " isotonic cone loss = 0.0163\n",
            "0.6355768442153931\n",
            " isotonic cone loss = 0.0160\n",
            "0.4407168924808502\n",
            " isotonic cone loss = 0.0159\n",
            "0.4026760458946228\n",
            " isotonic cone loss = 0.0164\n",
            "0.4853980541229248\n",
            " isotonic cone loss = 0.0167\n",
            "0.47572100162506104\n",
            " isotonic cone loss = 0.0170\n",
            "0.3788744807243347\n",
            " isotonic cone loss = 0.0167\n",
            "0.5010544657707214\n",
            " isotonic cone loss = 0.0162\n",
            "0.6208181977272034\n",
            " isotonic cone loss = 0.0162\n",
            "0.5026922821998596\n",
            " isotonic cone loss = 0.0167\n",
            "0.5260018110275269\n",
            " isotonic cone loss = 0.0165\n",
            "0.4340611398220062\n",
            " isotonic cone loss = 0.0162\n",
            "0.7647950053215027\n",
            " isotonic cone loss = 0.0161\n",
            "0.5259509086608887\n",
            " isotonic cone loss = 0.0161\n",
            "0.48873797059059143\n",
            " isotonic cone loss = 0.0163\n",
            "0.526658296585083\n",
            " isotonic cone loss = 0.0163\n",
            "0.41480663418769836\n",
            " isotonic cone loss = 0.0165\n",
            "0.45759859681129456\n",
            " isotonic cone loss = 0.0164\n",
            "0.5335494875907898\n",
            " isotonic cone loss = 0.0164\n",
            "0.5126213431358337\n",
            " isotonic cone loss = 0.0165\n",
            "0.40819790959358215\n",
            " isotonic cone loss = 0.0163\n",
            "0.4437653720378876\n",
            " isotonic cone loss = 0.0164\n",
            "0.47427845001220703\n",
            " isotonic cone loss = 0.0160\n",
            "0.6136599183082581\n",
            " isotonic cone loss = 0.0164\n",
            "0.43367624282836914\n",
            " isotonic cone loss = 0.0166\n",
            "0.5659261345863342\n",
            " isotonic cone loss = 0.0163\n",
            "0.5186728835105896\n",
            " isotonic cone loss = 0.0169\n",
            "0.4998365640640259\n",
            " isotonic cone loss = 0.0164\n",
            "0.4530031979084015\n",
            " isotonic cone loss = 0.0161\n",
            "0.4804081618785858\n",
            " isotonic cone loss = 0.0159\n",
            "0.5279297828674316\n",
            " isotonic cone loss = 0.0163\n",
            "0.47521689534187317\n",
            " isotonic cone loss = 0.0163\n",
            "0.4832013249397278\n",
            " isotonic cone loss = 0.0164\n",
            "0.585233211517334\n",
            " isotonic cone loss = 0.0163\n",
            "0.515146017074585\n",
            " isotonic cone loss = 0.0160\n",
            "0.5873884558677673\n",
            " isotonic cone loss = 0.0164\n",
            "0.5662955045700073\n",
            " isotonic cone loss = 0.0162\n",
            "0.5706663727760315\n",
            " isotonic cone loss = 0.0160\n",
            "0.7117979526519775\n",
            " isotonic cone loss = 0.0160\n",
            "0.5159671306610107\n",
            " isotonic cone loss = 0.0161\n",
            "0.4148336946964264\n",
            " isotonic cone loss = 0.0166\n",
            "0.5680578351020813\n",
            " isotonic cone loss = 0.0162\n",
            "0.5647242665290833\n",
            " isotonic cone loss = 0.0163\n",
            "0.5765130519866943\n",
            " isotonic cone loss = 0.0166\n",
            "0.6410625576972961\n",
            " isotonic cone loss = 0.0165\n",
            "0.5070436596870422\n",
            " isotonic cone loss = 0.0160\n",
            "0.5876833200454712\n",
            " isotonic cone loss = 0.0159\n",
            "0.62934410572052\n",
            " isotonic cone loss = 0.0161\n",
            "0.5978338122367859\n",
            " isotonic cone loss = 0.0162\n",
            "0.5527815818786621\n",
            " isotonic cone loss = 0.0162\n",
            "0.5815858244895935\n",
            " isotonic cone loss = 0.0163\n",
            "0.48236748576164246\n",
            " isotonic cone loss = 0.0162\n",
            "0.4482634961605072\n",
            " isotonic cone loss = 0.0162\n",
            "0.43864449858665466\n",
            " isotonic cone loss = 0.0164\n",
            "0.5689614415168762\n",
            " isotonic cone loss = 0.0166\n",
            "0.5663195252418518\n",
            " isotonic cone loss = 0.0160\n",
            "0.6014978289604187\n",
            " isotonic cone loss = 0.0164\n",
            "0.4175097346305847\n",
            " isotonic cone loss = 0.0163\n",
            "0.5999140739440918\n",
            " isotonic cone loss = 0.0163\n",
            "0.5898815393447876\n",
            " isotonic cone loss = 0.0164\n",
            "0.5658435821533203\n",
            " isotonic cone loss = 0.0163\n",
            "0.620843231678009\n",
            " isotonic cone loss = 0.0162\n",
            "0.5272793173789978\n",
            " isotonic cone loss = 0.0160\n",
            "0.5278756022453308\n",
            " isotonic cone loss = 0.0162\n",
            "0.5149332284927368\n",
            " isotonic cone loss = 0.0161\n",
            "0.5652292370796204\n",
            " isotonic cone loss = 0.0165\n",
            "0.6355807781219482\n",
            " isotonic cone loss = 0.0164\n",
            "0.49040552973747253\n",
            " isotonic cone loss = 0.0163\n",
            "0.5217894315719604\n",
            " isotonic cone loss = 0.0163\n",
            "0.44811171293258667\n",
            " isotonic cone loss = 0.0162\n",
            "0.453096479177475\n",
            " isotonic cone loss = 0.0166\n",
            "0.4611688554286957\n",
            " isotonic cone loss = 0.0163\n",
            "0.47309958934783936\n",
            " isotonic cone loss = 0.0165\n",
            "0.4846455454826355\n",
            " isotonic cone loss = 0.0162\n",
            "0.6033684611320496\n",
            " isotonic cone loss = 0.0162\n",
            "0.4743972420692444\n",
            " isotonic cone loss = 0.0160\n",
            "0.5644500851631165\n",
            " isotonic cone loss = 0.0162\n",
            "0.5108169913291931\n",
            " isotonic cone loss = 0.0165\n",
            "0.5695233345031738\n",
            " isotonic cone loss = 0.0164\n",
            "0.704018771648407\n",
            " isotonic cone loss = 0.0162\n",
            "0.5954223275184631\n",
            " isotonic cone loss = 0.0162\n",
            "0.6004433035850525\n",
            " isotonic cone loss = 0.0165\n",
            "0.5325320959091187\n",
            " isotonic cone loss = 0.0160\n",
            "0.5403251051902771\n",
            " isotonic cone loss = 0.0163\n",
            "0.41302362084388733\n",
            " isotonic cone loss = 0.0165\n",
            "0.6003564596176147\n",
            " isotonic cone loss = 0.0165\n",
            "0.5784509181976318\n",
            " isotonic cone loss = 0.0162\n",
            "0.58042311668396\n",
            " isotonic cone loss = 0.0163\n",
            "0.5294890403747559\n",
            " isotonic cone loss = 0.0159\n",
            "0.6266242265701294\n",
            " isotonic cone loss = 0.0164\n",
            "0.5650540590286255\n",
            " isotonic cone loss = 0.0168\n",
            "0.5854635238647461\n",
            " isotonic cone loss = 0.0168\n",
            "0.5387423038482666\n",
            " isotonic cone loss = 0.0169\n",
            "0.4623279571533203\n",
            " isotonic cone loss = 0.0161\n",
            "0.651168167591095\n",
            " isotonic cone loss = 0.0164\n",
            "0.5900413393974304\n",
            " isotonic cone loss = 0.0164\n",
            "0.5108134150505066\n",
            " isotonic cone loss = 0.0162\n",
            "0.6163609027862549\n",
            " isotonic cone loss = 0.0160\n",
            "0.5981857180595398\n",
            " isotonic cone loss = 0.0159\n",
            "0.7607971429824829\n",
            " isotonic cone loss = 0.0160\n",
            "0.5568956732749939\n",
            " isotonic cone loss = 0.0163\n",
            "0.5886345505714417\n",
            " isotonic cone loss = 0.0162\n",
            "0.4802404046058655\n",
            " isotonic cone loss = 0.0159\n",
            "0.7166742086410522\n",
            " isotonic cone loss = 0.0164\n",
            "0.5787478089332581\n",
            " isotonic cone loss = 0.0161\n",
            "0.5705870985984802\n",
            " isotonic cone loss = 0.0157\n",
            "0.6624082326889038\n",
            " isotonic cone loss = 0.0163\n",
            "0.6042624115943909\n",
            " isotonic cone loss = 0.0160\n",
            "0.6005541682243347\n",
            " isotonic cone loss = 0.0161\n",
            "0.49323609471321106\n",
            " isotonic cone loss = 0.0160\n",
            "0.41516873240470886\n",
            " isotonic cone loss = 0.0161\n",
            "0.457772821187973\n",
            " isotonic cone loss = 0.0164\n",
            "0.39381900429725647\n",
            " isotonic cone loss = 0.0165\n",
            "0.41503655910491943\n",
            " isotonic cone loss = 0.0163\n",
            "0.5504562854766846\n",
            " isotonic cone loss = 0.0161\n",
            "0.566836953163147\n",
            " isotonic cone loss = 0.0160\n",
            "0.45237553119659424\n",
            " isotonic cone loss = 0.0161\n",
            "0.520610511302948\n",
            " isotonic cone loss = 0.0162\n",
            "0.5504820942878723\n",
            " isotonic cone loss = 0.0164\n",
            "0.5634523630142212\n",
            " isotonic cone loss = 0.0161\n",
            "0.43778085708618164\n",
            " isotonic cone loss = 0.0166\n",
            "0.49798864126205444\n",
            " isotonic cone loss = 0.0162\n",
            "0.47538110613822937\n",
            " isotonic cone loss = 0.0167\n",
            "0.5443289279937744\n",
            " isotonic cone loss = 0.0163\n",
            "0.4527641236782074\n",
            " isotonic cone loss = 0.0160\n",
            "0.5777662396430969\n",
            " isotonic cone loss = 0.0160\n",
            "0.540370523929596\n",
            " isotonic cone loss = 0.0165\n",
            "0.4051753580570221\n",
            " isotonic cone loss = 0.0160\n",
            "0.4618709683418274\n",
            " isotonic cone loss = 0.0165\n",
            "0.3953125774860382\n",
            " isotonic cone loss = 0.0169\n",
            "0.3810509443283081\n",
            " isotonic cone loss = 0.0167\n",
            "0.40085306763648987\n",
            " isotonic cone loss = 0.0166\n",
            "0.5233693718910217\n",
            " isotonic cone loss = 0.0166\n",
            "0.48098528385162354\n",
            " isotonic cone loss = 0.0162\n",
            "0.40162765979766846\n",
            " isotonic cone loss = 0.0159\n",
            "0.44805335998535156\n",
            " isotonic cone loss = 0.0159\n",
            "0.5387234091758728\n",
            " isotonic cone loss = 0.0168\n",
            "0.31741803884506226\n",
            " isotonic cone loss = 0.0164\n",
            "0.35733506083488464\n",
            " isotonic cone loss = 0.0164\n",
            "0.5131931304931641\n",
            " isotonic cone loss = 0.0163\n",
            "0.48895207047462463\n",
            " isotonic cone loss = 0.0166\n",
            "0.3500593602657318\n",
            " isotonic cone loss = 0.0165\n",
            "0.5543413758277893\n",
            " isotonic cone loss = 0.0163\n",
            "0.4311843514442444\n",
            " isotonic cone loss = 0.0163\n",
            "0.4740819036960602\n",
            " isotonic cone loss = 0.0163\n",
            "0.5737763047218323\n",
            " isotonic cone loss = 0.0164\n",
            "0.37660735845565796\n",
            " isotonic cone loss = 0.0162\n",
            "0.3740859627723694\n",
            " isotonic cone loss = 0.0161\n",
            "0.41937193274497986\n",
            " isotonic cone loss = 0.0163\n",
            "0.4218643307685852\n",
            " isotonic cone loss = 0.0163\n",
            "0.47562161087989807\n",
            " isotonic cone loss = 0.0163\n",
            "0.41609036922454834\n",
            " isotonic cone loss = 0.0165\n",
            "0.3629307448863983\n",
            " isotonic cone loss = 0.0164\n",
            "0.4737105667591095\n",
            " isotonic cone loss = 0.0161\n",
            "0.5965080857276917\n",
            " isotonic cone loss = 0.0162\n",
            "0.4587225615978241\n",
            " isotonic cone loss = 0.0163\n",
            "0.6152976155281067\n",
            " isotonic cone loss = 0.0162\n",
            "0.3806624412536621\n",
            " isotonic cone loss = 0.0158\n",
            "0.6003290414810181\n",
            " isotonic cone loss = 0.0160\n",
            "0.4475715756416321\n",
            " isotonic cone loss = 0.0165\n",
            "0.3084994852542877\n",
            " isotonic cone loss = 0.0165\n",
            "0.43376287817955017\n",
            " isotonic cone loss = 0.0165\n",
            "0.39805227518081665\n",
            " isotonic cone loss = 0.0164\n",
            "0.4404821991920471\n",
            " isotonic cone loss = 0.0163\n",
            "0.49648889899253845\n",
            " isotonic cone loss = 0.0163\n",
            "0.41286659240722656\n",
            " isotonic cone loss = 0.0164\n",
            "0.44841697812080383\n",
            " isotonic cone loss = 0.0163\n",
            "0.47098755836486816\n",
            " isotonic cone loss = 0.0165\n",
            "0.5415952801704407\n",
            " isotonic cone loss = 0.0163\n",
            "0.43360602855682373\n",
            " isotonic cone loss = 0.0165\n",
            "0.5086801648139954\n",
            " isotonic cone loss = 0.0164\n",
            "0.3827868103981018\n",
            " isotonic cone loss = 0.0167\n",
            "0.4825747013092041\n",
            " isotonic cone loss = 0.0163\n",
            "0.4249964952468872\n",
            " isotonic cone loss = 0.0166\n",
            "0.4786033630371094\n",
            " isotonic cone loss = 0.0164\n",
            "0.5380970239639282\n",
            " isotonic cone loss = 0.0169\n",
            "0.6249793171882629\n",
            " isotonic cone loss = 0.0163\n",
            "0.48129406571388245\n",
            " isotonic cone loss = 0.0160\n",
            "0.45157673954963684\n",
            " isotonic cone loss = 0.0162\n",
            "0.46917396783828735\n",
            " isotonic cone loss = 0.0164\n",
            "0.48114636540412903\n",
            " isotonic cone loss = 0.0161\n",
            "0.6822783946990967\n",
            " isotonic cone loss = 0.0164\n",
            "0.6017324924468994\n",
            " isotonic cone loss = 0.0165\n",
            "0.39634427428245544\n",
            " isotonic cone loss = 0.0165\n",
            "0.4243623912334442\n",
            " isotonic cone loss = 0.0161\n",
            "0.4139229357242584\n",
            " isotonic cone loss = 0.0162\n",
            "0.48927247524261475\n",
            " isotonic cone loss = 0.0158\n",
            "0.542793333530426\n",
            " isotonic cone loss = 0.0162\n",
            "0.5259650945663452\n",
            " isotonic cone loss = 0.0160\n",
            "0.4812549948692322\n",
            " isotonic cone loss = 0.0162\n",
            "0.37881985306739807\n",
            " isotonic cone loss = 0.0162\n",
            "0.4515184164047241\n",
            " isotonic cone loss = 0.0161\n",
            "0.5890370607376099\n",
            " isotonic cone loss = 0.0159\n",
            "0.42890915274620056\n",
            " isotonic cone loss = 0.0162\n",
            "0.4939383268356323\n",
            " isotonic cone loss = 0.0162\n",
            "0.36165788769721985\n",
            " isotonic cone loss = 0.0163\n",
            "0.34295856952667236\n",
            " isotonic cone loss = 0.0160\n",
            "0.4900113642215729\n",
            " isotonic cone loss = 0.0161\n",
            "0.5218512415885925\n",
            " isotonic cone loss = 0.0158\n",
            "0.4733107089996338\n",
            " isotonic cone loss = 0.0165\n",
            "0.34022897481918335\n",
            " isotonic cone loss = 0.0163\n",
            "0.5497205853462219\n",
            " isotonic cone loss = 0.0161\n",
            "0.4903391897678375\n",
            " isotonic cone loss = 0.0163\n",
            "0.36869585514068604\n",
            " isotonic cone loss = 0.0163\n",
            "0.4569261372089386\n",
            " isotonic cone loss = 0.0159\n",
            "0.4348418712615967\n",
            " isotonic cone loss = 0.0166\n",
            "0.5722779035568237\n",
            " isotonic cone loss = 0.0162\n",
            "0.3543377220630646\n",
            " isotonic cone loss = 0.0159\n",
            "0.5038928985595703\n",
            " isotonic cone loss = 0.0166\n",
            "0.3850841522216797\n",
            " isotonic cone loss = 0.0163\n",
            "0.3676397204399109\n",
            " isotonic cone loss = 0.0163\n",
            "0.3654929995536804\n",
            " isotonic cone loss = 0.0162\n",
            "0.5391036868095398\n",
            " isotonic cone loss = 0.0165\n",
            "0.36407577991485596\n",
            " isotonic cone loss = 0.0164\n",
            "0.459009051322937\n",
            " isotonic cone loss = 0.0162\n",
            "0.48584383726119995\n",
            " isotonic cone loss = 0.0162\n",
            "0.36929938197135925\n",
            " isotonic cone loss = 0.0166\n",
            "0.35196465253829956\n",
            " isotonic cone loss = 0.0168\n",
            "0.3957097828388214\n",
            " isotonic cone loss = 0.0163\n",
            "0.4133499264717102\n",
            " isotonic cone loss = 0.0166\n",
            "0.2904590964317322\n",
            " isotonic cone loss = 0.0164\n",
            "0.42134541273117065\n",
            " isotonic cone loss = 0.0165\n",
            "0.4543704688549042\n",
            " isotonic cone loss = 0.0165\n",
            "0.21982473134994507\n",
            " isotonic cone loss = 0.0166\n",
            "0.33740654587745667\n",
            " isotonic cone loss = 0.0164\n",
            "0.35427847504615784\n",
            " isotonic cone loss = 0.0164\n",
            "0.5221571326255798\n",
            " isotonic cone loss = 0.0163\n",
            "0.42812567949295044\n",
            " isotonic cone loss = 0.0162\n",
            "0.4477645754814148\n",
            " isotonic cone loss = 0.0161\n",
            "0.404280424118042\n",
            " isotonic cone loss = 0.0161\n",
            "0.36396539211273193\n",
            " isotonic cone loss = 0.0166\n",
            "0.4043889045715332\n",
            " isotonic cone loss = 0.0169\n",
            "0.44225460290908813\n",
            " isotonic cone loss = 0.0164\n",
            "0.42630597949028015\n",
            " isotonic cone loss = 0.0164\n",
            "0.3695085644721985\n",
            " isotonic cone loss = 0.0164\n",
            "0.5310074090957642\n",
            " isotonic cone loss = 0.0168\n",
            "0.535106897354126\n",
            " isotonic cone loss = 0.0163\n",
            "0.49665567278862\n",
            " isotonic cone loss = 0.0158\n",
            "0.4868068993091583\n",
            " isotonic cone loss = 0.0161\n",
            "0.48455873131752014\n",
            " isotonic cone loss = 0.0157\n",
            "0.4917874336242676\n",
            " isotonic cone loss = 0.0164\n",
            "0.5274240374565125\n",
            " isotonic cone loss = 0.0166\n",
            "0.5416187047958374\n",
            " isotonic cone loss = 0.0162\n",
            "0.5467384457588196\n",
            " isotonic cone loss = 0.0163\n",
            "0.386042058467865\n",
            " isotonic cone loss = 0.0162\n",
            "0.46975234150886536\n",
            " isotonic cone loss = 0.0162\n",
            "0.5331510305404663\n",
            " isotonic cone loss = 0.0161\n",
            "0.6152045130729675\n",
            " isotonic cone loss = 0.0162\n",
            "0.4413413107395172\n",
            " isotonic cone loss = 0.0163\n",
            "0.5450341701507568\n",
            " isotonic cone loss = 0.0162\n",
            "0.5343860983848572\n",
            " isotonic cone loss = 0.0165\n",
            "0.48768341541290283\n",
            " isotonic cone loss = 0.0163\n",
            "0.4918985068798065\n",
            " isotonic cone loss = 0.0160\n",
            "0.47248175740242004\n",
            " isotonic cone loss = 0.0164\n",
            "0.48074403405189514\n",
            " isotonic cone loss = 0.0161\n",
            "0.4234684705734253\n",
            " isotonic cone loss = 0.0166\n",
            "0.42286139726638794\n",
            " isotonic cone loss = 0.0163\n",
            "0.529981791973114\n",
            " isotonic cone loss = 0.0164\n",
            "0.5941681265830994\n",
            " isotonic cone loss = 0.0158\n",
            "0.4878382384777069\n",
            " isotonic cone loss = 0.0164\n",
            "0.5362874269485474\n",
            " isotonic cone loss = 0.0166\n",
            "0.4084321856498718\n",
            " isotonic cone loss = 0.0159\n",
            "0.44035428762435913\n",
            " isotonic cone loss = 0.0164\n",
            "0.4754856824874878\n",
            " isotonic cone loss = 0.0162\n",
            "0.42232829332351685\n",
            " isotonic cone loss = 0.0161\n",
            "0.4732206165790558\n",
            " isotonic cone loss = 0.0164\n",
            "0.505348265171051\n",
            " isotonic cone loss = 0.0163\n",
            "0.4250469207763672\n",
            " isotonic cone loss = 0.0164\n",
            "0.3772733211517334\n",
            " isotonic cone loss = 0.0166\n",
            "0.4012690782546997\n",
            " isotonic cone loss = 0.0163\n",
            "0.4260610342025757\n",
            " isotonic cone loss = 0.0164\n",
            "0.42677488923072815\n",
            " isotonic cone loss = 0.0159\n",
            "0.5685288310050964\n",
            " isotonic cone loss = 0.0161\n",
            "0.5047862529754639\n",
            " isotonic cone loss = 0.0160\n",
            "0.47398513555526733\n",
            " isotonic cone loss = 0.0163\n",
            "0.39175358414649963\n",
            " isotonic cone loss = 0.0162\n",
            "0.4066029191017151\n",
            " isotonic cone loss = 0.0164\n",
            "0.4129720628261566\n",
            " isotonic cone loss = 0.0162\n",
            "0.46883127093315125\n",
            " isotonic cone loss = 0.0164\n",
            "0.5053773522377014\n",
            " isotonic cone loss = 0.0168\n",
            "0.43332114815711975\n",
            " isotonic cone loss = 0.0168\n",
            "0.4723770320415497\n",
            " isotonic cone loss = 0.0163\n",
            "0.4740172028541565\n",
            " isotonic cone loss = 0.0162\n",
            "0.3772584795951843\n",
            " isotonic cone loss = 0.0163\n",
            "0.4263933598995209\n",
            " isotonic cone loss = 0.0156\n",
            "0.471539705991745\n",
            " isotonic cone loss = 0.0164\n",
            "0.4293344020843506\n",
            " isotonic cone loss = 0.0162\n",
            "0.4787299633026123\n",
            " isotonic cone loss = 0.0161\n",
            "0.44636809825897217\n",
            " isotonic cone loss = 0.0161\n",
            "0.4858587384223938\n",
            " isotonic cone loss = 0.0159\n",
            "0.5151984691619873\n",
            " isotonic cone loss = 0.0161\n",
            "0.5467734336853027\n",
            " isotonic cone loss = 0.0166\n",
            "0.36218300461769104\n",
            " isotonic cone loss = 0.0164\n",
            "0.5324702858924866\n",
            " isotonic cone loss = 0.0162\n",
            "0.5182818174362183\n",
            " isotonic cone loss = 0.0161\n",
            "0.4219377338886261\n",
            " isotonic cone loss = 0.0162\n",
            "0.5785102248191833\n",
            " isotonic cone loss = 0.0163\n",
            "0.48175910115242004\n",
            " isotonic cone loss = 0.0160\n",
            "0.3964696228504181\n",
            " isotonic cone loss = 0.0162\n",
            "0.4634988605976105\n",
            " isotonic cone loss = 0.0161\n",
            "0.4688844382762909\n",
            " isotonic cone loss = 0.0162\n",
            "0.3427823781967163\n",
            " isotonic cone loss = 0.0159\n",
            "0.5009287595748901\n",
            " isotonic cone loss = 0.0161\n",
            "0.5028955936431885\n",
            " isotonic cone loss = 0.0161\n",
            "0.41232937574386597\n",
            " isotonic cone loss = 0.0163\n",
            "0.41794610023498535\n",
            " isotonic cone loss = 0.0161\n",
            "0.4596177339553833\n",
            " isotonic cone loss = 0.0162\n",
            "0.3977004587650299\n",
            " isotonic cone loss = 0.0166\n",
            "0.4445793032646179\n",
            " isotonic cone loss = 0.0159\n",
            "0.4810793995857239\n",
            " isotonic cone loss = 0.0163\n",
            "0.48164603114128113\n",
            " isotonic cone loss = 0.0165\n",
            "0.3743772506713867\n",
            " isotonic cone loss = 0.0167\n",
            "0.35043805837631226\n",
            " isotonic cone loss = 0.0167\n",
            "0.4377213716506958\n",
            " isotonic cone loss = 0.0165\n",
            "0.46154889464378357\n",
            " isotonic cone loss = 0.0160\n",
            "0.49918311834335327\n",
            " isotonic cone loss = 0.0162\n",
            "0.35876667499542236\n",
            " isotonic cone loss = 0.0160\n",
            "0.42784062027931213\n",
            " isotonic cone loss = 0.0164\n",
            "0.3637717664241791\n",
            " isotonic cone loss = 0.0165\n",
            "0.42362794280052185\n",
            " isotonic cone loss = 0.0168\n",
            "0.34854620695114136\n",
            " isotonic cone loss = 0.0167\n",
            "0.4802214205265045\n",
            " isotonic cone loss = 0.0165\n",
            "0.46815863251686096\n",
            " isotonic cone loss = 0.0164\n",
            "0.4500350058078766\n",
            " isotonic cone loss = 0.0165\n",
            "0.45825907588005066\n",
            " isotonic cone loss = 0.0165\n",
            "0.4414912462234497\n",
            " isotonic cone loss = 0.0167\n",
            "0.33440789580345154\n",
            " isotonic cone loss = 0.0164\n",
            "0.4585714638233185\n",
            " isotonic cone loss = 0.0164\n",
            "0.3712257742881775\n",
            " isotonic cone loss = 0.0168\n",
            "0.4343860149383545\n",
            " isotonic cone loss = 0.0164\n",
            "0.4100461006164551\n",
            " isotonic cone loss = 0.0168\n",
            "0.3865632712841034\n",
            " isotonic cone loss = 0.0162\n",
            "0.46700891852378845\n",
            " isotonic cone loss = 0.0165\n",
            "0.4207063913345337\n",
            " isotonic cone loss = 0.0164\n",
            "0.37371107935905457\n",
            " isotonic cone loss = 0.0163\n",
            "0.38887467980384827\n",
            " isotonic cone loss = 0.0166\n",
            "0.492959588766098\n",
            " isotonic cone loss = 0.0165\n",
            "0.40456655621528625\n",
            " isotonic cone loss = 0.0165\n",
            "0.4925974905490875\n",
            " isotonic cone loss = 0.0167\n",
            "0.4395570456981659\n",
            " isotonic cone loss = 0.0164\n",
            "0.4408752918243408\n",
            " isotonic cone loss = 0.0164\n",
            "0.3314404785633087\n",
            " isotonic cone loss = 0.0168\n",
            "0.44612762331962585\n",
            " isotonic cone loss = 0.0164\n",
            "0.4188937544822693\n",
            " isotonic cone loss = 0.0164\n",
            "0.5079635977745056\n",
            " isotonic cone loss = 0.0164\n",
            "0.4133702516555786\n",
            " isotonic cone loss = 0.0162\n",
            "0.41106289625167847\n",
            " isotonic cone loss = 0.0164\n",
            "0.4971930980682373\n",
            " isotonic cone loss = 0.0167\n",
            "0.4842248558998108\n",
            " isotonic cone loss = 0.0162\n",
            "0.4513784646987915\n",
            " isotonic cone loss = 0.0163\n",
            "0.4297991096973419\n",
            " isotonic cone loss = 0.0165\n",
            "0.4254422187805176\n",
            " isotonic cone loss = 0.0162\n",
            "0.55185866355896\n",
            " isotonic cone loss = 0.0163\n",
            "0.4064434766769409\n",
            " isotonic cone loss = 0.0159\n",
            "0.6374131441116333\n",
            " isotonic cone loss = 0.0160\n",
            "0.506231427192688\n",
            " isotonic cone loss = 0.0159\n",
            "0.38641229271888733\n",
            " isotonic cone loss = 0.0160\n",
            "0.5415025353431702\n",
            " isotonic cone loss = 0.0164\n",
            "0.38072943687438965\n",
            " isotonic cone loss = 0.0163\n",
            "0.5270707607269287\n",
            " isotonic cone loss = 0.0159\n",
            "0.45202091336250305\n",
            " isotonic cone loss = 0.0161\n",
            "0.553132176399231\n",
            " isotonic cone loss = 0.0159\n",
            "0.5331925749778748\n",
            " isotonic cone loss = 0.0160\n",
            "0.5636146664619446\n",
            " isotonic cone loss = 0.0166\n",
            "0.45264989137649536\n",
            " isotonic cone loss = 0.0161\n",
            "0.5606918334960938\n",
            " isotonic cone loss = 0.0162\n",
            "0.5637961626052856\n",
            " isotonic cone loss = 0.0162\n",
            "0.5230326652526855\n",
            " isotonic cone loss = 0.0158\n",
            "0.5539051294326782\n",
            " isotonic cone loss = 0.0158\n",
            "0.5776255130767822\n",
            " isotonic cone loss = 0.0159\n",
            "0.4893803894519806\n",
            " isotonic cone loss = 0.0159\n",
            "0.5696901679039001\n",
            " isotonic cone loss = 0.0159\n",
            "0.5395575165748596\n",
            " isotonic cone loss = 0.0162\n",
            "0.47001421451568604\n",
            " isotonic cone loss = 0.0161\n",
            "0.522986114025116\n",
            " isotonic cone loss = 0.0161\n",
            "0.4474906325340271\n",
            " isotonic cone loss = 0.0158\n",
            "0.5184249877929688\n",
            " isotonic cone loss = 0.0158\n",
            "0.43004322052001953\n",
            " isotonic cone loss = 0.0162\n",
            "0.5044558644294739\n",
            " isotonic cone loss = 0.0162\n",
            "0.48738959431648254\n",
            " isotonic cone loss = 0.0161\n",
            "0.4866143763065338\n",
            " isotonic cone loss = 0.0159\n",
            "0.5284388065338135\n",
            " isotonic cone loss = 0.0160\n",
            "0.4841272830963135\n",
            " isotonic cone loss = 0.0160\n",
            "0.5363840460777283\n",
            " isotonic cone loss = 0.0161\n",
            "0.5719444751739502\n",
            " isotonic cone loss = 0.0156\n",
            "0.6276857852935791\n",
            " isotonic cone loss = 0.0161\n",
            "0.5513569712638855\n",
            " isotonic cone loss = 0.0162\n",
            "0.683674156665802\n",
            " isotonic cone loss = 0.0161\n",
            "0.5599647164344788\n",
            " isotonic cone loss = 0.0160\n",
            "0.5781614780426025\n",
            " isotonic cone loss = 0.0157\n",
            "0.5711212754249573\n",
            " isotonic cone loss = 0.0160\n",
            "0.565356433391571\n",
            " isotonic cone loss = 0.0158\n",
            "0.654732346534729\n",
            " isotonic cone loss = 0.0161\n",
            "0.5789294242858887\n",
            " isotonic cone loss = 0.0159\n",
            "0.5640146732330322\n",
            " isotonic cone loss = 0.0156\n",
            "0.5387715101242065\n",
            " isotonic cone loss = 0.0160\n",
            "0.5726880431175232\n",
            " isotonic cone loss = 0.0163\n",
            "0.5018264651298523\n",
            " isotonic cone loss = 0.0161\n",
            "0.5831263661384583\n",
            " isotonic cone loss = 0.0164\n",
            "0.4416205883026123\n",
            " isotonic cone loss = 0.0161\n",
            "0.5506935119628906\n",
            " isotonic cone loss = 0.0154\n",
            "0.5815876126289368\n",
            " isotonic cone loss = 0.0153\n",
            "0.5593235492706299\n",
            " isotonic cone loss = 0.0156\n",
            "0.5524448156356812\n",
            " isotonic cone loss = 0.0157\n",
            "0.5199072957038879\n",
            " isotonic cone loss = 0.0163\n",
            "0.4837498068809509\n",
            " isotonic cone loss = 0.0160\n",
            "0.542466402053833\n",
            " isotonic cone loss = 0.0161\n",
            "0.5767353773117065\n",
            " isotonic cone loss = 0.0164\n",
            "0.5296378135681152\n",
            " isotonic cone loss = 0.0161\n",
            "0.45081934332847595\n",
            " isotonic cone loss = 0.0163\n",
            "0.5574701428413391\n",
            " isotonic cone loss = 0.0164\n",
            "0.446024090051651\n",
            " isotonic cone loss = 0.0162\n",
            "0.45559462904930115\n",
            " isotonic cone loss = 0.0160\n",
            "0.5128688812255859\n",
            " isotonic cone loss = 0.0162\n",
            "0.42155730724334717\n",
            " isotonic cone loss = 0.0159\n",
            "0.36611101031303406\n",
            " isotonic cone loss = 0.0163\n",
            "0.4248970150947571\n",
            " isotonic cone loss = 0.0159\n",
            "0.36710265278816223\n",
            " isotonic cone loss = 0.0159\n",
            "0.3566882312297821\n",
            " isotonic cone loss = 0.0161\n",
            "0.4791432321071625\n",
            " isotonic cone loss = 0.0162\n",
            "0.4183851480484009\n",
            " isotonic cone loss = 0.0164\n",
            "0.38823944330215454\n",
            " isotonic cone loss = 0.0162\n",
            "0.3694661259651184\n",
            " isotonic cone loss = 0.0160\n",
            "0.37261244654655457\n",
            " isotonic cone loss = 0.0164\n",
            "0.39812466502189636\n",
            " isotonic cone loss = 0.0165\n",
            "0.33303302526474\n",
            " isotonic cone loss = 0.0162\n",
            "0.5232248902320862\n",
            " isotonic cone loss = 0.0166\n",
            "0.390633761882782\n",
            " isotonic cone loss = 0.0162\n",
            "0.44385606050491333\n",
            " isotonic cone loss = 0.0164\n",
            "0.4265781342983246\n",
            " isotonic cone loss = 0.0162\n",
            "0.39406296610832214\n",
            " isotonic cone loss = 0.0166\n",
            "0.28956690430641174\n",
            " isotonic cone loss = 0.0164\n",
            "0.3271797299385071\n",
            " isotonic cone loss = 0.0166\n",
            "0.3032117187976837\n",
            " isotonic cone loss = 0.0160\n",
            "0.4405345618724823\n",
            " isotonic cone loss = 0.0163\n",
            "0.3832104802131653\n",
            " isotonic cone loss = 0.0159\n",
            "0.358285129070282\n",
            " isotonic cone loss = 0.0161\n",
            "0.5003650784492493\n",
            " isotonic cone loss = 0.0167\n",
            "0.3287321627140045\n",
            " isotonic cone loss = 0.0163\n",
            "0.43676283955574036\n",
            " isotonic cone loss = 0.0162\n",
            "0.5091028809547424\n",
            " isotonic cone loss = 0.0162\n",
            "0.3804069459438324\n",
            " isotonic cone loss = 0.0160\n",
            "0.314898282289505\n",
            " isotonic cone loss = 0.0165\n",
            "0.2924242913722992\n",
            " isotonic cone loss = 0.0164\n",
            "0.3730712831020355\n",
            " isotonic cone loss = 0.0163\n",
            "0.2840818464756012\n",
            " isotonic cone loss = 0.0165\n",
            "0.40044525265693665\n",
            " isotonic cone loss = 0.0165\n",
            "0.40600624680519104\n",
            " isotonic cone loss = 0.0165\n",
            "0.40232375264167786\n",
            " isotonic cone loss = 0.0163\n",
            "0.40296801924705505\n",
            " isotonic cone loss = 0.0165\n",
            "0.35201194882392883\n",
            " isotonic cone loss = 0.0162\n",
            "0.42105209827423096\n",
            " isotonic cone loss = 0.0159\n",
            "0.42409002780914307\n",
            " isotonic cone loss = 0.0163\n",
            "0.2764737010002136\n",
            " isotonic cone loss = 0.0160\n",
            "0.37894266843795776\n",
            " isotonic cone loss = 0.0164\n",
            "0.3479364514350891\n",
            " isotonic cone loss = 0.0163\n",
            "0.3928639888763428\n",
            " isotonic cone loss = 0.0165\n",
            "0.2690339982509613\n",
            " isotonic cone loss = 0.0164\n",
            "0.37962308526039124\n",
            " isotonic cone loss = 0.0167\n",
            "0.41449061036109924\n",
            " isotonic cone loss = 0.0165\n",
            "0.2779352068901062\n",
            " isotonic cone loss = 0.0168\n",
            "0.374458372592926\n",
            " isotonic cone loss = 0.0166\n",
            "0.3747738301753998\n",
            " isotonic cone loss = 0.0163\n",
            "0.46864891052246094\n",
            " isotonic cone loss = 0.0162\n",
            "0.46201321482658386\n",
            " isotonic cone loss = 0.0158\n",
            "0.5512625575065613\n",
            " isotonic cone loss = 0.0161\n",
            "0.3364831805229187\n",
            " isotonic cone loss = 0.0162\n",
            "0.3643304705619812\n",
            " isotonic cone loss = 0.0165\n",
            "0.4319480061531067\n",
            " isotonic cone loss = 0.0167\n",
            "0.350213885307312\n",
            " isotonic cone loss = 0.0164\n",
            "0.4366350471973419\n",
            " isotonic cone loss = 0.0167\n",
            "0.428587406873703\n",
            " isotonic cone loss = 0.0165\n",
            "0.47228386998176575\n",
            " isotonic cone loss = 0.0160\n",
            "0.3753893971443176\n",
            " isotonic cone loss = 0.0160\n",
            "0.4536982476711273\n",
            " isotonic cone loss = 0.0159\n",
            "0.40971121191978455\n",
            " isotonic cone loss = 0.0161\n",
            "0.3423503041267395\n",
            " isotonic cone loss = 0.0164\n",
            "0.39768823981285095\n",
            " isotonic cone loss = 0.0166\n",
            "0.40541112422943115\n",
            " isotonic cone loss = 0.0164\n",
            "0.3214034140110016\n",
            " isotonic cone loss = 0.0162\n",
            "0.43627315759658813\n",
            " isotonic cone loss = 0.0166\n",
            "0.41091156005859375\n",
            " isotonic cone loss = 0.0166\n",
            "0.34992510080337524\n",
            " isotonic cone loss = 0.0166\n",
            "0.4374437630176544\n",
            " isotonic cone loss = 0.0167\n",
            "0.2768397033214569\n",
            " isotonic cone loss = 0.0169\n",
            "0.2841719090938568\n",
            " isotonic cone loss = 0.0169\n",
            "0.4582594931125641\n",
            " isotonic cone loss = 0.0167\n",
            "0.48982465267181396\n",
            " isotonic cone loss = 0.0167\n",
            "0.4745870530605316\n",
            " isotonic cone loss = 0.0167\n",
            "0.532715380191803\n",
            " isotonic cone loss = 0.0168\n",
            "0.3936569392681122\n",
            " isotonic cone loss = 0.0163\n",
            "0.4409164488315582\n",
            " isotonic cone loss = 0.0163\n",
            "0.4315240979194641\n",
            " isotonic cone loss = 0.0162\n",
            "0.463942289352417\n",
            " isotonic cone loss = 0.0160\n",
            "0.3882589340209961\n",
            " isotonic cone loss = 0.0167\n",
            "0.3539258539676666\n",
            " isotonic cone loss = 0.0165\n",
            "0.34756091237068176\n",
            " isotonic cone loss = 0.0168\n",
            "0.330568790435791\n",
            " isotonic cone loss = 0.0164\n",
            "0.3714756667613983\n",
            " isotonic cone loss = 0.0160\n",
            "0.31892797350883484\n",
            " isotonic cone loss = 0.0163\n",
            "0.4197697639465332\n",
            " isotonic cone loss = 0.0164\n",
            "0.4025832712650299\n",
            " isotonic cone loss = 0.0163\n",
            "0.28285646438598633\n",
            " isotonic cone loss = 0.0165\n",
            "0.47452160716056824\n",
            " isotonic cone loss = 0.0165\n",
            "0.2909349203109741\n",
            " isotonic cone loss = 0.0164\n",
            "0.355324387550354\n",
            " isotonic cone loss = 0.0165\n",
            "0.3933428227901459\n",
            " isotonic cone loss = 0.0165\n",
            "0.34605127573013306\n",
            " isotonic cone loss = 0.0160\n",
            "0.417293518781662\n",
            " isotonic cone loss = 0.0165\n",
            "0.3284786641597748\n",
            " isotonic cone loss = 0.0160\n",
            "0.44755470752716064\n",
            " isotonic cone loss = 0.0164\n",
            "0.4896193742752075\n",
            " isotonic cone loss = 0.0161\n",
            "0.47983190417289734\n",
            " isotonic cone loss = 0.0163\n",
            "0.39639031887054443\n",
            " isotonic cone loss = 0.0163\n",
            "0.37290817499160767\n",
            " isotonic cone loss = 0.0165\n",
            "0.4075411260128021\n",
            " isotonic cone loss = 0.0164\n",
            "0.4149339199066162\n",
            " isotonic cone loss = 0.0163\n",
            "0.4354545474052429\n",
            " isotonic cone loss = 0.0166\n",
            "0.4518302083015442\n",
            " isotonic cone loss = 0.0159\n",
            "0.6496877670288086\n",
            " isotonic cone loss = 0.0162\n",
            "0.38580846786499023\n",
            " isotonic cone loss = 0.0162\n",
            "0.44037628173828125\n",
            " isotonic cone loss = 0.0164\n",
            "0.5292288661003113\n",
            " isotonic cone loss = 0.0167\n",
            "0.45339179039001465\n",
            " isotonic cone loss = 0.0164\n",
            "0.44462382793426514\n",
            " isotonic cone loss = 0.0162\n",
            "0.5766994953155518\n",
            " isotonic cone loss = 0.0162\n",
            "0.5076072812080383\n",
            " isotonic cone loss = 0.0158\n",
            "0.49154162406921387\n",
            " isotonic cone loss = 0.0159\n",
            "0.5324523448944092\n",
            " isotonic cone loss = 0.0161\n",
            "0.4310494363307953\n",
            " isotonic cone loss = 0.0159\n",
            "0.5362798571586609\n",
            " isotonic cone loss = 0.0161\n",
            "0.46606749296188354\n",
            " isotonic cone loss = 0.0161\n",
            "0.5219693183898926\n",
            " isotonic cone loss = 0.0162\n",
            "0.3889275789260864\n",
            " isotonic cone loss = 0.0164\n",
            "0.4202684760093689\n",
            " isotonic cone loss = 0.0165\n",
            "0.3945876955986023\n",
            " isotonic cone loss = 0.0167\n",
            "0.36060667037963867\n",
            " isotonic cone loss = 0.0167\n",
            "0.4296751618385315\n",
            " isotonic cone loss = 0.0160\n",
            "0.5133523344993591\n",
            " isotonic cone loss = 0.0159\n",
            "0.4811590015888214\n",
            " isotonic cone loss = 0.0161\n",
            "0.4241693913936615\n",
            " isotonic cone loss = 0.0161\n",
            "0.5198036432266235\n",
            " isotonic cone loss = 0.0156\n",
            "0.5336747169494629\n",
            " isotonic cone loss = 0.0159\n",
            "0.4597717225551605\n",
            " isotonic cone loss = 0.0159\n",
            "0.5653422474861145\n",
            " isotonic cone loss = 0.0164\n",
            "0.4344683289527893\n",
            " isotonic cone loss = 0.0161\n",
            "0.4020475447177887\n",
            " isotonic cone loss = 0.0161\n",
            "0.46583855152130127\n",
            " isotonic cone loss = 0.0159\n",
            "0.579892635345459\n",
            " isotonic cone loss = 0.0165\n",
            "0.4655691087245941\n",
            " isotonic cone loss = 0.0162\n",
            "0.5342150330543518\n",
            " isotonic cone loss = 0.0161\n",
            "0.4123010039329529\n",
            " isotonic cone loss = 0.0162\n",
            "0.5684197545051575\n",
            " isotonic cone loss = 0.0160\n",
            "0.4738253355026245\n",
            " isotonic cone loss = 0.0160\n",
            "0.43905109167099\n",
            " isotonic cone loss = 0.0159\n",
            "0.381437748670578\n",
            " isotonic cone loss = 0.0164\n",
            "0.44377371668815613\n",
            " isotonic cone loss = 0.0161\n",
            "0.45761245489120483\n",
            " isotonic cone loss = 0.0161\n",
            "0.4288969933986664\n",
            " isotonic cone loss = 0.0163\n",
            "0.5305632948875427\n",
            " isotonic cone loss = 0.0162\n",
            "0.38167980313301086\n",
            " isotonic cone loss = 0.0160\n",
            "0.4350677728652954\n",
            " isotonic cone loss = 0.0162\n",
            "0.46023616194725037\n",
            " isotonic cone loss = 0.0161\n",
            "0.4393818974494934\n",
            " isotonic cone loss = 0.0166\n",
            "0.39114582538604736\n",
            " isotonic cone loss = 0.0163\n",
            "0.487543523311615\n",
            " isotonic cone loss = 0.0159\n",
            "0.47080424427986145\n",
            " isotonic cone loss = 0.0161\n",
            "0.48101553320884705\n",
            " isotonic cone loss = 0.0163\n",
            "0.5179955959320068\n",
            " isotonic cone loss = 0.0160\n",
            "0.5355488657951355\n",
            " isotonic cone loss = 0.0162\n",
            "0.4109416902065277\n",
            " isotonic cone loss = 0.0159\n",
            "0.4459376037120819\n",
            " isotonic cone loss = 0.0161\n",
            "0.4193687438964844\n",
            " isotonic cone loss = 0.0159\n",
            "0.4999168813228607\n",
            " isotonic cone loss = 0.0163\n",
            "0.4770848751068115\n",
            " isotonic cone loss = 0.0161\n",
            "0.3771909773349762\n",
            " isotonic cone loss = 0.0158\n",
            "0.4150509536266327\n",
            " isotonic cone loss = 0.0161\n",
            "0.4717418849468231\n",
            " isotonic cone loss = 0.0161\n",
            "0.43844911456108093\n",
            " isotonic cone loss = 0.0164\n",
            "0.4982128143310547\n",
            " isotonic cone loss = 0.0162\n",
            "0.6593589186668396\n",
            " isotonic cone loss = 0.0164\n",
            "0.5968002080917358\n",
            " isotonic cone loss = 0.0160\n",
            "0.42406168580055237\n",
            " isotonic cone loss = 0.0158\n",
            "0.5608338713645935\n",
            " isotonic cone loss = 0.0156\n",
            "0.6092744469642639\n",
            " isotonic cone loss = 0.0156\n",
            "0.46536940336227417\n",
            " isotonic cone loss = 0.0160\n",
            "0.4910789430141449\n",
            " isotonic cone loss = 0.0157\n",
            "0.4571506679058075\n",
            " isotonic cone loss = 0.0157\n",
            "0.47741439938545227\n",
            " isotonic cone loss = 0.0163\n",
            "0.35115551948547363\n",
            " isotonic cone loss = 0.0161\n",
            "0.4230496883392334\n",
            " isotonic cone loss = 0.0161\n",
            "0.36672109365463257\n",
            " isotonic cone loss = 0.0164\n",
            "0.4536956548690796\n",
            " isotonic cone loss = 0.0163\n",
            "0.4727299213409424\n",
            " isotonic cone loss = 0.0156\n",
            "0.5591436624526978\n",
            " isotonic cone loss = 0.0165\n",
            "0.37163224816322327\n",
            " isotonic cone loss = 0.0160\n",
            "0.5243142247200012\n",
            " isotonic cone loss = 0.0160\n",
            "0.38877183198928833\n",
            " isotonic cone loss = 0.0157\n",
            "0.4495633840560913\n",
            " isotonic cone loss = 0.0161\n",
            "0.4036564528942108\n",
            " isotonic cone loss = 0.0160\n",
            "0.4225291609764099\n",
            " isotonic cone loss = 0.0163\n",
            "0.34614700078964233\n",
            " isotonic cone loss = 0.0161\n",
            "0.38178354501724243\n",
            " isotonic cone loss = 0.0164\n",
            "0.34106937050819397\n",
            " isotonic cone loss = 0.0163\n",
            "0.501671552658081\n",
            " isotonic cone loss = 0.0161\n",
            "0.4237077832221985\n",
            " isotonic cone loss = 0.0161\n",
            "0.3575677275657654\n",
            " isotonic cone loss = 0.0161\n",
            "0.45858675241470337\n",
            " isotonic cone loss = 0.0158\n",
            "0.37688174843788147\n",
            " isotonic cone loss = 0.0160\n",
            "0.5461542010307312\n",
            " isotonic cone loss = 0.0159\n",
            "0.43844103813171387\n",
            " isotonic cone loss = 0.0163\n",
            "0.4570958912372589\n",
            " isotonic cone loss = 0.0158\n",
            "0.45972537994384766\n",
            " isotonic cone loss = 0.0162\n",
            "0.48340827226638794\n",
            " isotonic cone loss = 0.0156\n",
            "0.5885181427001953\n",
            " isotonic cone loss = 0.0158\n",
            "0.4597887396812439\n",
            " isotonic cone loss = 0.0160\n",
            "0.48697587847709656\n",
            " isotonic cone loss = 0.0165\n",
            "0.35109949111938477\n",
            " isotonic cone loss = 0.0163\n",
            "0.45285674929618835\n",
            " isotonic cone loss = 0.0164\n",
            "0.5142425298690796\n",
            " isotonic cone loss = 0.0160\n",
            "0.40658193826675415\n",
            " isotonic cone loss = 0.0166\n",
            "0.4671211242675781\n",
            " isotonic cone loss = 0.0161\n",
            "0.4559488594532013\n",
            " isotonic cone loss = 0.0163\n",
            "0.31834083795547485\n",
            " isotonic cone loss = 0.0164\n",
            "0.4030584990978241\n",
            " isotonic cone loss = 0.0162\n",
            "0.3724144399166107\n",
            " isotonic cone loss = 0.0163\n",
            "0.4170779287815094\n",
            " isotonic cone loss = 0.0159\n",
            "0.45610418915748596\n",
            " isotonic cone loss = 0.0154\n",
            "0.3925096094608307\n",
            " isotonic cone loss = 0.0162\n",
            "0.3775411546230316\n",
            " isotonic cone loss = 0.0158\n",
            "0.4319675862789154\n",
            " isotonic cone loss = 0.0161\n",
            "0.35964903235435486\n",
            " isotonic cone loss = 0.0162\n",
            "0.396582692861557\n",
            " isotonic cone loss = 0.0160\n",
            "0.2614823877811432\n",
            " isotonic cone loss = 0.0161\n",
            "0.40314701199531555\n",
            " isotonic cone loss = 0.0162\n",
            "0.4884125590324402\n",
            " isotonic cone loss = 0.0162\n",
            "0.35585418343544006\n",
            " isotonic cone loss = 0.0162\n",
            "0.4130256474018097\n",
            " isotonic cone loss = 0.0164\n",
            "0.4589587152004242\n",
            " isotonic cone loss = 0.0160\n",
            "0.3977285325527191\n",
            " isotonic cone loss = 0.0160\n",
            "0.5663996338844299\n",
            " isotonic cone loss = 0.0162\n",
            "0.5276011824607849\n",
            " isotonic cone loss = 0.0157\n",
            "0.5161341428756714\n",
            " isotonic cone loss = 0.0158\n",
            "0.4891907572746277\n",
            " isotonic cone loss = 0.0158\n",
            "0.46518516540527344\n",
            " isotonic cone loss = 0.0158\n",
            "0.5556784868240356\n",
            " isotonic cone loss = 0.0161\n",
            "0.5589354038238525\n",
            " isotonic cone loss = 0.0162\n",
            "0.4502173066139221\n",
            " isotonic cone loss = 0.0164\n",
            "0.49962377548217773\n",
            " isotonic cone loss = 0.0159\n",
            "0.4931476414203644\n",
            " isotonic cone loss = 0.0159\n",
            "0.45862269401550293\n",
            " isotonic cone loss = 0.0164\n",
            "0.3989821672439575\n",
            " isotonic cone loss = 0.0159\n",
            "0.5153660774230957\n",
            " isotonic cone loss = 0.0160\n",
            "0.5540221929550171\n",
            " isotonic cone loss = 0.0159\n",
            "0.4752722978591919\n",
            " isotonic cone loss = 0.0160\n",
            "0.4757421612739563\n",
            " isotonic cone loss = 0.0159\n",
            "0.4845145046710968\n",
            " isotonic cone loss = 0.0160\n",
            "0.41448521614074707\n",
            " isotonic cone loss = 0.0160\n",
            "0.43247997760772705\n",
            " isotonic cone loss = 0.0167\n",
            "0.43127599358558655\n",
            " isotonic cone loss = 0.0165\n",
            "0.3846455216407776\n",
            " isotonic cone loss = 0.0162\n",
            "0.4293796420097351\n",
            " isotonic cone loss = 0.0161\n",
            "0.44592341780662537\n",
            " isotonic cone loss = 0.0164\n",
            "0.47775545716285706\n",
            " isotonic cone loss = 0.0160\n",
            "0.5322946906089783\n",
            " isotonic cone loss = 0.0159\n",
            "0.5076613426208496\n",
            " isotonic cone loss = 0.0160\n",
            "0.42834654450416565\n",
            " isotonic cone loss = 0.0163\n",
            "0.5520691275596619\n",
            " isotonic cone loss = 0.0161\n",
            "0.43287393450737\n",
            " isotonic cone loss = 0.0159\n",
            "0.42912566661834717\n",
            " isotonic cone loss = 0.0160\n",
            "0.503309965133667\n",
            " isotonic cone loss = 0.0157\n",
            "0.479696124792099\n",
            " isotonic cone loss = 0.0160\n",
            "0.377118319272995\n",
            " isotonic cone loss = 0.0162\n",
            "0.37777796387672424\n",
            " isotonic cone loss = 0.0162\n",
            "0.4914933741092682\n",
            " isotonic cone loss = 0.0161\n",
            "0.4453604817390442\n",
            " isotonic cone loss = 0.0162\n",
            "0.41863638162612915\n",
            " isotonic cone loss = 0.0159\n",
            "0.4798548221588135\n",
            " isotonic cone loss = 0.0155\n",
            "0.4257538318634033\n",
            " isotonic cone loss = 0.0162\n",
            "0.4268554747104645\n",
            " isotonic cone loss = 0.0163\n",
            "0.4033452272415161\n",
            " isotonic cone loss = 0.0158\n",
            "0.3821803629398346\n",
            " isotonic cone loss = 0.0161\n",
            "0.44894227385520935\n",
            " isotonic cone loss = 0.0162\n",
            "0.37775179743766785\n",
            " isotonic cone loss = 0.0160\n",
            "0.5302806496620178\n",
            " isotonic cone loss = 0.0159\n",
            "0.3751111924648285\n",
            " isotonic cone loss = 0.0162\n",
            "0.41024067997932434\n",
            " isotonic cone loss = 0.0162\n",
            "0.33529582619667053\n",
            " isotonic cone loss = 0.0164\n",
            "0.3785170614719391\n",
            " isotonic cone loss = 0.0163\n",
            "0.40368136763572693\n",
            " isotonic cone loss = 0.0163\n",
            "0.4027895927429199\n",
            " isotonic cone loss = 0.0160\n",
            "0.46579939126968384\n",
            " isotonic cone loss = 0.0159\n",
            "0.4267319142818451\n",
            " isotonic cone loss = 0.0160\n",
            "0.43516233563423157\n",
            " isotonic cone loss = 0.0159\n",
            "0.5088332295417786\n",
            " isotonic cone loss = 0.0162\n",
            "0.5785719156265259\n",
            " isotonic cone loss = 0.0161\n",
            "0.3698404133319855\n",
            " isotonic cone loss = 0.0161\n",
            "0.4480491876602173\n",
            " isotonic cone loss = 0.0161\n",
            "0.5099275708198547\n",
            " isotonic cone loss = 0.0160\n",
            "0.35755956172943115\n",
            " isotonic cone loss = 0.0161\n",
            "0.42954909801483154\n",
            " isotonic cone loss = 0.0161\n",
            "0.46364304423332214\n",
            " isotonic cone loss = 0.0159\n",
            "0.48391905426979065\n",
            " isotonic cone loss = 0.0162\n",
            "0.437766432762146\n",
            " isotonic cone loss = 0.0163\n",
            "0.35739660263061523\n",
            " isotonic cone loss = 0.0157\n",
            "0.3383597135543823\n",
            " isotonic cone loss = 0.0164\n",
            "0.44047704339027405\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4105206561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4105206561.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m## Compute auxiliary belief persistence loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrank_future_sequence_loss_soft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss_order\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mordered_future_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3997292960.py\u001b[0m in \u001b[0;36mrank_future_sequence_loss_soft\u001b[0;34m(logits, targets, max_future_steps, decay, temperature, reduction)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mΔ\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_future_steps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 6) Train / eval functions\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb[0], yb[0]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits , loss = model(xb,yb)\n",
        "        B, T, V = logits.shape\n",
        "        loss = loss\n",
        "        # Backprop\n",
        "        ## Compute auxiliary belief persistence loss\n",
        "        loss_rank = 1e-3 * rank_future_sequence_loss_soft(logits, yb)\n",
        "        loss_order= 1e-3 * ordered_future_loss(logits, yb)\n",
        "\n",
        "        print(f\" isotonic cone loss = {loss_rank.item() + loss_order.item():.4f}\")\n",
        "\n",
        "        # Total loss (tune the scale if needed)\n",
        "        loss = loss  + loss_rank + loss_order\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        print(loss.item())\n",
        "        total_loss += loss.item()\n",
        "        losses.append(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "# 7) Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss = train_epoch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nf2029jSJl9P",
        "outputId": "3fb00dcc-2482-4d15-ea87-46bab3028296"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a7190aae8d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c_XtPYjAb2M",
        "outputId": "d08ea56e-99fe-4573-cef5-78227ba9ec2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: What light through yonder window breaks?\n",
            "Juliet, Doth you love me? well down to-night.\n",
            "\n",
            "ROMEO:\n",
            "What dear hath villain less thou do we fly?\n",
            "\n",
            "RETVERD:\n",
            "No doubt, no must be: every wisdom you hence:\n",
            "And say the manner of you be admind company:\n",
            "And yet her lord, you leave me and I\n",
            "Was but nose with which you do not suffer them,\n",
            "That thus I found, you go along doors.\n",
            "\n",
            "JULIET:\n",
            "Under you conjure my daught I warrant give.\n",
            "Lo, thou art patraches you firm against them\n",
            "To be as it so well being my tongue\n",
            "To signify the true Iriop stiff myrave,\n",
            "Some shadows you the broke in throwly san,\n",
            "To answer twenty herb upon myself:\n",
            "If this burn down to waitor to love;\n",
            "And in good commons to France of Pompey;\n",
            "So the mayor is bed to put a years,\n",
            "Preprett in English base,\n",
            "As to take last with wind \n",
            "Weeps in the sudden call my curst!\n",
            "\n",
            "PETRUCHIO:\n",
            "What did short should be be sine and Duke\n",
            "That discature is not for it with a grew?\n",
            "\n",
            "LUCIO:\n",
            "A while: the duke is children's son, Sir for London,\n",
            "whose duke church dow on the days by my bosom:\n",
            "The day of balls, and learn approse\n",
            "Must ondest Apoll, and their wester's themself:\n",
            "O, he bids us best die, but thought of all,\n",
            "And call appear, and one that calles from the light.\n",
            "\n",
            "First Murderer:\n",
            "Our wishing she were under holy tongue woe:\n",
            "Make way, bald, O, how lion-law, for this lay bear;\n",
            "He is, by Marcius, 'tis break of his new,\n",
            "Who is laid of all, dear Marcius, 'tis last.\n",
            "\n",
            "Third Citizen, 'tis true turn this unhold,\n",
            "Who in his sovereign, how lies not light one,\n",
            "May give me like a sea-light be appow'd by the sea--\n",
            "As hear his day, but 'ems divorced him took\n",
            "The days of billing humour! who's the dead follow\n",
            "Abours beggan, for earth am! for his sighs, bring heart\n",
            "To call me resisted Absoquet, that he dares me!\n",
            "Your grace methought that valiant liege,\n",
            "Have way-light shall into my brow heart--\n",
            "How here's heart? What, how means here told,\n",
            "He's dis-like, and not like messerve a lame.\n",
            "Lo, field thou canst unfortune amen are gone:\n",
            "And\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "def decode_chars(token_ids, itos):\n",
        "    \"\"\"\n",
        "    Decodes a list of character token IDs into a string.\n",
        "    \"\"\"\n",
        "    return ''.join([itos[i] for i in token_ids])\n",
        "\n",
        "def encode_chars(text, stoi):\n",
        "    \"\"\"\n",
        "    Encodes a string into a list of token IDs, one per character.\n",
        "    \"\"\"\n",
        "    return [stoi.get(c, 0) for c in text]\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def decode_sequence_char(\n",
        "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
        "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Character-level decoding from a prompt using the model's logits.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    start_ids = encode_chars(prompt, stoi)\n",
        "    idx = torch.tensor([start_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        context = idx[:, -block_size:]\n",
        "        logits, _ = model(context,None)\n",
        "        last_logits = logits[:, -1, :]\n",
        "\n",
        "\n",
        "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
        "\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "    return decode_chars(idx[0].tolist(), itos)\n",
        "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi = meta[\"stoi\"]\n",
        "itos = meta[\"itos\"]\n",
        "\n",
        "prompt = \"ROMEO: What light through yonder window breaks?\\nJuliet, Doth you love me?\"\n",
        "generated = decode_sequence_char(\n",
        "    model=model,\n",
        "    stoi=stoi,\n",
        "    itos=itos,\n",
        "    prompt=prompt,\n",
        "    max_new_tokens=1900,\n",
        "    block_size=1024,\n",
        "    use_fenchel=False,\n",
        "    tau=1.5,\n",
        "    fenchel_iters=2,\n",
        "    temperature=0.4\n",
        ")\n",
        "\n",
        "print(generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "VRHZquqsWBbL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def decode_chars(token_ids, itos):\n",
        "    \"\"\"\n",
        "    Decodes a list of character token IDs into a string.\n",
        "    \"\"\"\n",
        "    return ''.join([itos[i] for i in token_ids])\n",
        "\n",
        "def encode_chars(text, stoi):\n",
        "    \"\"\"\n",
        "    Encodes a string into a list of token IDs, one per character.\n",
        "    \"\"\"\n",
        "    return [stoi.get(c, 0) for c in text]\n",
        "def decode_sequence_char_greedy(model, stoi, itos, prompt, max_new_tokens=100, block_size=256):\n",
        "    \"\"\"\n",
        "    Deterministic (greedy) character-level decoding for exact memorization checks.\n",
        "    This avoids torch.multinomial() entirely.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    start_ids = encode_chars(prompt, stoi)\n",
        "    idx = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        context = idx[:, -block_size:]\n",
        "        logits, _ = model(context, None)\n",
        "        last_logits = logits[:, -1, :]\n",
        "        next_token = torch.argmax(last_logits, dim=-1, keepdim=True)  # greedy\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "    return decode_chars(idx[0].tolist(), itos)\n",
        "\n",
        "# Usage:\n",
        "prompt = \"Good, speak to the mariners. Fall to, yarely, or we run ourselves aground. Bestir, bestir!\"\n",
        "generated = decode_sequence_char_greedy(\n",
        "    model=model,\n",
        "    stoi=stoi,\n",
        "    itos=itos,\n",
        "    prompt=prompt,\n",
        "    max_new_tokens=2048,\n",
        "    block_size=1024\n",
        ")\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmCa0c92XsUk",
        "outputId": "e98ef511-0886-4f1d-881f-147a6772dd03"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good, speak to the mariners. Fall to, yarely, or we run ourselves aground. Bestir, bestir!\n",
            "\n",
            "KING HENRY VI:\n",
            "Such a night of EDY BRDye herb he doth perfort him,\n",
            "For her our love a prince have nor senume three\n",
            "Deriving five shrow and wide of five this day.\n",
            "3 GREMIO:\n",
            "And after his prince and low in etern,\n",
            "To sight our sweet pluinted spirit,\n",
            "Shall be reve highness of my tale, and married,\n",
            "So for that sends not my what is free.\n",
            "\n",
            "KATHARINA:\n",
            "Good morrow breaks his son are espect;\n",
            "For Warwick, in that ever ways, at alone.\n",
            "Now is my widdes nor Grosan is the woman,\n",
            "That we must have touch with 'Gainst our seal,\n",
            "And should come friends the frost thou art.\n",
            "\n",
            "KING HENRY VI:\n",
            "Touch'd you the cush of Marcius Salent good fees?\n",
            "\n",
            "KING RICHARD II:\n",
            "Well said is far contrary, soft them as they with the\n",
            "duke of the charity.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Whom? why, you can well? Edward is Been?\n",
            "Farewell, be break'd broach tarry shoose.\n",
            "\n",
            "LADY MARGARET:\n",
            "A wretched for these arms be long strike.\n",
            "\n",
            "KING RICHARD II:\n",
            "O, good, my royal husband so were from thence.\n",
            "\n",
            "BAPTIO:\n",
            "And find over may be dear well of life,\n",
            "That this pray 'I'll see 'twas Marcius will be done:\n",
            "My Juliet be, let Extilus it dares:\n",
            "How now, my let liege, did one divour bless\n",
            "To lie now the privy thousand flowers.\n",
            "\n",
            "BAGENT:\n",
            "And fear you, fair Marcius is Romeo's mother:\n",
            "He will be day for all; for that cannot do:\n",
            "Remore the queen, for that he dares not live,\n",
            "That 'twains value in the lasts, 'tis breaker me,\n",
            "And how he follows in the senument to live,\n",
            "Whose soldiers up her love: who go finds by\n",
            "The days of life's daughter. Do, my great Marcius\n",
            "Load Alas, the duke holy of the king.\n",
            "\n",
            "LADY ANNE:\n",
            "All met the prouds be just said his strike,\n",
            "Would too loave the thanks and full of life:\n",
            "The dag AMARLIO:\n",
            "Our sights Marcius, Marcius, Warwick,\n",
            "Whose bagot by Absence his bar, Warwick,\n",
            "Whose took Warwick shall be dwick'd out\n",
            "As hearty that may brib he dark:\n",
            "Why, help Messel, gentle lare guilty--\n",
            "Would be made you sit amen, for thought uptore,\n",
            "That patient letters the nor lies up, but jumpso!\n",
            "\n",
            "Lord:\n",
            "Let A sold, and go well be marriage:\n",
            "Who prays beside, 'twain, 'tis a should be so,\n",
            "From heart-bac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz8NUsW4Qdfj",
        "outputId": "6617c9bf-7a84-4f62-9213-3922767e8248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6614656"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fIPU3pjCQcqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEZgPXclyKlb",
        "outputId": "a13082b8-2ff3-4cfb-9e51-a8f9fff9499b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jacobian rank (final token, projected): 66\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import grad\n",
        "'''\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        embeddings = torch.cat([self.transformer.wte[i](idx)  for i in range(self.n_head)], dim=-1)\n",
        "        embeddings = embeddings + pos_emb\n",
        "        x = self.transformer.drop(embeddings)\n",
        "        x_orig = x.clone()\n",
        "        for stage in self.transformer.h:  # stages are ExplorerEngineerStage\n",
        "          x = stage(x, x_orig)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "'''\n",
        "\n",
        "\n",
        "def compute_jacobian_rank(model, idx, projection_dim=324):\n",
        "    \"\"\"\n",
        "    Compute the rank of the Jacobian of projected logits w.r.t. input embeddings\n",
        "    for the final token position only, with output space optionally projected.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = idx.device\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    b, t = idx.size()\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "    emb = torch.cat([model.transformer.wte[i](idx)  for i in range(model.n_head)], dim=-1)\n",
        "    emb = emb + pos_emb\n",
        "    emb.requires_grad_(True)\n",
        "\n",
        "\n",
        "    # Forward pass using embedding input\n",
        "    def forward_emb(emb_input):\n",
        "        xorig = emb_input.clone()\n",
        "        x = model.transformer.drop(emb_input)\n",
        "        accum = torch.zeros_like(x)\n",
        "        for i, block in enumerate(model.transformer.h):\n",
        "            x= block(x,xorig)\n",
        "\n",
        "        x = model.transformer.ln_f(x)\n",
        "        logits = model.lm_head(x)\n",
        "        return logits[:, -1, :projection_dim]  # final token, projected\n",
        "\n",
        "    output = forward_emb(emb)\n",
        "    jacobian_rows = []\n",
        "\n",
        "    for i in range(output.shape[-1]):\n",
        "        grad_output = torch.zeros_like(output)\n",
        "        grad_output[:, i] = 1.0\n",
        "        grad_i = grad(output, emb, grad_outputs=grad_output, retain_graph=True)[0]\n",
        "        row = grad_i[:, -1, :].detach().cpu().numpy()  # final token, last dim\n",
        "        jacobian_rows.append(row.squeeze())\n",
        "\n",
        "    J = np.stack(jacobian_rows, axis=0)  # shape: [proj_dim, emb_dim]\n",
        "    rank = np.linalg.matrix_rank(J)\n",
        "    return rank\n",
        "\n",
        "\n",
        "def compute_drift_trajectories(model, idx):\n",
        "    \"\"\"\n",
        "    Compute drift vectors Δh = h_{l+1} - h_l across transformer layers\n",
        "    at each token position.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = idx.device\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    b, t = idx.size()\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "    emb = torch.cat([model.transformer.wte[i](idx)  for i in range(model.n_head)], dim=-1)\n",
        "    x = emb + pos_emb\n",
        "    xorig = x.clone()\n",
        "\n",
        "    layers = []\n",
        "    dy = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for block in model.transformer.h:\n",
        "            x= block(x,xorig)\n",
        "            layers.append(x.clone())\n",
        "\n",
        "    drifts = [layers[i+1] - layers[i] for i in range(len(layers)-1)]\n",
        "    drift_norms = [d.norm(dim=-1).mean(dim=-1).cpu().numpy() for d in drifts]\n",
        "    return drift_norms\n",
        "\n",
        "\n",
        "def plot_drift(drift_norms):\n",
        "    \"\"\"\n",
        "    Plot average drift norm per layer.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot([np.mean(d) for d in drift_norms], marker='o')\n",
        "    plt.title(\"Average Drift Norm Across Layers\")\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(\"‖Δh‖ mean\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_ids = torch.randint(0, model.config.vocab_size, (1, model.config.n_head*2)).to(device)\n",
        "\n",
        "jac_rank = compute_jacobian_rank(model, input_ids)\n",
        "drift_norms = compute_drift_trajectories(model, input_ids)\n",
        "plot_drift(drift_norms)\n",
        "\n",
        "print(\"Jacobian rank (final token, projected):\", jac_rank)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKXFCMBzyKlc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def collect_drift_matrix(model, idx):\n",
        "    \"\"\"\n",
        "    Compute drift vectors Δh = h_{l+1} - h_l across transformer layers\n",
        "    at each token position.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = idx.device\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    b, t = idx.size()\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "    emb = torch.cat([model.transformer.wte[i](idx)  for i in range(model.n_head)], dim=-1)\n",
        "    x = emb + pos_emb\n",
        "    xorig = x.clone()\n",
        "\n",
        "    states = []\n",
        "    dy = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for block in model.transformer.h:\n",
        "            x= block(x,xorig)\n",
        "            states.append(x.clone())\n",
        "\n",
        "    drifts = [states[i+1] - states[i] for i in range(len(states)-1)]  # list of tensors [B,T,C]\n",
        "    drift_matrix = torch.cat([d.reshape(-1, d.shape[-1]).cpu() for d in drifts], dim=0)\n",
        "    return drift_matrix  # shape: [N, D]\n",
        "\n",
        "def run_drift_pca(drift_matrix, k=40):\n",
        "    \"\"\"\n",
        "    Run PCA and report explained variance for top-k components.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=k)\n",
        "    pca.fit(drift_matrix.numpy())\n",
        "    explained = pca.explained_variance_ratio_\n",
        "    return explained, pca\n",
        "\n",
        "def plot_explained_variance(explained):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(np.cumsum(explained) * 100, marker='o')\n",
        "    plt.xlabel(\"Principal Component\")\n",
        "    plt.ylabel(\"Cumulative Variance Explained (%)\")\n",
        "    plt.title(\"Drift Trajectory PCA: Explained Variance\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZThjWOQeyKlc"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "import seaborn as sns\n",
        "\n",
        "def get_projected_residuals(drift_matrix, pca):\n",
        "    \"\"\"\n",
        "    Project Δh onto PCA space and get residuals.\n",
        "    \"\"\"\n",
        "    proj = pca.transform(drift_matrix.numpy())\n",
        "    recon = pca.inverse_transform(proj)\n",
        "    residuals = drift_matrix.numpy() - recon\n",
        "    return proj, residuals\n",
        "\n",
        "def fit_gmm(proj_data, k=4):\n",
        "    \"\"\"\n",
        "    Fit GMM to PCA-projected drift vectors to identify latent regimes.\n",
        "    \"\"\"\n",
        "    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=0)\n",
        "    gmm.fit(proj_data)\n",
        "    labels = gmm.predict(proj_data)\n",
        "    return gmm, labels\n",
        "\n",
        "def plot_gmm_clusters(proj_data, labels):\n",
        "    \"\"\"\n",
        "    Plot GMM clustering over first 2 PCA components.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=proj_data[:, 0], y=proj_data[:, 1], hue=labels, palette=\"tab10\", s=10)\n",
        "    plt.title(\"Latent Regimes from Drift PCA (GMM Clusters)\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUPNo1flyKlc",
        "outputId": "7155fabd-2238-4941-ceb9-5748d67c7634"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Inputs\n",
        "input_ids = torch.randint(0, model.config.vocab_size, (1, model.config.n_head*2)).to(device)\n",
        "\n",
        "# Drift matrix\n",
        "drift_matrix = collect_drift_matrix(model, input_ids)\n",
        "\n",
        "# PCA and variance\n",
        "explained, pca = run_drift_pca(drift_matrix, k=40)\n",
        "plot_explained_variance(explained)\n",
        "\n",
        "# Residuals and GMM regime fit\n",
        "proj, residuals = get_projected_residuals(drift_matrix, pca)\n",
        "gmm, labels = fit_gmm(proj, k=4)\n",
        "plot_gmm_clusters(proj, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_2KT6IcyKlc",
        "outputId": "f2594511-2436-4ad0-bde3-66c33d7b6911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Baseline] Val loss: 1.8266\n",
            "\n",
            "--- Per-Layer Ablation Report ---\n",
            "Ablate Layer  0: Loss = 3.8196 | Δ = 1.9930\n",
            "Ablate Layer  1: Loss = 1.8639 | Δ = 0.0372\n",
            "Ablate Layer  2: Loss = 2.3641 | Δ = 0.5375\n",
            "Ablate Layer  3: Loss = 2.9427 | Δ = 1.1161\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from contextlib import contextmanager\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==============================\n",
        "# 🔁 LAYER ABLATION LOGIC BLOCK\n",
        "# =============================\n",
        "\n",
        "# === Config ===\n",
        "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
        "block_size = 256\n",
        "batch_size = 1\n",
        "# === Load tokenizer metadata ===\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# === Load mmap data (char-level tokens, uint16) ===\n",
        "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
        "\n",
        "# === Efficient GPU Batch Sampler ===\n",
        "class GPUBatchDataset(Dataset):\n",
        "    def __init__(self, mmap_file, block_size, batch_size, device):\n",
        "        self.data = mmap_file\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.total = len(self.data) - block_size - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "        for i in range(self.batch_size):\n",
        "            start = np.random.randint(0, self.total // self.block_size) * self.block_size\n",
        "            X[i] = self.data[start : start + self.block_size]\n",
        "            Y[i] = self.data[start + 1 : start + 1 + self.block_size]\n",
        "        return (\n",
        "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
        "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
        "        )\n",
        "\n",
        "val_dataset = GPUBatchDataset(val_ids, block_size, batch_size=1, device=device)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "losses = []\n",
        "import types\n",
        "import inspect\n",
        "@contextmanager\n",
        "def layer_ablation_context(model, layers_to_ablate):\n",
        "    \"\"\"Context manager that ablates selected layers by making them identity maps\n",
        "    with respect to the primary stream `x`, while still accepting `x_orig`.\"\"\"\n",
        "    original_forwards = [block.forward for block in model.transformer.h]\n",
        "\n",
        "    def make_identity_forward(block, original_fn):\n",
        "        # Keep the same call shape; return the incoming x unchanged.\n",
        "        # Works whether the block expects (x) or (x, x_orig, ...).\n",
        "        def ablated_forward(self, *args, **kwargs):\n",
        "            # Bound method: args do NOT include self.\n",
        "            # We expect at least x in args or kwargs.\n",
        "            if len(args) >= 1:\n",
        "                x = args[0]\n",
        "                return x\n",
        "            # Fallback if caller used kwargs\n",
        "            if \"x\" in kwargs:\n",
        "                return kwargs[\"x\"]\n",
        "            # If signature is unusual, defer to original to avoid crash ?1\n",
        "            return original_fn(*args, **kwargs)\n",
        "        return types.MethodType(ablated_forward, block)\n",
        "\n",
        "    try:\n",
        "        for i, block in enumerate(model.transformer.h):\n",
        "            if i in layers_to_ablate:\n",
        "                model.transformer.h[i].forward = make_identity_forward(block, original_forwards[i])\n",
        "        yield\n",
        "    finally:\n",
        "        for i, block in enumerate(model.transformer.h):\n",
        "            model.transformer.h[i].forward = original_forwards[i]\n",
        "@torch.no_grad()\n",
        "def eval_epoch(max_batches=50):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        xb, yb = xb[0], yb[0]\n",
        "        logits, _ = model(xb, yb)\n",
        "        B, T, V = logits.shape\n",
        "        total_loss += criterion(logits.view(B * T, V),\n",
        "                                yb.view(B * T)).item()\n",
        "    return total_loss / max_batches\n",
        "\n",
        "# ---- Evaluate baseline without any ablation ----\n",
        "with layer_ablation_context(model, layers_to_ablate=set()):\n",
        "    baseline_val_loss = eval_epoch()\n",
        "print(f\"[Baseline] Val loss: {baseline_val_loss:.4f}\")\n",
        "\n",
        "# ---- Run per-layer ablation safely ----\n",
        "print(\"\\n--- Per-Layer Ablation Report ---\")\n",
        "results = []\n",
        "for i in range(model.config.n_layer):\n",
        "    with layer_ablation_context(model, layers_to_ablate={i}):\n",
        "        loss = eval_epoch()\n",
        "    delta = loss - baseline_val_loss\n",
        "    results.append((i, loss, delta))\n",
        "    print(f\"Ablate Layer {i:2d}: Loss = {loss:.4f} | Δ = {delta:.4f}\")\n",
        "\n",
        "# ---- Plotting ----\n",
        "layer_ids = [i for i, _, _ in results]\n",
        "delta_vals = [delta for _, _, delta in results]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(layer_ids, delta_vals, marker='o')\n",
        "plt.title(\"Δ Loss from Per-Layer Ablation\")\n",
        "plt.xlabel(\"Layer Index\")\n",
        "plt.ylabel(\"Δ Validation Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZSEf8RyKlc",
        "outputId": "18ccaa53-a533-4aab-a281-14256761fff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Probe: ['why', 'what', 'where', 'when'] vs who\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "Module [ModuleList] is missing the required \"forward\" function",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m words, outlier \u001b[38;5;129;01min\u001b[39;00m test_sets:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔍 Probe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutlier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mplot_parallelogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutlier\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mplot_parallelogram\u001b[39m\u001b[34m(words, outlier)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_parallelogram\u001b[39m(words, outlier=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     vecs = [\u001b[43mget_char_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m     28\u001b[39m     labels = words.copy()\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m outlier:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mget_char_embedding\u001b[39m\u001b[34m(word)\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     emb = \u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [1, T, d]\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m emb.mean(dim=\u001b[32m1\u001b[39m).squeeze(\u001b[32m0\u001b[39m).cpu().numpy()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:399\u001b[39m, in \u001b[36m_forward_unimplemented\u001b[39m\u001b[34m(self, *input)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, *\u001b[38;5;28minput\u001b[39m: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[33;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m \u001b[33;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    400\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] is missing the required \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m function\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    401\u001b[39m     )\n",
            "\u001b[31mNotImplementedError\u001b[39m: Module [ModuleList] is missing the required \"forward\" function"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# === Load vocab ===\n",
        "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "\n",
        "stoi = meta[\"stoi\"]\n",
        "itos = meta[\"itos\"]\n",
        "device = next(model.parameters()).device\n",
        "wte = model.transformer.wte\n",
        "\n",
        "# === Char embedding extractor ===\n",
        "def get_char_embedding(word):\n",
        "    ids = [stoi[c] for c in word if c in stoi]\n",
        "    if not ids:\n",
        "        return None\n",
        "    with torch.no_grad():\n",
        "        emb = wte(torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0))  # [1, T, d]\n",
        "        return emb.mean(dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "# === Plot one parallelogram ===\n",
        "def plot_parallelogram(words, outlier=None):\n",
        "    vecs = [get_char_embedding(w) for w in words]\n",
        "    labels = words.copy()\n",
        "    if outlier:\n",
        "        vecs.append(get_char_embedding(outlier))\n",
        "        labels.append(outlier)\n",
        "\n",
        "    # drop None values\n",
        "    vecs = [v for v in vecs if v is not None]\n",
        "    if len(vecs) < 4:\n",
        "        print(f\"⚠️ Not enough valid embeddings for: {labels}\")\n",
        "        return\n",
        "\n",
        "    vecs = np.stack(vecs)\n",
        "    proj = PCA(n_components=2).fit_transform(vecs)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(proj[:, 0], proj[:, 1], color='black')\n",
        "    for i, label in enumerate(labels):\n",
        "        plt.text(proj[i, 0], proj[i, 1], label, fontsize=10)\n",
        "\n",
        "    if len(proj) >= 4:\n",
        "        A, B, C, D = proj[:4]\n",
        "        plt.plot([A[0], B[0]], [A[1], B[1]], 'r--')\n",
        "        plt.plot([C[0], D[0]], [C[1], D[1]], 'r--')\n",
        "        plt.plot([A[0], C[0]], [A[1], C[1]], 'b--')\n",
        "        plt.plot([B[0], D[0]], [B[1], D[1]], 'b--')\n",
        "\n",
        "    plt.title(\"Parallelogram Probe\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Define test sets ===\n",
        "test_sets = [\n",
        "    ([\"why\", \"what\", \"where\", \"when\"], \"who\"),               # verb→third person\n",
        "    ([\"prepare\", \"begin\", \"cease\", \"end\"], \"remain\"),               # singular→plural\n",
        "    ([\"happy\", \"sad\", \"disagreeable\", \"willing\"], \"afraid\"),       # negation prefix\n",
        "    ([\"king\", \"queen\", \"man\", \"woman\"], \"apple\"),            # gender pairs\n",
        "    ([\"king\", \"knight\", \"man\", \"maiden\"], \"book\"),\n",
        "    ([\"to\", \"from\", \"in\", \"out\"], \"the\"),                    # function-word symmetry\n",
        "]\n",
        "\n",
        "# === Run all probes ===\n",
        "for words, outlier in test_sets:\n",
        "    print(f\"\\n🔍 Probe: {words} vs {outlier}\")\n",
        "    plot_parallelogram(words, outlier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "97LSWZHnyKlc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "c80671b8-2cd6-4887-e5d3-9c97a72f1060"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2242291821.py, line 54)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2242291821.py\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    for cat in\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import torch, pickle, matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# ---------- load vocab ----------\n",
        "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "\n",
        "# ---------- model bits ----------\n",
        "device = next(model.parameters()).device\n",
        "model.eval()                      # <— turn off dropout everywhere\n",
        "wte = model.transformer.wte\n",
        "wte.eval()                        #   (StairEmbed also has dropout)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_char_embedding(word):\n",
        "    ids = [stoi[c] for c in word if c in stoi]\n",
        "    if len(ids) < 2:              # ← need ≥2 chars for HailFire geometry\n",
        "        return None\n",
        "    with torch.no_grad():\n",
        "        t = torch.tensor(ids, device=device).unsqueeze(0)      # [1, T]\n",
        "        emb = wte(t).mean(dim=1).squeeze(0)                    # (d,)\n",
        "        return emb.cpu().numpy()\n",
        "\n",
        "# ---------- categories ----------\n",
        "categories = {\n",
        "    \"Verbs\":        [\"play\",\"run\",\"eat\",\"sleep\",\"jump\",\"talk\",\"walk\"],\n",
        "    \"Nouns\":        [\"dog\",\"tree\",\"car\",\"book\",\"child\",\"house\",\"apple\"],\n",
        "    \"FunctionWords\":[\"the\",\"and\",\"in\",\"on\",\"to\",\"of\",\"a   \"],\n",
        "    \"Punctuation\":  [\".    \",\",    \",\"!    \",\"?    \"],\n",
        "}\n",
        "\n",
        "# ---------- gather ----------\n",
        "vecs, labels, words = [], [], []\n",
        "for cat, wl in categories.items():\n",
        "    for w in wl:\n",
        "        v = get_char_embedding(w)\n",
        "        if v is not None:\n",
        "            vecs.append(v)\n",
        "            labels.append(cat)\n",
        "            words.append(w)\n",
        "\n",
        "vecs = np.stack(vecs)             # ← now guaranteed rectangular\n",
        "\n",
        "# ---------- PCA ----------\n",
        "proj = PCA(n_components=2).fit_transform(vecs)\n",
        "\n",
        "# ---------- plot ----------\n",
        "colors = dict(Verbs=\"blue\", Nouns=\"red\",\n",
        "              FunctionWords=\"green\", Punctuation=\"purple\")\n",
        "\n",
        "plt.figure(figsize=(9,7))\n",
        "for cat in\n",
        " categories:\n",
        "    idx = [i for i,l in enumerate(labels) if l==cat]\n",
        "    plt.scatter(proj[idx,0], proj[idx,1], label=cat, color=colors[cat])\n",
        "    for i in idx:\n",
        "        plt.text(proj[i,0], proj[i,1], words[i], fontsize=9)\n",
        "\n",
        "plt.title(\"Word-class clusters (char-averaged embeddings)\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "plt.show()\n",
        "def coop2_fn(R: torch.Tensor, C: torch.Tensor,n) -> torch.Tensor:\n",
        "    x = R**2 + 2*R + C * (1 + R.abs())- R * (1 + C.abs())\n",
        "    return torch.clamp(x, min=-n-1, max=n+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoFwC55KyKld"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}