{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/PyITD/blob/main/Artificial_General_Intelligence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "p4-dcQlo_qoG"
      },
      "outputs": [],
      "source": [
        "#Ashem vohu vahishtem asti ushta asti ushta ahmai yad ashai vahishtai ashem\n",
        "#copyright joshuah.rainstar@gmail.com 2025\n",
        "#some concepts borrowed from various papers.\n",
        "#add_hypersphere_phase_heads by me.\n",
        "#use my code and my ideas- but if you have money, I want some. I need a new car and a house.\n",
        "\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "\n",
        "def add_hypersphere_phase_heads(x: torch.Tensor, num_segs: int, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x: [B,T,C]\n",
        "    num_segs: number of segments to divide C dimension into\n",
        "    First segment gets no phase info, subsequent segments get lag 1, 2, 3, etc.\n",
        "    \"\"\"\n",
        "    B, T, C = x.shape\n",
        "    if T == 0 or num_segs <= 0:\n",
        "        return x\n",
        "\n",
        "    assert C % num_segs == 0, f\"C ({C}) must be divisible by num_segs ({num_segs})\"\n",
        "    seg_size = C // num_segs\n",
        "\n",
        "    # Reshape to treat segments as separate heads: [B,T,C] -> [B,num_segs,T,seg_size]\n",
        "    x_reshaped = x.view(B, T, num_segs, seg_size).transpose(1, 2)  # [B,num_segs,T,seg_size]\n",
        "\n",
        "    # Skip first segment, process segments 1 onwards with lags 1, 2, 3, ...\n",
        "    if num_segs > 1:\n",
        "        # Create lags: [1, 2, 3, ..., num_segs-1] for segments 1, 2, 3, ..., num_segs-1\n",
        "        L_h = torch.arange(1, num_segs, device=x.device, dtype=torch.long)  # [num_segs-1]\n",
        "\n",
        "        # Process only segments 1 onwards\n",
        "        x_to_process = x_reshaped[:, 1:]  # [B, num_segs-1, T, seg_size]\n",
        "\n",
        "        v = F.normalize(x_to_process, dim=-1, eps=eps)  # [B,num_segs-1,T,seg_size]\n",
        "\n",
        "        t = torch.arange(T, device=x.device)  # [T]\n",
        "        src = (t.unsqueeze(0) - L_h.view(-1, 1)).clamp_min(0)  # [num_segs-1,T]\n",
        "\n",
        "        # gather anchors per segment and time\n",
        "        anchor = v.gather(\n",
        "            dim=2,\n",
        "            index=src.view(1, num_segs-1, T, 1).expand(B, num_segs-1, T, seg_size)\n",
        "        )\n",
        "        cosA = (v * anchor).sum(dim=-1).clamp(-1.0 + eps, 1.0 - eps)  # [B,num_segs-1,T]\n",
        "        scalar = (cosA / max(seg_size, 1)).unsqueeze(-1)  # [B,num_segs-1,T,1]\n",
        "\n",
        "        # Add scalar to processed segments\n",
        "        x_modified = x_to_process + scalar  # [B,num_segs-1,T,seg_size]\n",
        "\n",
        "        # Combine unchanged first segment with modified segments\n",
        "        x_reshaped = torch.cat([x_reshaped[:, :1], x_modified], dim=1)  # [B,num_segs,T,seg_size]\n",
        "\n",
        "    # Reshape back to original format: [B,num_segs,T,seg_size] -> [B,T,C]\n",
        "    return x_reshaped.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "\n",
        "class KalmanSweepMHGainsOptimal(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimal form with:\n",
        "    - Sink mechanism via adaptive R\n",
        "    - Shared V: first chunk broadcast to all heads\n",
        "    - Fused operations for efficiency\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg, n_passes: int = 12, init_logQ: float = -2.0, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.n_head = cfg.n_head\n",
        "        self.dh = cfg.n_embd // cfg.n_head\n",
        "        self.n_passes = n_passes\n",
        "        self.eps = eps\n",
        "\n",
        "        # Single fused projection for all operations\n",
        "        # Now outputs 5 components: [H_diag, y, R_diag, A_modulation, sink_gate]\n",
        "        self.fused_proj = nn.Linear(3 * self.dh, 5 * self.dh)\n",
        "\n",
        "        # Base transition (can be modulated)\n",
        "        self.A_base = nn.Parameter(torch.eye(self.dh))\n",
        "\n",
        "        # Process noise\n",
        "        self.logQ = nn.Parameter(torch.full((self.n_head, self.dh), init_logQ))\n",
        "\n",
        "        # Scales\n",
        "        self.scales = nn.Parameter(torch.ones(3, self.n_head))  # H_scale, R_scale, sink_scale\n",
        "\n",
        "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, D = Q.shape\n",
        "        H, dh = self.n_head, self.dh\n",
        "\n",
        "        # Extract first chunk of V and broadcast to all heads\n",
        "        V_reshaped = V.view(B, N, H, dh)  # [B, N, H, dh]\n",
        "        V_shared = V_reshaped[:, :, 0:1, :].expand(B, N, H, dh)  # Broadcast first head to all\n",
        "        V_broadcast = V_shared.contiguous().view(B, N, D)  # [B, N, D]\n",
        "\n",
        "        # Use original Q, K but shared V for projection\n",
        "        QKV = torch.cat([Q, K, V_broadcast], dim=-1).view(B * N * H, 3 * dh)\n",
        "\n",
        "        # Fused projection and activation\n",
        "        out = self.fused_proj(QKV).view(B, N, H, 5 * dh)\n",
        "        H_raw, y, R_raw, A_mod, sink_raw = out.chunk(5, dim=-1)\n",
        "\n",
        "        # Apply activations and scales\n",
        "        H_diag = torch.sigmoid(H_raw) * self.scales[0].view(1, 1, H, 1)\n",
        "\n",
        "        # Sink mechanism: when sink_gate â†’ 0, R â†’ âˆž (ignore input)\n",
        "        sink_gate = torch.sigmoid(sink_raw) * self.scales[2].view(1, 1, H, 1)\n",
        "        R_base = F.softplus(R_raw) * self.scales[1].view(1, 1, H, 1) + self.eps\n",
        "        R_diag = R_base / (sink_gate + 0.01)  # When sink_gateâ†’0, R becomes very large\n",
        "\n",
        "        # Modulated transition matrix\n",
        "        A_mod_sigmoid = torch.sigmoid(A_mod).view(B * N * H, dh, 1)\n",
        "        A = self.A_base.unsqueeze(0) * A_mod_sigmoid  # (B*N*H, dh, dh)\n",
        "\n",
        "        # Pre-compute constants\n",
        "        Q_diag = torch.exp(self.logQ).clamp(min=self.eps).view(1, 1, H, dh)\n",
        "\n",
        "        # Initialize with cold start\n",
        "        P = torch.ones(B, N, H, dh, device=Q.device)\n",
        "        HP = H_diag * P\n",
        "        S = HP * H_diag + R_diag\n",
        "        K_gain = HP / S\n",
        "\n",
        "        if self.n_passes == 1:\n",
        "            return K_gain.reshape(B, N, D)\n",
        "\n",
        "        # State for multi-pass\n",
        "        x = K_gain * y\n",
        "        P = P - K_gain * HP\n",
        "\n",
        "        # Parallel passes\n",
        "        for pass_idx in range(1, self.n_passes):\n",
        "            # Shift and transform in one operation\n",
        "            x_flat = x[:, :-1].reshape(B * (N-1) * H, dh)\n",
        "            x_pred_flat = torch.bmm(\n",
        "                A[:B*(N-1)*H].view(B*(N-1)*H, dh, dh),\n",
        "                x_flat.unsqueeze(-1)\n",
        "            ).squeeze(-1).view(B, N-1, H, dh)\n",
        "\n",
        "            x_prev = torch.cat([\n",
        "                torch.zeros(B, 1, H, dh, device=x.device),\n",
        "                x_pred_flat\n",
        "            ], dim=1)\n",
        "\n",
        "            P_prev = torch.cat([\n",
        "                torch.ones(B, 1, H, dh, device=P.device),\n",
        "                P[:, :-1] + Q_diag\n",
        "            ], dim=1)\n",
        "\n",
        "            # Parallel Kalman update\n",
        "            HP = H_diag * P_prev\n",
        "            S = HP * H_diag + R_diag\n",
        "            K_gain = HP / S\n",
        "\n",
        "            innov = y - H_diag * x_prev\n",
        "            x = x_prev + K_gain * innov\n",
        "            P = P_prev - K_gain * HP\n",
        "\n",
        "        return K_gain.reshape(B, N, D)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------- DynMix --\n",
        "class DynMix(nn.Module):\n",
        "    \"\"\"\n",
        "    Symplectic mixer for â‰¥3 tensors (B,T,E).  See original author for details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, step: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.h = float(step)\n",
        "\n",
        "    @staticmethod\n",
        "    def _coop(R: torch.Tensor, C: torch.Tensor, h: float) -> torch.Tensor:\n",
        "        w = torch.sigmoid((R * C).sum(dim=-1, keepdim=True) / (2 * R.size(-1) ** 0.5))\n",
        "        k1 = w * (C - R)\n",
        "        k2 = w * (C - (R + h * k1))\n",
        "        return R + 0.5 * h * (k1 + k2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _mix_list(xs: List[torch.Tensor], h: float) -> List[torch.Tensor]:\n",
        "        n = len(xs)\n",
        "        if n < 3:\n",
        "            raise ValueError(\"Need at least 3 components\")\n",
        "        stacked = torch.stack(xs, 0)\n",
        "        total = stacked.sum(0, keepdim=False)\n",
        "        out = []\n",
        "        for i in range(n):\n",
        "            others_mean = (total - stacked[i]) / (n - 1)\n",
        "            out.append(DynMix._coop(stacked[i], others_mean, h))\n",
        "        return out\n",
        "\n",
        "    def forward(self, comps: List[torch.Tensor], loop_iters: int = 2) -> List[torch.Tensor]:\n",
        "        for _ in range(loop_iters):\n",
        "            comps = DynMix._mix_list(comps, self.h)\n",
        "        return comps\n",
        "\n",
        "\n",
        "\n",
        "class CausalKalman(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.cal = KalmanSweepMHGainsOptimal(cfg=config)\n",
        "        self.dynmix = DynMix()\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # QKV projection\n",
        "        qkv = self.c_attn(x)  # [B, T, 3*C]\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        s = [q,k,v]\n",
        "        s = self.dynmix(s) #apply mixing\n",
        "        q, k, v = s\n",
        "        s = self.cal(q,k,v)\n",
        "\n",
        "        return x*s\n",
        "\n",
        "\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"RoPE implementation for better positional encoding\"\"\"\n",
        "    def __init__(self, dim, max_seq_len=8192, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.base = base\n",
        "\n",
        "        # Precompute frequencies\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, q, k):\n",
        "        seq_len = q.size(-2)\n",
        "        device = q.device\n",
        "\n",
        "        # Generate position indices\n",
        "        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.outer(t, self.inv_freq)  # [seq_len, dim//2]\n",
        "\n",
        "        # Create rotation matrix\n",
        "        cos = freqs.cos().to(q.dtype)\n",
        "        sin = freqs.sin().to(q.dtype)\n",
        "\n",
        "        # Apply rotation\n",
        "        q_rot = self._apply_rope(q, cos, sin)\n",
        "        k_rot = self._apply_rope(k, cos, sin)\n",
        "\n",
        "        return q_rot, k_rot\n",
        "\n",
        "    def _apply_rope(self, x, cos, sin):\n",
        "        # x: [B, H, T, head_dim]\n",
        "        # cos, sin: [T, head_dim//2]\n",
        "\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]  # Split even/odd dimensions\n",
        "        cos = cos.view(1, 1, cos.size(0), cos.size(1))\n",
        "        sin = sin.view(1, 1, sin.size(0), sin.size(1))\n",
        "\n",
        "        # Apply rotation\n",
        "        rotated = torch.stack([\n",
        "            x1 * cos - x2 * sin,\n",
        "            x1 * sin + x2 * cos\n",
        "        ], dim=-1).flatten(-2)\n",
        "\n",
        "        return rotated\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "        # Sink tokens - learnable parameters for attention stabilization\n",
        "        self.sinks = nn.Parameter(torch.randn(config.n_head) * 0.02)\n",
        "\n",
        "        # Use flash attention when available, but handle sinks manually\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "\n",
        "        # RoPE embeddings (optional improvement)\n",
        "        self.use_rope = True\n",
        "        if self.use_rope:\n",
        "            self.rope = RotaryPositionalEmbedding(self.head_dim)\n",
        "\n",
        "        # For non-flash attention fallback\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                               .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def _create_causal_mask(self, T, device, dtype):\n",
        "        \"\"\"Create causal mask avoiding -inf to prevent NaNs\"\"\"\n",
        "        mask = torch.tril(torch.ones(T, T, device=device, dtype=dtype))\n",
        "        # Use large negative value instead of -inf to avoid NaN issues\n",
        "        large_neg = torch.finfo(dtype).min / 2\n",
        "        return mask.masked_fill(mask == 0, large_neg)\n",
        "\n",
        "    def _sink_stabilized_softmax(self, attn, sinks, dim=-1):\n",
        "        \"\"\"Softmax with sink stabilization to prevent attention collapse\"\"\"\n",
        "        # attn: [B, H, T, T]\n",
        "        # sinks: [H] -> [1, H, 1, 1]\n",
        "\n",
        "        max_logits = torch.amax(attn, dim=dim, keepdim=True)  # [B, H, T, 1]\n",
        "        sinks_expanded = sinks.view(1, -1, 1, 1)  # [1, H, 1, 1]\n",
        "\n",
        "        # Stabilizer is max of attention logits and sink values\n",
        "        stabilizer = torch.maximum(max_logits, sinks_expanded)\n",
        "\n",
        "        # Compute exp scores\n",
        "        exp_attn = torch.exp(attn - stabilizer)\n",
        "        exp_sinks = torch.exp(sinks_expanded - stabilizer)\n",
        "\n",
        "        # Normalize with sink contribution\n",
        "        normalizer = exp_attn.sum(dim=dim, keepdim=True) + exp_sinks\n",
        "        probs = exp_attn / normalizer\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # QKV projection\n",
        "        qkv = self.c_attn(x)  # [B, T, 3*C]\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B, H, T, head_dim]\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B, H, T, head_dim]\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B, H, T, head_dim]\n",
        "\n",
        "        # Apply RoPE if enabled\n",
        "        q, k = self.rope(q, k)\n",
        "\n",
        "        # Attention computation\n",
        "        if self.flash and not self.training:  # Use flash attention for inference only\n",
        "            # For flash attention, we need to handle sinks differently\n",
        "            # This is a simplified approach - you might need custom CUDA kernels for full optimization\n",
        "            y = F.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=0.0,  # Flash attention handles dropout internally\n",
        "                is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            # Manual attention with sink stabilization\n",
        "            attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, H, T, T]\n",
        "\n",
        "            # Apply causal mask (avoiding -inf)\n",
        "            causal_mask = self._create_causal_mask(T, x.device, attn.dtype)\n",
        "            attn = attn + causal_mask.view(1, 1, T, T)\n",
        "\n",
        "            # Apply sink-stabilized softmax\n",
        "            probs = self._sink_stabilized_softmax(attn, self.sinks)\n",
        "            probs = self.attn_dropout(probs)\n",
        "\n",
        "            # Apply attention to values\n",
        "            y = torch.matmul(probs, v)  # [B, H, T, head_dim]\n",
        "\n",
        "        # Reshape back to original format\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)  # Replace MLP with MMMLP\n",
        "        self.n_head = config.n_head\n",
        "        # output projection\n",
        "    def forward(self, x):\n",
        "        x1 = self.ln_1(x)\n",
        "        x1 = add_hypersphere_phase_heads(x1, self.n_head)\n",
        "        x1 = self.attn(x1)\n",
        "        x = x + x1\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class BlockKal(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.kal = CausalKalman(config)\n",
        "        # output projection\n",
        "    def forward(self, x):\n",
        "        x1 = self.ln_1(x)\n",
        "        x1 = self.kal(x1)\n",
        "        return x1\n",
        "\n",
        "class ExplorerEngineerStage(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.explorer= BlockKal(config)   # H-module\n",
        "        self.engineer = Block(config)   # H-module\n",
        "        self.n_head = config.n_head\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        residual: torch.Tensor,\n",
        "    ):\n",
        "\n",
        "        landmarks = self.explorer(x)\n",
        "        mapping = self.engineer(x+landmarks)\n",
        "        return mapping\n",
        "\n",
        "# ---------------- PATHFINDER (end-to-end) ----------------\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 2\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.ModuleList([nn.Embedding(config.vocab_size, config.n_embd//config.n_head) for _ in range(config.n_head)]),\n",
        "\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([ExplorerEngineerStage(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "        # forward the GPT model itself\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        embeddings = torch.cat([self.transformer.wte[i](idx)  for i in range(self.n_head)], dim=-1)\n",
        "        embeddings = embeddings + pos_emb\n",
        "        x = self.transformer.drop(embeddings)\n",
        "        x_orig = x.clone()\n",
        "        for stage in self.transformer.h:  # stages are ExplorerEngineerStage\n",
        "            x = stage(x, x_orig)\n",
        "\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "\n",
        "            # Total loss = main loss + weighted load balance loss\n",
        "\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFGVJvlN_yfW",
        "outputId": "0346e42e-e768-4782-82c8-fa9f7f940295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Downloading aochildes.txt...\n",
            "ðŸ“¥ Downloading cbt.txt...\n",
            "ðŸ“¥ Downloading children_stories.txt...\n",
            "ðŸ“¥ Downloading gutenberg.txt...\n",
            "ðŸ“¥ Downloading qed.txt...\n",
            "ðŸ“¥ Downloading simple_wikipedia.txt...\n",
            "ðŸ“¥ Downloading switchboard.txt...\n",
            "ðŸ“¥ Downloading wikipedia.txt...\n",
            "ðŸ“¥ Downloading shakespeare.txt...\n",
            "âœ… Done. Files saved to ./babylm_10m_cleaned\n"
          ]
        }
      ],
      "source": [
        "import requests, os\n",
        "\n",
        "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
        "target_dir = \"./babylm_10m_cleaned\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "file_names = [\n",
        "    \"aochildes.txt\",\n",
        "    \"cbt.txt\",\n",
        "    \"children_stories.txt\",\n",
        "    \"gutenberg.txt\",\n",
        "    \"qed.txt\",\n",
        "    \"simple_wikipedia.txt\",\n",
        "    \"switchboard.txt\",\n",
        "    \"wikipedia.txt\"\n",
        "]\n",
        "\n",
        "# Optional addition: Shakespeare from another dataset\n",
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
        "shakespeare_fname = \"shakespeare.txt\"\n",
        "\n",
        "# Combined download logic\n",
        "all_files = [(base_url + fname, fname) for fname in file_names]\n",
        "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
        "\n",
        "\n",
        "# Download loop\n",
        "for url, fname in all_files:\n",
        "    out_path = os.path.join(target_dir, fname)\n",
        "    print(f\"ðŸ“¥ Downloading {fname}...\")\n",
        "    resp = requests.get(url)\n",
        "    if resp.status_code == 200:\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(resp.text)\n",
        "    else:\n",
        "        print(f\"âŒ Failed to download {fname} ({resp.status_code})\")\n",
        "\n",
        "print(f\"âœ… Done. Files saved to {target_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0fFuL2a_sAF",
        "outputId": "6f4308d5-6efd-4dcc-8c17-55433a23e968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Char tokenizer finalized.\n",
            "ðŸ§¾ Train tokens: 1016242 | Val tokens: 99152\n",
            "ðŸ”¤ Vocab size: 66\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# === Paths ===\n",
        "source_dir = \"./babylm_10m_cleaned\"\n",
        "out_dir    = \"./babylm_char_tokenized\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "file_names = [\n",
        "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
        "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\",\"shakespeare.txt\"\n",
        "]\n",
        "\n",
        "# === Load and split ===\n",
        "train_texts, val_texts = [], []\n",
        "char_set = set()\n",
        "\n",
        "for fname in file_names:\n",
        "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        n = len(lines)\n",
        "        split = int(0.9 * n)\n",
        "        train_part = \"\".join(lines[:split])\n",
        "        val_part   = \"\".join(lines[split:])\n",
        "        train_texts.append(train_part)\n",
        "        val_texts.append(val_part)\n",
        "        char_set.update(train_part)\n",
        "        char_set.update(val_part)\n",
        "\n",
        "full_train = \"\\n\".join(train_texts)\n",
        "full_val   = \"\\n\".join(val_texts)\n",
        "\n",
        "# === Final vocab ===\n",
        "char_set = sorted(set(char_set))\n",
        "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "# === Encode function ===\n",
        "def encode(text):\n",
        "    return [stoi.get(c, 0) for c in text]\n",
        "\n",
        "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
        "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
        "\n",
        "# === Save ===\n",
        "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
        "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
        "\n",
        "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"vocab_size\": len(stoi),\n",
        "        \"stoi\": stoi,\n",
        "        \"itos\": itos\n",
        "    }, f)\n",
        "\n",
        "print(f\"âœ… Char tokenizer finalized.\")\n",
        "print(f\"ðŸ§¾ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
        "print(f\"ðŸ”¤ Vocab size: {len(stoi)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42l_Fa8_v9z",
        "outputId": "12065c68-bb39-4a2d-b066-54cc932acce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 4.00M\n",
            "4265024\n",
            "4.206038951873779\n",
            "3.448025941848755\n",
            "3.264317035675049\n",
            "3.2458746433258057\n",
            "2.965972423553467\n",
            "2.864518880844116\n",
            "2.7491366863250732\n",
            "2.778557062149048\n",
            "2.7185423374176025\n",
            "2.709573268890381\n",
            "2.7286391258239746\n",
            "2.6885392665863037\n",
            "2.6743569374084473\n",
            "2.673966646194458\n",
            "2.6194071769714355\n",
            "2.6455554962158203\n",
            "2.594837188720703\n",
            "2.6015074253082275\n",
            "2.5868191719055176\n",
            "2.6008496284484863\n",
            "2.5479509830474854\n",
            "2.586310386657715\n",
            "2.545546531677246\n",
            "2.5570316314697266\n",
            "2.526249408721924\n",
            "2.5643632411956787\n",
            "2.495905876159668\n",
            "2.512298345565796\n",
            "2.5494487285614014\n",
            "2.521327018737793\n",
            "2.540677785873413\n",
            "2.5330417156219482\n",
            "2.510721206665039\n",
            "2.481621742248535\n",
            "2.505736827850342\n",
            "2.5187361240386963\n",
            "2.4647376537323\n",
            "2.448183059692383\n",
            "2.439758777618408\n",
            "2.451565742492676\n",
            "2.439084053039551\n",
            "2.4305715560913086\n",
            "2.4737255573272705\n",
            "2.40793514251709\n",
            "2.3882827758789062\n",
            "2.391331434249878\n",
            "2.41064453125\n",
            "2.376721143722534\n",
            "2.3731486797332764\n",
            "2.3479416370391846\n",
            "2.3733973503112793\n",
            "2.3073792457580566\n",
            "2.3042359352111816\n",
            "2.3022170066833496\n",
            "2.3260414600372314\n",
            "2.288222551345825\n",
            "2.36149263381958\n",
            "2.308286428451538\n",
            "2.3094542026519775\n",
            "2.256849765777588\n",
            "2.2798001766204834\n",
            "2.2519404888153076\n",
            "2.2972946166992188\n",
            "2.273413896560669\n",
            "2.2586214542388916\n",
            "2.3069846630096436\n",
            "2.2630844116210938\n",
            "2.223113775253296\n",
            "2.2299013137817383\n",
            "2.2184574604034424\n",
            "2.2295637130737305\n",
            "2.21138858795166\n",
            "2.270139455795288\n",
            "2.2396209239959717\n",
            "2.1802566051483154\n",
            "2.2654242515563965\n",
            "2.2135426998138428\n",
            "2.140995979309082\n",
            "2.203136920928955\n",
            "2.177183151245117\n",
            "2.235588550567627\n",
            "2.173445701599121\n",
            "2.174604654312134\n",
            "2.1713695526123047\n",
            "2.172771453857422\n",
            "2.1523494720458984\n",
            "2.1524391174316406\n",
            "2.1686346530914307\n",
            "2.1797540187835693\n",
            "2.1831791400909424\n",
            "2.173769235610962\n",
            "2.155505895614624\n",
            "2.1666524410247803\n",
            "2.1390371322631836\n",
            "2.12260103225708\n",
            "2.1237809658050537\n",
            "2.086301326751709\n",
            "2.110696315765381\n",
            "2.061690330505371\n",
            "2.103318452835083\n",
            "2.124110460281372\n",
            "2.0504257678985596\n",
            "2.062467336654663\n",
            "2.1010193824768066\n",
            "2.0667593479156494\n",
            "2.074388027191162\n",
            "2.05666446685791\n",
            "2.078428030014038\n",
            "2.08160662651062\n",
            "2.0421438217163086\n",
            "2.0353150367736816\n",
            "2.0504226684570312\n",
            "2.0409231185913086\n",
            "2.0364832878112793\n",
            "2.020883560180664\n",
            "2.0625195503234863\n",
            "2.029634952545166\n",
            "2.0474350452423096\n",
            "2.0531702041625977\n",
            "1.991943120956421\n",
            "2.0485870838165283\n",
            "2.063946485519409\n",
            "2.0415496826171875\n",
            "2.0596423149108887\n",
            "1.9962443113327026\n",
            "2.0159220695495605\n",
            "1.986625075340271\n",
            "1.96168053150177\n",
            "1.9973698854446411\n",
            "1.964037299156189\n",
            "2.0475142002105713\n",
            "1.979137659072876\n",
            "1.9553160667419434\n",
            "1.9638547897338867\n",
            "1.9735881090164185\n",
            "1.9956583976745605\n",
            "1.9473323822021484\n",
            "1.9566566944122314\n",
            "1.9526499509811401\n",
            "1.9631493091583252\n",
            "1.9433188438415527\n",
            "1.9255788326263428\n",
            "1.9262275695800781\n",
            "1.9308778047561646\n",
            "1.917672872543335\n",
            "1.9458500146865845\n",
            "1.9829695224761963\n",
            "1.883345365524292\n",
            "1.9624526500701904\n",
            "1.8550342321395874\n",
            "1.8763880729675293\n",
            "1.9105781316757202\n",
            "1.8103771209716797\n",
            "1.9068819284439087\n",
            "1.9113924503326416\n",
            "1.8736648559570312\n",
            "1.9248332977294922\n",
            "1.8816142082214355\n",
            "1.8689074516296387\n",
            "1.913778305053711\n",
            "1.8671472072601318\n",
            "1.9253120422363281\n",
            "1.8665330410003662\n",
            "1.8745075464248657\n",
            "1.86403489112854\n",
            "1.8489012718200684\n",
            "1.8893976211547852\n",
            "1.8697607517242432\n",
            "1.861415982246399\n",
            "1.8590965270996094\n",
            "1.8216657638549805\n",
            "1.8627445697784424\n",
            "1.8514705896377563\n",
            "1.8605488538742065\n",
            "1.8197271823883057\n",
            "1.870615005493164\n",
            "1.830155849456787\n",
            "1.8307714462280273\n",
            "1.8471264839172363\n",
            "1.852713942527771\n",
            "1.8365815877914429\n",
            "1.8146100044250488\n",
            "1.8606714010238647\n",
            "1.85010826587677\n",
            "1.8209141492843628\n",
            "1.8329554796218872\n",
            "1.817978024482727\n",
            "1.772660255432129\n",
            "1.837830662727356\n",
            "1.7848870754241943\n",
            "1.7771660089492798\n",
            "1.8081415891647339\n",
            "1.781917691230774\n",
            "1.8106495141983032\n",
            "1.7978605031967163\n",
            "1.7281314134597778\n",
            "1.7584456205368042\n",
            "1.7875988483428955\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# === Config ===\n",
        "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
        "block_size = 1024\n",
        "batch_size = 8\n",
        "\n",
        "# === Load tokenizer metadata ===\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# === Load mmap data (char-level tokens, uint16) ===\n",
        "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
        "\n",
        "# === Efficient GPU Batch Sampler ===\n",
        "class GPUBatchDataset(Dataset):\n",
        "    def __init__(self, mmap_file, block_size, batch_size, device, jitter=63, p_aligned=0.5):\n",
        "        self.data = mmap_file\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.total = len(self.data) - block_size - 1\n",
        "        self.n_blocks = self.total // self.block_size\n",
        "        self.jitter = int(jitter)          # small random offset added to aligned start\n",
        "        self.p_aligned = float(p_aligned)  # mix aligned and jittered\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # choose a base aligned block\n",
        "            base_block = np.random.randint(0, self.n_blocks)\n",
        "            start = base_block * self.block_size\n",
        "\n",
        "            # with probability, add a small jitter (keeps cache-friendly contiguous reads)\n",
        "            if np.random.rand() > self.p_aligned:\n",
        "                j = np.random.randint(0, self.jitter + 1)\n",
        "                start = min(start + j, self.total)  # stay in range\n",
        "\n",
        "            X[i] = self.data[start : start + self.block_size]\n",
        "            Y[i] = self.data[start + 1 : start + 1 + self.block_size]\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
        "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
        "        )\n",
        "\n",
        "# === DataLoader ===\n",
        "train_dataset = GPUBatchDataset(train_ids, block_size, batch_size, device)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "# === Model ===\n",
        "config = GPTConfig(\n",
        "    vocab_size=len(stoi),\n",
        "    n_layer=4,\n",
        "    n_embd=256,\n",
        "    n_head=16,\n",
        "    dropout=0.0,\n",
        "    block_size=block_size\n",
        ")\n",
        "model = GPT(config)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "print(len(torch.nn.utils.parameters_to_vector(model.parameters())))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "losses = []\n",
        "\n",
        "# === Training Loop ===\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "          xb, yb = xb[0], yb[0]  # unwrap batch dimension\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits, loss = model(xb, yb)\n",
        "          loss = loss\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "          losses.append(loss.item())\n",
        "          print(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# === Run Training ===\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval_ce(mmap_file, n_batches=200):\n",
        "    model.eval()\n",
        "    T = block_size\n",
        "    losses = []\n",
        "    for _ in range(n_batches):\n",
        "        start = np.random.randint(0, len(mmap_file) - T - 2)  # any position, not aligned\n",
        "        X = torch.from_numpy(np.array(mmap_file[start:start+T], dtype=np.int64)).unsqueeze(0).to(device)\n",
        "        Y = torch.from_numpy(np.array(mmap_file[start+1:start+1+T], dtype=np.int64)).unsqueeze(0).to(device)\n",
        "        logits, loss = model(X, Y)\n",
        "        losses.append(loss.item())\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "print(\"train CE:\", eval_ce(train_ids, n_batches=200))\n",
        "print(\"val   CE:\", eval_ce(val_ids,   n_batches=200))\n",
        "print(\"train PPL:\", math.exp(eval_ce(train_ids, 200)))\n",
        "print(\"val   PPL:\", math.exp(eval_ce(val_ids,   200)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUAzrYGZVwmj",
        "outputId": "f4d42f2a-09e6-42dc-d9dc-b06686768534"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train CE: 0.7668087463080883\n",
            "val   CE: 2.228817498087883\n",
            "train PPL: 2.1165031509998733\n",
            "val   PPL: 8.864717826463952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Z8YZsTPrGnLm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def expected_rank_of_token(scores: torch.Tensor,\n",
        "                           token_ids: torch.Tensor,\n",
        "                           temperature: float = 1.0) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the expected rank of the given token at each position, without full V x V matrix.\n",
        "    \"\"\"\n",
        "    # scores: (..., V), token_ids: (...,)\n",
        "\n",
        "    # Gather score of the target token\n",
        "    score_i = scores.gather(-1, token_ids.unsqueeze(-1))  # (..., 1)\n",
        "\n",
        "    # Î”_j = score_j - score_i\n",
        "    diff = scores - score_i  # (..., V)\n",
        "\n",
        "    # P(j beats i)\n",
        "    p = torch.sigmoid(diff / temperature)\n",
        "\n",
        "    # Expected rank = 1 + sum_j P(j > i)\n",
        "    return 1.0 + p.sum(dim=-1)  # (...,)\n",
        "\n",
        "\n",
        "def rank_future_sequence_loss_soft(\n",
        "    logits: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    max_future_steps: int = 15,\n",
        "    decay: float = 0.5,\n",
        "    temperature: float = 1.0,\n",
        "    reduction: str = \"mean\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Memory-efficient smooth rank loss. For each t, matches rank of x_{t+Î”} to Î”.\n",
        "    logits  â€¦ (B, T, V) â€“ model scores\n",
        "    targets â€¦ (B, T)    â€“ token ids\n",
        "    \"\"\"\n",
        "    B, T, V = logits.shape\n",
        "    device = logits.device\n",
        "    total_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    for Î” in range(2, max_future_steps + 1):\n",
        "        if Î” >= T:\n",
        "            break\n",
        "\n",
        "        # Current time-step logits (for rank eval)\n",
        "        cur_logits  = logits[:, :-Î”, :]          # (B, Tâˆ’Î”, V)\n",
        "        fut_targets = targets[:, Î”:]             # (B, Tâˆ’Î”)\n",
        "\n",
        "        # Efficient rank of ground-truth future token\n",
        "        tgt_exp_rank = expected_rank_of_token(cur_logits, fut_targets, temperature)  # (B, Tâˆ’Î”)\n",
        "\n",
        "        # Penalize distance from desired rank Î”\n",
        "        step_loss = F.l1_loss(\n",
        "            tgt_exp_rank,\n",
        "            torch.full_like(tgt_exp_rank, float(Î”)),\n",
        "            reduction=reduction\n",
        "        )\n",
        "\n",
        "        # Apply decay for further future steps\n",
        "        total_loss = total_loss + step_loss * (decay ** (Î” - 1))\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def ordered_future_loss(logits: torch.Tensor,\n",
        "                        targets: torch.Tensor,\n",
        "                        N: int = 15,\n",
        "                        decay: float = 0.7,\n",
        "                        tau: float = 1.0,\n",
        "                        reduction: str = \"mean\"):\n",
        "    \"\"\"\n",
        "    Penalise when the logits at step t do *not* respect the order of the next N tokens.\n",
        "\n",
        "        top-1 logit should match token t+1\n",
        "        top-2 logit should match token t+2\n",
        "        ...\n",
        "        top-N logit should match token t+N\n",
        "\n",
        "    logits  â€“ (B, T, V)\n",
        "    targets â€“ (B, T)\n",
        "    \"\"\"\n",
        "    B, T, V = logits.shape\n",
        "    device  = logits.device\n",
        "\n",
        "    if N < 2:\n",
        "        return torch.tensor(0., device=device)\n",
        "\n",
        "    # windows where t+N fits in sequence\n",
        "    valid_T = T - (N + 1)\n",
        "    if valid_T <= 0:\n",
        "        return torch.tensor(0., device=device)\n",
        "\n",
        "    # (B, valid_T, N) â†’ future token ids for each offset 2..N\n",
        "    future_ids = torch.stack([targets[:, 2+k : 2+k+valid_T] for k in range(N)],\n",
        "                         dim=-1)\n",
        "\n",
        "    # (B, valid_T, N) â†’ gather logits of those future tokens *now* (at step t)\n",
        "    step_logits = logits[:, :valid_T, :].gather(\n",
        "        -1, future_ids)                       # logit(x_{t+k})\n",
        "\n",
        "    # pair-wise differences  Î”_{k,j} = logit_k âˆ’ logit_j, shape (B, valid_T, N, N)\n",
        "    diff = step_logits.unsqueeze(-1) - step_logits.unsqueeze(-2)\n",
        "\n",
        "    # upper-triangular mask k<j (ignore diag & lower triangle)\n",
        "    k_lt_j = torch.triu(torch.ones(N, N, device=device, dtype=torch.bool), 1)\n",
        "\n",
        "    # logistic ranking loss\n",
        "    pair_loss = F.softplus(-diff / tau)       # log(1+e^{-Î”/Ï„})\n",
        "    pair_loss = pair_loss[..., k_lt_j]        # keep k<j entries, now shape (B, valid_T, M)\n",
        "\n",
        "    # geometric weights per k (distance from current step)\n",
        "    k_idx = torch.arange(N, device=device)\n",
        "    weight = decay ** k_idx                   # shape (N,)\n",
        "    # broadcast to pair-wise (k<j) selector\n",
        "    weight_pair = weight.unsqueeze(-1).expand(N, N)[k_lt_j]  # (M,)\n",
        "\n",
        "    pair_loss = pair_loss * weight_pair       # (B, valid_T, M)\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        return pair_loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        return pair_loss.sum()\n",
        "    else:                                     # 'none'\n",
        "        return pair_loss                      # (B, valid_T, M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O0JnDKAyKlb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HEljjIcEGnLn",
        "outputId": "810a21e7-7741-485c-e5b2-5a7e92a778e1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " isotonic cone loss = 0.0156\n",
            "1.169595718383789\n",
            " isotonic cone loss = 0.0152\n",
            "1.2065304517745972\n",
            " isotonic cone loss = 0.0150\n",
            "1.2187845706939697\n",
            " isotonic cone loss = 0.0151\n",
            "1.1520516872406006\n",
            " isotonic cone loss = 0.0150\n",
            "1.1762464046478271\n",
            " isotonic cone loss = 0.0151\n",
            "1.2183260917663574\n",
            " isotonic cone loss = 0.0154\n",
            "1.1274793148040771\n",
            " isotonic cone loss = 0.0148\n",
            "1.2465052604675293\n",
            " isotonic cone loss = 0.0152\n",
            "1.1584665775299072\n",
            " isotonic cone loss = 0.0155\n",
            "1.1487865447998047\n",
            " isotonic cone loss = 0.0153\n",
            "1.072631597518921\n",
            " isotonic cone loss = 0.0154\n",
            "1.1511025428771973\n",
            " isotonic cone loss = 0.0154\n",
            "1.2227845191955566\n",
            " isotonic cone loss = 0.0154\n",
            "1.1533159017562866\n",
            " isotonic cone loss = 0.0149\n",
            "1.1267443895339966\n",
            " isotonic cone loss = 0.0150\n",
            "1.1448618173599243\n",
            " isotonic cone loss = 0.0147\n",
            "1.2396254539489746\n",
            " isotonic cone loss = 0.0152\n",
            "1.2524195909500122\n",
            " isotonic cone loss = 0.0151\n",
            "1.1676467657089233\n",
            " isotonic cone loss = 0.0156\n",
            "1.122477650642395\n",
            " isotonic cone loss = 0.0151\n",
            "1.1845453977584839\n",
            " isotonic cone loss = 0.0152\n",
            "1.1947187185287476\n",
            " isotonic cone loss = 0.0149\n",
            "1.2298359870910645\n",
            " isotonic cone loss = 0.0153\n",
            "1.105392336845398\n",
            " isotonic cone loss = 0.0152\n",
            "1.1614112854003906\n",
            " isotonic cone loss = 0.0153\n",
            "1.295424222946167\n",
            " isotonic cone loss = 0.0148\n",
            "1.1892430782318115\n",
            " isotonic cone loss = 0.0150\n",
            "1.1876683235168457\n",
            " isotonic cone loss = 0.0150\n",
            "1.1926511526107788\n",
            " isotonic cone loss = 0.0155\n",
            "1.218536376953125\n",
            " isotonic cone loss = 0.0153\n",
            "1.2014857530593872\n",
            " isotonic cone loss = 0.0154\n",
            "1.1853171586990356\n",
            " isotonic cone loss = 0.0157\n",
            "1.1762776374816895\n",
            " isotonic cone loss = 0.0151\n",
            "1.2191247940063477\n",
            " isotonic cone loss = 0.0150\n",
            "1.2400771379470825\n",
            " isotonic cone loss = 0.0150\n",
            "1.2155874967575073\n",
            " isotonic cone loss = 0.0149\n",
            "1.1400198936462402\n",
            " isotonic cone loss = 0.0151\n",
            "1.1580110788345337\n",
            " isotonic cone loss = 0.0149\n",
            "1.2081716060638428\n",
            " isotonic cone loss = 0.0148\n",
            "1.1424349546432495\n",
            " isotonic cone loss = 0.0148\n",
            "1.187467336654663\n",
            " isotonic cone loss = 0.0152\n",
            "1.1658811569213867\n",
            " isotonic cone loss = 0.0152\n",
            "1.216386079788208\n",
            " isotonic cone loss = 0.0152\n",
            "1.1811788082122803\n",
            " isotonic cone loss = 0.0151\n",
            "1.1635266542434692\n",
            " isotonic cone loss = 0.0149\n",
            "1.2686328887939453\n",
            " isotonic cone loss = 0.0150\n",
            "1.1617145538330078\n",
            " isotonic cone loss = 0.0151\n",
            "1.1303726434707642\n",
            " isotonic cone loss = 0.0151\n",
            "1.1178103685379028\n",
            " isotonic cone loss = 0.0148\n",
            "1.2407801151275635\n",
            " isotonic cone loss = 0.0149\n",
            "1.1815060377120972\n",
            " isotonic cone loss = 0.0151\n",
            "1.1490854024887085\n",
            " isotonic cone loss = 0.0147\n",
            "1.2751307487487793\n",
            " isotonic cone loss = 0.0148\n",
            "1.2396574020385742\n",
            " isotonic cone loss = 0.0150\n",
            "1.1282322406768799\n",
            " isotonic cone loss = 0.0150\n",
            "1.1744892597198486\n",
            " isotonic cone loss = 0.0152\n",
            "1.2389942407608032\n",
            " isotonic cone loss = 0.0148\n",
            "1.2383129596710205\n",
            " isotonic cone loss = 0.0149\n",
            "1.1842231750488281\n",
            " isotonic cone loss = 0.0147\n",
            "1.2128571271896362\n",
            " isotonic cone loss = 0.0148\n",
            "1.186344027519226\n",
            " isotonic cone loss = 0.0151\n",
            "1.1780263185501099\n",
            " isotonic cone loss = 0.0150\n",
            "1.171142816543579\n",
            " isotonic cone loss = 0.0152\n",
            "1.2161530256271362\n",
            " isotonic cone loss = 0.0150\n",
            "1.1528294086456299\n",
            " isotonic cone loss = 0.0150\n",
            "1.2496538162231445\n",
            " isotonic cone loss = 0.0148\n",
            "1.179898977279663\n",
            " isotonic cone loss = 0.0154\n",
            "1.1334108114242554\n",
            " isotonic cone loss = 0.0149\n",
            "1.1659951210021973\n",
            " isotonic cone loss = 0.0151\n",
            "1.115504503250122\n",
            " isotonic cone loss = 0.0145\n",
            "1.1085453033447266\n",
            " isotonic cone loss = 0.0149\n",
            "1.218530535697937\n",
            " isotonic cone loss = 0.0147\n",
            "1.1585301160812378\n",
            " isotonic cone loss = 0.0150\n",
            "1.183469533920288\n",
            " isotonic cone loss = 0.0148\n",
            "1.2214072942733765\n",
            " isotonic cone loss = 0.0147\n",
            "1.2041871547698975\n",
            " isotonic cone loss = 0.0145\n",
            "1.1378880739212036\n",
            " isotonic cone loss = 0.0149\n",
            "1.157349944114685\n",
            " isotonic cone loss = 0.0152\n",
            "1.1263569593429565\n",
            " isotonic cone loss = 0.0150\n",
            "1.22783625125885\n",
            " isotonic cone loss = 0.0150\n",
            "1.2024250030517578\n",
            " isotonic cone loss = 0.0148\n",
            "1.1438215970993042\n",
            " isotonic cone loss = 0.0148\n",
            "1.1666843891143799\n",
            " isotonic cone loss = 0.0146\n",
            "1.166016697883606\n",
            " isotonic cone loss = 0.0147\n",
            "1.2196874618530273\n",
            " isotonic cone loss = 0.0148\n",
            "1.154403805732727\n",
            " isotonic cone loss = 0.0150\n",
            "1.1539363861083984\n",
            " isotonic cone loss = 0.0154\n",
            "1.2560570240020752\n",
            " isotonic cone loss = 0.0153\n",
            "1.1586236953735352\n",
            " isotonic cone loss = 0.0154\n",
            "1.1665786504745483\n",
            " isotonic cone loss = 0.0151\n",
            "1.2224150896072388\n",
            " isotonic cone loss = 0.0151\n",
            "1.303907871246338\n",
            " isotonic cone loss = 0.0150\n",
            "1.120193600654602\n",
            " isotonic cone loss = 0.0146\n",
            "1.2667688131332397\n",
            " isotonic cone loss = 0.0144\n",
            "1.1575690507888794\n",
            " isotonic cone loss = 0.0148\n",
            "1.1590628623962402\n",
            " isotonic cone loss = 0.0151\n",
            "1.2277315855026245\n",
            " isotonic cone loss = 0.0147\n",
            "1.1772422790527344\n",
            " isotonic cone loss = 0.0150\n",
            "1.1925846338272095\n",
            " isotonic cone loss = 0.0144\n",
            "1.2006930112838745\n",
            " isotonic cone loss = 0.0148\n",
            "1.2049390077590942\n",
            " isotonic cone loss = 0.0148\n",
            "1.2072685956954956\n",
            " isotonic cone loss = 0.0150\n",
            "1.0814201831817627\n",
            " isotonic cone loss = 0.0150\n",
            "1.1536781787872314\n",
            " isotonic cone loss = 0.0153\n",
            "1.1565417051315308\n",
            " isotonic cone loss = 0.0148\n",
            "1.1513255834579468\n",
            " isotonic cone loss = 0.0145\n",
            "1.159843921661377\n",
            " isotonic cone loss = 0.0141\n",
            "1.1973801851272583\n",
            " isotonic cone loss = 0.0148\n",
            "1.0467658042907715\n",
            " isotonic cone loss = 0.0148\n",
            "1.1654282808303833\n",
            " isotonic cone loss = 0.0145\n",
            "1.1753424406051636\n",
            " isotonic cone loss = 0.0146\n",
            "1.2698757648468018\n",
            " isotonic cone loss = 0.0149\n",
            "1.1660959720611572\n",
            " isotonic cone loss = 0.0148\n",
            "1.2491695880889893\n",
            " isotonic cone loss = 0.0149\n",
            "1.1701242923736572\n",
            " isotonic cone loss = 0.0147\n",
            "1.1658705472946167\n",
            " isotonic cone loss = 0.0145\n",
            "1.187648057937622\n",
            " isotonic cone loss = 0.0150\n",
            "1.1342602968215942\n",
            " isotonic cone loss = 0.0148\n",
            "1.1947702169418335\n",
            " isotonic cone loss = 0.0148\n",
            "1.0815584659576416\n",
            " isotonic cone loss = 0.0149\n",
            "1.1372239589691162\n",
            " isotonic cone loss = 0.0151\n",
            "1.1863220930099487\n",
            " isotonic cone loss = 0.0150\n",
            "1.1317917108535767\n",
            " isotonic cone loss = 0.0149\n",
            "1.168088674545288\n",
            " isotonic cone loss = 0.0151\n",
            "1.1927547454833984\n",
            " isotonic cone loss = 0.0150\n",
            "1.125116229057312\n",
            " isotonic cone loss = 0.0148\n",
            "1.1171865463256836\n",
            " isotonic cone loss = 0.0150\n",
            "1.1797508001327515\n",
            " isotonic cone loss = 0.0148\n",
            "1.2338987588882446\n",
            " isotonic cone loss = 0.0147\n",
            "1.1436493396759033\n",
            " isotonic cone loss = 0.0149\n",
            "1.1351993083953857\n",
            " isotonic cone loss = 0.0150\n",
            "1.1893185377120972\n",
            " isotonic cone loss = 0.0150\n",
            "1.2068721055984497\n",
            " isotonic cone loss = 0.0150\n",
            "1.2013428211212158\n",
            " isotonic cone loss = 0.0147\n",
            "1.182804822921753\n",
            " isotonic cone loss = 0.0150\n",
            "1.2155017852783203\n",
            " isotonic cone loss = 0.0149\n",
            "1.2314764261245728\n",
            " isotonic cone loss = 0.0154\n",
            "1.096110224723816\n",
            " isotonic cone loss = 0.0151\n",
            "1.132493019104004\n",
            " isotonic cone loss = 0.0151\n",
            "1.1676998138427734\n",
            " isotonic cone loss = 0.0147\n",
            "1.2042371034622192\n",
            " isotonic cone loss = 0.0148\n",
            "1.2028934955596924\n",
            " isotonic cone loss = 0.0147\n",
            "1.1629533767700195\n",
            " isotonic cone loss = 0.0144\n",
            "1.1013201475143433\n",
            " isotonic cone loss = 0.0148\n",
            "1.0896742343902588\n",
            " isotonic cone loss = 0.0143\n",
            "1.2322087287902832\n",
            " isotonic cone loss = 0.0148\n",
            "1.1722007989883423\n",
            " isotonic cone loss = 0.0143\n",
            "1.2467464208602905\n",
            " isotonic cone loss = 0.0148\n",
            "1.087638258934021\n",
            " isotonic cone loss = 0.0146\n",
            "1.145043134689331\n",
            " isotonic cone loss = 0.0147\n",
            "1.108196496963501\n",
            " isotonic cone loss = 0.0150\n",
            "1.0932691097259521\n",
            " isotonic cone loss = 0.0147\n",
            "1.2342698574066162\n",
            " isotonic cone loss = 0.0150\n",
            "1.1567513942718506\n",
            " isotonic cone loss = 0.0148\n",
            "1.1061469316482544\n",
            " isotonic cone loss = 0.0146\n",
            "1.1906864643096924\n",
            " isotonic cone loss = 0.0147\n",
            "1.1062489748001099\n",
            " isotonic cone loss = 0.0144\n",
            "1.276712417602539\n",
            " isotonic cone loss = 0.0141\n",
            "1.137877106666565\n",
            " isotonic cone loss = 0.0146\n",
            "1.1145129203796387\n",
            " isotonic cone loss = 0.0148\n",
            "1.1543117761611938\n",
            " isotonic cone loss = 0.0152\n",
            "1.147608757019043\n",
            " isotonic cone loss = 0.0147\n",
            "1.1961727142333984\n",
            " isotonic cone loss = 0.0146\n",
            "1.1776418685913086\n",
            " isotonic cone loss = 0.0145\n",
            "1.1592847108840942\n",
            " isotonic cone loss = 0.0146\n",
            "1.145458459854126\n",
            " isotonic cone loss = 0.0149\n",
            "1.1508939266204834\n",
            " isotonic cone loss = 0.0147\n",
            "1.1955643892288208\n",
            " isotonic cone loss = 0.0150\n",
            "1.1533595323562622\n",
            " isotonic cone loss = 0.0147\n",
            "1.21421480178833\n",
            " isotonic cone loss = 0.0146\n",
            "1.11105477809906\n",
            " isotonic cone loss = 0.0147\n",
            "1.1657509803771973\n",
            " isotonic cone loss = 0.0145\n",
            "1.1723421812057495\n",
            " isotonic cone loss = 0.0147\n",
            "1.1635180711746216\n",
            " isotonic cone loss = 0.0144\n",
            "1.1804083585739136\n",
            " isotonic cone loss = 0.0148\n",
            "1.1979646682739258\n",
            " isotonic cone loss = 0.0146\n",
            "1.1526373624801636\n",
            " isotonic cone loss = 0.0150\n",
            "1.1688231229782104\n",
            " isotonic cone loss = 0.0149\n",
            "1.2108154296875\n",
            " isotonic cone loss = 0.0147\n",
            "1.0638483762741089\n",
            " isotonic cone loss = 0.0148\n",
            "1.1121702194213867\n",
            " isotonic cone loss = 0.0148\n",
            "1.1487927436828613\n",
            " isotonic cone loss = 0.0147\n",
            "1.1511260271072388\n",
            " isotonic cone loss = 0.0148\n",
            "1.2049343585968018\n",
            " isotonic cone loss = 0.0146\n",
            "1.146016240119934\n",
            " isotonic cone loss = 0.0150\n",
            "1.1362481117248535\n",
            " isotonic cone loss = 0.0146\n",
            "1.1408535242080688\n",
            " isotonic cone loss = 0.0148\n",
            "1.2198432683944702\n",
            " isotonic cone loss = 0.0148\n",
            "1.0537968873977661\n",
            " isotonic cone loss = 0.0151\n",
            "1.180752158164978\n",
            " isotonic cone loss = 0.0152\n",
            "1.1919946670532227\n",
            " isotonic cone loss = 0.0149\n",
            "1.1583824157714844\n",
            " isotonic cone loss = 0.0146\n",
            "1.1940604448318481\n",
            " isotonic cone loss = 0.0146\n",
            "1.1744176149368286\n",
            " isotonic cone loss = 0.0147\n",
            "1.1483988761901855\n",
            " isotonic cone loss = 0.0148\n",
            "1.0837621688842773\n",
            " isotonic cone loss = 0.0149\n",
            "1.1019271612167358\n",
            " isotonic cone loss = 0.0151\n",
            "1.110294222831726\n",
            " isotonic cone loss = 0.0147\n",
            "1.1814868450164795\n",
            " isotonic cone loss = 0.0145\n",
            "1.153854489326477\n",
            " isotonic cone loss = 0.0149\n",
            "1.0781590938568115\n",
            " isotonic cone loss = 0.0146\n",
            "1.1899148225784302\n",
            " isotonic cone loss = 0.0146\n",
            "1.1676819324493408\n",
            " isotonic cone loss = 0.0145\n",
            "1.1482388973236084\n",
            " isotonic cone loss = 0.0149\n",
            "1.1760246753692627\n",
            " isotonic cone loss = 0.0150\n",
            "1.1705204248428345\n",
            " isotonic cone loss = 0.0148\n",
            "1.2114145755767822\n",
            " isotonic cone loss = 0.0149\n",
            "1.1563597917556763\n",
            " isotonic cone loss = 0.0146\n",
            "1.1444003582000732\n",
            " isotonic cone loss = 0.0146\n",
            "1.1476541757583618\n",
            " isotonic cone loss = 0.0144\n",
            "1.1871459484100342\n",
            " isotonic cone loss = 0.0148\n",
            "1.1551755666732788\n",
            " isotonic cone loss = 0.0151\n",
            "1.198403239250183\n",
            " isotonic cone loss = 0.0148\n",
            "1.2191072702407837\n",
            " isotonic cone loss = 0.0151\n",
            "1.1729300022125244\n",
            " isotonic cone loss = 0.0146\n",
            "1.1847989559173584\n",
            " isotonic cone loss = 0.0146\n",
            "1.1818616390228271\n",
            " isotonic cone loss = 0.0149\n",
            "1.056070327758789\n",
            " isotonic cone loss = 0.0152\n",
            "1.1528654098510742\n",
            " isotonic cone loss = 0.0149\n",
            "1.1587789058685303\n",
            " isotonic cone loss = 0.0152\n",
            "1.0971834659576416\n",
            " isotonic cone loss = 0.0151\n",
            "1.131465196609497\n",
            " isotonic cone loss = 0.0144\n",
            "1.1361559629440308\n",
            " isotonic cone loss = 0.0144\n",
            "1.1532055139541626\n",
            " isotonic cone loss = 0.0146\n",
            "1.1638214588165283\n",
            " isotonic cone loss = 0.0148\n",
            "1.1125720739364624\n",
            " isotonic cone loss = 0.0147\n",
            "1.1142549514770508\n",
            " isotonic cone loss = 0.0149\n",
            "1.1502058506011963\n",
            " isotonic cone loss = 0.0144\n",
            "1.2713816165924072\n",
            " isotonic cone loss = 0.0145\n",
            "1.1655499935150146\n",
            " isotonic cone loss = 0.0147\n",
            "1.0996838808059692\n",
            " isotonic cone loss = 0.0145\n",
            "1.1719894409179688\n",
            " isotonic cone loss = 0.0148\n",
            "1.1261271238327026\n",
            " isotonic cone loss = 0.0147\n",
            "1.1601760387420654\n",
            " isotonic cone loss = 0.0144\n",
            "1.1127325296401978\n",
            " isotonic cone loss = 0.0150\n",
            "1.1802319288253784\n",
            " isotonic cone loss = 0.0147\n",
            "1.1947681903839111\n",
            " isotonic cone loss = 0.0145\n",
            "1.177146315574646\n",
            " isotonic cone loss = 0.0146\n",
            "1.1651692390441895\n",
            " isotonic cone loss = 0.0145\n",
            "1.1691945791244507\n",
            " isotonic cone loss = 0.0147\n",
            "1.108784556388855\n",
            " isotonic cone loss = 0.0144\n",
            "1.1452887058258057\n",
            " isotonic cone loss = 0.0144\n",
            "1.1454986333847046\n",
            " isotonic cone loss = 0.0146\n",
            "1.1585747003555298\n",
            " isotonic cone loss = 0.0145\n",
            "1.1556494235992432\n",
            " isotonic cone loss = 0.0147\n",
            "1.1496872901916504\n",
            " isotonic cone loss = 0.0145\n",
            "1.1618728637695312\n",
            " isotonic cone loss = 0.0149\n",
            "1.0897645950317383\n",
            " isotonic cone loss = 0.0147\n",
            "1.1007829904556274\n",
            " isotonic cone loss = 0.0148\n",
            "1.0973694324493408\n",
            " isotonic cone loss = 0.0146\n",
            "1.1659120321273804\n",
            " isotonic cone loss = 0.0146\n",
            "1.145461916923523\n",
            " isotonic cone loss = 0.0147\n",
            "1.059180736541748\n",
            " isotonic cone loss = 0.0145\n",
            "1.1077042818069458\n",
            " isotonic cone loss = 0.0146\n",
            "1.0859413146972656\n",
            " isotonic cone loss = 0.0148\n",
            "1.1070555448532104\n",
            " isotonic cone loss = 0.0151\n",
            "1.1950371265411377\n",
            " isotonic cone loss = 0.0150\n",
            "1.0631383657455444\n",
            " isotonic cone loss = 0.0141\n",
            "1.1892480850219727\n",
            " isotonic cone loss = 0.0146\n",
            "1.177842617034912\n",
            " isotonic cone loss = 0.0149\n",
            "1.151726484298706\n",
            " isotonic cone loss = 0.0150\n",
            "1.1295647621154785\n",
            " isotonic cone loss = 0.0146\n",
            "1.1335408687591553\n",
            " isotonic cone loss = 0.0148\n",
            "1.0819745063781738\n",
            " isotonic cone loss = 0.0151\n",
            "1.1320812702178955\n",
            " isotonic cone loss = 0.0150\n",
            "1.186673641204834\n",
            " isotonic cone loss = 0.0147\n",
            "1.1279418468475342\n",
            " isotonic cone loss = 0.0145\n",
            "1.110564112663269\n",
            " isotonic cone loss = 0.0148\n",
            "1.136509895324707\n",
            " isotonic cone loss = 0.0149\n",
            "1.1284123659133911\n",
            " isotonic cone loss = 0.0150\n",
            "1.1519972085952759\n",
            " isotonic cone loss = 0.0146\n",
            "1.1598138809204102\n",
            " isotonic cone loss = 0.0147\n",
            "1.1975884437561035\n",
            " isotonic cone loss = 0.0145\n",
            "1.1135436296463013\n",
            " isotonic cone loss = 0.0144\n",
            "1.1912295818328857\n",
            " isotonic cone loss = 0.0147\n",
            "1.0890313386917114\n",
            " isotonic cone loss = 0.0148\n",
            "1.175513744354248\n",
            " isotonic cone loss = 0.0143\n",
            "1.1776692867279053\n",
            " isotonic cone loss = 0.0146\n",
            "1.1546783447265625\n",
            " isotonic cone loss = 0.0145\n",
            "1.122807264328003\n",
            " isotonic cone loss = 0.0143\n",
            "1.141033411026001\n",
            " isotonic cone loss = 0.0145\n",
            "1.0651394128799438\n",
            " isotonic cone loss = 0.0144\n",
            "1.1569868326187134\n",
            " isotonic cone loss = 0.0149\n",
            "1.132773756980896\n",
            " isotonic cone loss = 0.0147\n",
            "1.0635864734649658\n",
            " isotonic cone loss = 0.0147\n",
            "1.1647868156433105\n",
            " isotonic cone loss = 0.0144\n",
            "1.1845816373825073\n",
            " isotonic cone loss = 0.0146\n",
            "1.1784565448760986\n",
            " isotonic cone loss = 0.0146\n",
            "1.1536865234375\n",
            " isotonic cone loss = 0.0148\n",
            "1.1558527946472168\n",
            " isotonic cone loss = 0.0148\n",
            "1.104654312133789\n",
            " isotonic cone loss = 0.0150\n",
            "1.1564871072769165\n",
            " isotonic cone loss = 0.0149\n",
            "1.1156556606292725\n",
            " isotonic cone loss = 0.0151\n",
            "1.1719111204147339\n",
            " isotonic cone loss = 0.0145\n",
            "1.0607311725616455\n",
            " isotonic cone loss = 0.0147\n",
            "1.0662082433700562\n",
            " isotonic cone loss = 0.0142\n",
            "1.1553212404251099\n",
            " isotonic cone loss = 0.0145\n",
            "1.1228008270263672\n",
            " isotonic cone loss = 0.0143\n",
            "1.1587998867034912\n",
            " isotonic cone loss = 0.0145\n",
            "1.1119359731674194\n",
            " isotonic cone loss = 0.0147\n",
            "1.1818417310714722\n",
            " isotonic cone loss = 0.0147\n",
            "1.107008934020996\n",
            " isotonic cone loss = 0.0146\n",
            "1.2248790264129639\n",
            " isotonic cone loss = 0.0147\n",
            "1.0975521802902222\n",
            " isotonic cone loss = 0.0144\n",
            "1.128883957862854\n",
            " isotonic cone loss = 0.0144\n",
            "1.1998589038848877\n",
            " isotonic cone loss = 0.0147\n",
            "1.0539981126785278\n",
            " isotonic cone loss = 0.0151\n",
            "1.087960124015808\n",
            " isotonic cone loss = 0.0149\n",
            "1.1790250539779663\n",
            " isotonic cone loss = 0.0146\n",
            "1.155641794204712\n",
            " isotonic cone loss = 0.0145\n",
            "1.010499119758606\n",
            " isotonic cone loss = 0.0149\n",
            "1.0381622314453125\n",
            " isotonic cone loss = 0.0146\n",
            "1.1939055919647217\n",
            " isotonic cone loss = 0.0148\n",
            "1.1397374868392944\n",
            " isotonic cone loss = 0.0148\n",
            "1.0927823781967163\n",
            " isotonic cone loss = 0.0146\n",
            "1.130643606185913\n",
            " isotonic cone loss = 0.0146\n",
            "1.0491952896118164\n",
            " isotonic cone loss = 0.0146\n",
            "1.1933236122131348\n",
            " isotonic cone loss = 0.0142\n",
            "1.1368144750595093\n",
            " isotonic cone loss = 0.0144\n",
            "1.1461689472198486\n",
            " isotonic cone loss = 0.0149\n",
            "1.1235558986663818\n",
            " isotonic cone loss = 0.0145\n",
            "1.1014593839645386\n",
            " isotonic cone loss = 0.0143\n",
            "1.215761423110962\n",
            " isotonic cone loss = 0.0146\n",
            "1.1735862493515015\n",
            " isotonic cone loss = 0.0145\n",
            "1.2006129026412964\n",
            " isotonic cone loss = 0.0146\n",
            "1.1512945890426636\n",
            " isotonic cone loss = 0.0148\n",
            "1.1655547618865967\n",
            " isotonic cone loss = 0.0147\n",
            "1.1390724182128906\n",
            " isotonic cone loss = 0.0147\n",
            "1.1462116241455078\n",
            " isotonic cone loss = 0.0145\n",
            "1.1591267585754395\n",
            " isotonic cone loss = 0.0145\n",
            "1.0701912641525269\n",
            " isotonic cone loss = 0.0146\n",
            "1.1801444292068481\n",
            " isotonic cone loss = 0.0146\n",
            "1.0630937814712524\n",
            " isotonic cone loss = 0.0147\n",
            "1.1186437606811523\n",
            " isotonic cone loss = 0.0146\n",
            "1.1213425397872925\n",
            " isotonic cone loss = 0.0145\n",
            "1.1194442510604858\n",
            " isotonic cone loss = 0.0145\n",
            "1.1294407844543457\n",
            " isotonic cone loss = 0.0146\n",
            "1.1000064611434937\n",
            " isotonic cone loss = 0.0144\n",
            "1.2059844732284546\n",
            " isotonic cone loss = 0.0145\n",
            "1.1878726482391357\n",
            " isotonic cone loss = 0.0148\n",
            "1.1421599388122559\n",
            " isotonic cone loss = 0.0147\n",
            "1.0977928638458252\n",
            " isotonic cone loss = 0.0144\n",
            "1.1342717409133911\n",
            " isotonic cone loss = 0.0145\n",
            "1.116348385810852\n",
            " isotonic cone loss = 0.0144\n",
            "1.1636892557144165\n",
            " isotonic cone loss = 0.0147\n",
            "1.1649174690246582\n",
            " isotonic cone loss = 0.0146\n",
            "1.1386572122573853\n",
            " isotonic cone loss = 0.0145\n",
            "1.1074525117874146\n",
            " isotonic cone loss = 0.0141\n",
            "1.2362602949142456\n",
            " isotonic cone loss = 0.0144\n",
            "1.0695027112960815\n",
            " isotonic cone loss = 0.0145\n",
            "1.1390323638916016\n",
            " isotonic cone loss = 0.0144\n",
            "1.1629821062088013\n",
            " isotonic cone loss = 0.0147\n",
            "1.043846607208252\n",
            " isotonic cone loss = 0.0147\n",
            "1.162239909172058\n",
            " isotonic cone loss = 0.0146\n",
            "1.1257649660110474\n",
            " isotonic cone loss = 0.0146\n",
            "1.12796151638031\n",
            " isotonic cone loss = 0.0144\n",
            "1.1167353391647339\n",
            " isotonic cone loss = 0.0147\n",
            "1.0902559757232666\n",
            " isotonic cone loss = 0.0143\n",
            "1.1955209970474243\n",
            " isotonic cone loss = 0.0145\n",
            "1.1311887502670288\n",
            " isotonic cone loss = 0.0145\n",
            "1.1133843660354614\n",
            " isotonic cone loss = 0.0145\n",
            "1.0814919471740723\n",
            " isotonic cone loss = 0.0145\n",
            "1.1393983364105225\n",
            " isotonic cone loss = 0.0149\n",
            "1.1541002988815308\n",
            " isotonic cone loss = 0.0146\n",
            "1.1052790880203247\n",
            " isotonic cone loss = 0.0143\n",
            "1.1635817289352417\n",
            " isotonic cone loss = 0.0146\n",
            "1.1196835041046143\n",
            " isotonic cone loss = 0.0149\n",
            "1.0509666204452515\n",
            " isotonic cone loss = 0.0144\n",
            "1.1056323051452637\n",
            " isotonic cone loss = 0.0145\n",
            "1.1615396738052368\n",
            " isotonic cone loss = 0.0144\n",
            "1.2553040981292725\n",
            " isotonic cone loss = 0.0142\n",
            "1.1014460325241089\n",
            " isotonic cone loss = 0.0144\n",
            "1.027355432510376\n",
            " isotonic cone loss = 0.0143\n",
            "1.108241319656372\n",
            " isotonic cone loss = 0.0145\n",
            "1.127351999282837\n",
            " isotonic cone loss = 0.0141\n",
            "1.1355266571044922\n",
            " isotonic cone loss = 0.0143\n",
            "1.060227394104004\n",
            " isotonic cone loss = 0.0145\n",
            "1.0724401473999023\n",
            " isotonic cone loss = 0.0145\n",
            "1.0516860485076904\n",
            " isotonic cone loss = 0.0144\n",
            "1.1252678632736206\n",
            " isotonic cone loss = 0.0146\n",
            "1.0547133684158325\n",
            " isotonic cone loss = 0.0146\n",
            "1.0587090253829956\n",
            " isotonic cone loss = 0.0144\n",
            "1.0631309747695923\n",
            " isotonic cone loss = 0.0144\n",
            "1.1067990064620972\n",
            " isotonic cone loss = 0.0143\n",
            "1.163273572921753\n",
            " isotonic cone loss = 0.0145\n",
            "1.1598860025405884\n",
            " isotonic cone loss = 0.0144\n",
            "1.1137371063232422\n",
            " isotonic cone loss = 0.0145\n",
            "1.1031420230865479\n",
            " isotonic cone loss = 0.0145\n",
            "1.1985313892364502\n",
            " isotonic cone loss = 0.0146\n",
            "1.1427106857299805\n",
            " isotonic cone loss = 0.0144\n",
            "1.1031640768051147\n",
            " isotonic cone loss = 0.0146\n",
            "1.1406705379486084\n",
            " isotonic cone loss = 0.0146\n",
            "1.1242619752883911\n",
            " isotonic cone loss = 0.0147\n",
            "1.1104540824890137\n",
            " isotonic cone loss = 0.0145\n",
            "1.123229742050171\n",
            " isotonic cone loss = 0.0146\n",
            "1.1193526983261108\n",
            " isotonic cone loss = 0.0149\n",
            "1.0614665746688843\n",
            " isotonic cone loss = 0.0143\n",
            "1.13886296749115\n",
            " isotonic cone loss = 0.0146\n",
            "1.2023017406463623\n",
            " isotonic cone loss = 0.0145\n",
            "1.109622597694397\n",
            " isotonic cone loss = 0.0146\n",
            "1.1782814264297485\n",
            " isotonic cone loss = 0.0145\n",
            "1.0735979080200195\n",
            " isotonic cone loss = 0.0147\n",
            "1.1366046667099\n",
            " isotonic cone loss = 0.0144\n",
            "1.1777762174606323\n",
            " isotonic cone loss = 0.0143\n",
            "1.1709932088851929\n",
            " isotonic cone loss = 0.0146\n",
            "1.1298530101776123\n",
            " isotonic cone loss = 0.0141\n",
            "1.1323143243789673\n",
            " isotonic cone loss = 0.0147\n",
            "1.1863151788711548\n",
            " isotonic cone loss = 0.0146\n",
            "1.1482229232788086\n",
            " isotonic cone loss = 0.0144\n",
            "1.0830801725387573\n",
            " isotonic cone loss = 0.0144\n",
            "1.0528393983840942\n",
            " isotonic cone loss = 0.0147\n",
            "1.1298216581344604\n",
            " isotonic cone loss = 0.0143\n",
            "1.0612365007400513\n",
            " isotonic cone loss = 0.0143\n",
            "1.0767663717269897\n",
            " isotonic cone loss = 0.0144\n",
            "1.1000170707702637\n",
            " isotonic cone loss = 0.0145\n",
            "1.1144587993621826\n",
            " isotonic cone loss = 0.0143\n",
            "1.0604580640792847\n",
            " isotonic cone loss = 0.0144\n",
            "1.1130318641662598\n",
            " isotonic cone loss = 0.0142\n",
            "1.0973377227783203\n",
            " isotonic cone loss = 0.0146\n",
            "1.043121337890625\n",
            " isotonic cone loss = 0.0143\n",
            "1.1222089529037476\n",
            " isotonic cone loss = 0.0147\n",
            "1.057948350906372\n",
            " isotonic cone loss = 0.0146\n",
            "1.0971564054489136\n",
            " isotonic cone loss = 0.0148\n",
            "1.13413405418396\n",
            " isotonic cone loss = 0.0146\n",
            "1.0648244619369507\n",
            " isotonic cone loss = 0.0142\n",
            "1.1508471965789795\n",
            " isotonic cone loss = 0.0145\n",
            "1.1518408060073853\n",
            " isotonic cone loss = 0.0146\n",
            "1.1055002212524414\n",
            " isotonic cone loss = 0.0146\n",
            "1.155927300453186\n",
            " isotonic cone loss = 0.0146\n",
            "1.1221941709518433\n",
            " isotonic cone loss = 0.0146\n",
            "1.0366586446762085\n",
            " isotonic cone loss = 0.0145\n",
            "1.092358112335205\n",
            " isotonic cone loss = 0.0148\n",
            "1.0424160957336426\n",
            " isotonic cone loss = 0.0145\n",
            "1.0251433849334717\n",
            " isotonic cone loss = 0.0144\n",
            "1.094714641571045\n",
            " isotonic cone loss = 0.0148\n",
            "1.0192004442214966\n",
            " isotonic cone loss = 0.0142\n",
            "1.049792766571045\n",
            " isotonic cone loss = 0.0145\n",
            "1.080572247505188\n",
            " isotonic cone loss = 0.0143\n",
            "1.125030755996704\n",
            " isotonic cone loss = 0.0148\n",
            "1.1405178308486938\n",
            " isotonic cone loss = 0.0146\n",
            "1.098241925239563\n",
            " isotonic cone loss = 0.0144\n",
            "1.1113203763961792\n",
            " isotonic cone loss = 0.0142\n",
            "1.2150827646255493\n",
            " isotonic cone loss = 0.0140\n",
            "1.1289108991622925\n",
            " isotonic cone loss = 0.0142\n",
            "1.0630640983581543\n",
            " isotonic cone loss = 0.0145\n",
            "1.1021734476089478\n",
            " isotonic cone loss = 0.0147\n",
            "1.0790668725967407\n",
            " isotonic cone loss = 0.0145\n",
            "1.1178131103515625\n",
            " isotonic cone loss = 0.0143\n",
            "1.1123822927474976\n",
            " isotonic cone loss = 0.0141\n",
            "1.1902832984924316\n",
            " isotonic cone loss = 0.0145\n",
            "1.1813985109329224\n",
            " isotonic cone loss = 0.0142\n",
            "1.126146912574768\n",
            " isotonic cone loss = 0.0144\n",
            "1.1627650260925293\n",
            " isotonic cone loss = 0.0143\n",
            "1.1404013633728027\n",
            " isotonic cone loss = 0.0143\n",
            "1.1004027128219604\n",
            " isotonic cone loss = 0.0144\n",
            "1.0912578105926514\n",
            " isotonic cone loss = 0.0145\n",
            "1.1583054065704346\n",
            " isotonic cone loss = 0.0145\n",
            "1.055916428565979\n",
            " isotonic cone loss = 0.0147\n",
            "1.0737483501434326\n",
            " isotonic cone loss = 0.0146\n",
            "1.1130130290985107\n",
            " isotonic cone loss = 0.0142\n",
            "1.1649682521820068\n",
            " isotonic cone loss = 0.0144\n",
            "1.1644984483718872\n",
            " isotonic cone loss = 0.0145\n",
            "1.1282521486282349\n",
            " isotonic cone loss = 0.0141\n",
            "1.142665982246399\n",
            " isotonic cone loss = 0.0141\n",
            "1.1225484609603882\n",
            " isotonic cone loss = 0.0145\n",
            "1.0591835975646973\n",
            " isotonic cone loss = 0.0139\n",
            "1.1082396507263184\n",
            " isotonic cone loss = 0.0144\n",
            "1.1159071922302246\n",
            " isotonic cone loss = 0.0145\n",
            "1.10246741771698\n",
            " isotonic cone loss = 0.0141\n",
            "1.1043760776519775\n",
            " isotonic cone loss = 0.0144\n",
            "0.975161612033844\n",
            " isotonic cone loss = 0.0144\n",
            "1.1029040813446045\n",
            " isotonic cone loss = 0.0146\n",
            "1.0260961055755615\n",
            " isotonic cone loss = 0.0143\n",
            "1.0683847665786743\n",
            " isotonic cone loss = 0.0144\n",
            "1.058585286140442\n",
            " isotonic cone loss = 0.0147\n",
            "1.050123691558838\n",
            " isotonic cone loss = 0.0149\n",
            "1.1038050651550293\n",
            " isotonic cone loss = 0.0146\n",
            "1.0569559335708618\n",
            " isotonic cone loss = 0.0146\n",
            "1.0171464681625366\n",
            " isotonic cone loss = 0.0146\n",
            "1.0593560934066772\n",
            " isotonic cone loss = 0.0146\n",
            "1.0864148139953613\n",
            " isotonic cone loss = 0.0146\n",
            "1.0530883073806763\n",
            " isotonic cone loss = 0.0145\n",
            "1.1149464845657349\n",
            " isotonic cone loss = 0.0142\n",
            "1.1428430080413818\n",
            " isotonic cone loss = 0.0143\n",
            "1.1221754550933838\n",
            " isotonic cone loss = 0.0142\n",
            "1.1441529989242554\n",
            " isotonic cone loss = 0.0144\n",
            "1.0656510591506958\n",
            " isotonic cone loss = 0.0148\n",
            "1.1716945171356201\n",
            " isotonic cone loss = 0.0149\n",
            "1.0851222276687622\n",
            " isotonic cone loss = 0.0148\n",
            "1.09330153465271\n",
            " isotonic cone loss = 0.0148\n",
            "1.0995949506759644\n",
            " isotonic cone loss = 0.0143\n",
            "1.1574923992156982\n",
            " isotonic cone loss = 0.0142\n",
            "1.1006981134414673\n",
            " isotonic cone loss = 0.0145\n",
            "1.1580824851989746\n",
            " isotonic cone loss = 0.0146\n",
            "1.1099419593811035\n",
            " isotonic cone loss = 0.0143\n",
            "1.1467889547348022\n",
            " isotonic cone loss = 0.0146\n",
            "1.1049816608428955\n",
            " isotonic cone loss = 0.0144\n",
            "1.1977288722991943\n",
            " isotonic cone loss = 0.0143\n",
            "1.1411259174346924\n",
            " isotonic cone loss = 0.0142\n",
            "1.0705407857894897\n",
            " isotonic cone loss = 0.0144\n",
            "1.0715272426605225\n",
            " isotonic cone loss = 0.0144\n",
            "1.1221661567687988\n",
            " isotonic cone loss = 0.0147\n",
            "1.1729177236557007\n",
            " isotonic cone loss = 0.0144\n",
            "1.0546222925186157\n",
            " isotonic cone loss = 0.0146\n",
            "1.0814656019210815\n",
            " isotonic cone loss = 0.0141\n",
            "1.0777602195739746\n",
            " isotonic cone loss = 0.0144\n",
            "1.142443060874939\n",
            " isotonic cone loss = 0.0142\n",
            "1.007959008216858\n",
            " isotonic cone loss = 0.0144\n",
            "0.99833744764328\n",
            " isotonic cone loss = 0.0140\n",
            "1.1457431316375732\n",
            " isotonic cone loss = 0.0143\n",
            "1.1534616947174072\n",
            " isotonic cone loss = 0.0141\n",
            "1.1365644931793213\n",
            " isotonic cone loss = 0.0140\n",
            "1.1324819326400757\n",
            " isotonic cone loss = 0.0142\n",
            "1.1091504096984863\n",
            " isotonic cone loss = 0.0143\n",
            "1.1480218172073364\n",
            " isotonic cone loss = 0.0146\n",
            "1.1064610481262207\n",
            " isotonic cone loss = 0.0147\n",
            "1.1766413450241089\n",
            " isotonic cone loss = 0.0142\n",
            "1.1342335939407349\n",
            " isotonic cone loss = 0.0145\n",
            "1.154269814491272\n",
            " isotonic cone loss = 0.0147\n",
            "1.1167539358139038\n",
            " isotonic cone loss = 0.0144\n",
            "1.140357255935669\n",
            " isotonic cone loss = 0.0145\n",
            "1.1467758417129517\n",
            " isotonic cone loss = 0.0145\n",
            "1.115568995475769\n",
            " isotonic cone loss = 0.0145\n",
            "1.1071780920028687\n",
            " isotonic cone loss = 0.0144\n",
            "1.1025209426879883\n",
            " isotonic cone loss = 0.0146\n",
            "1.0815339088439941\n",
            " isotonic cone loss = 0.0146\n",
            "1.033537745475769\n",
            " isotonic cone loss = 0.0146\n",
            "1.042805790901184\n",
            " isotonic cone loss = 0.0142\n",
            "1.1400963068008423\n",
            " isotonic cone loss = 0.0143\n",
            "1.121009111404419\n",
            " isotonic cone loss = 0.0146\n",
            "1.1696784496307373\n",
            " isotonic cone loss = 0.0146\n",
            "1.0331006050109863\n",
            " isotonic cone loss = 0.0148\n",
            "1.0866602659225464\n",
            " isotonic cone loss = 0.0145\n",
            "1.0923192501068115\n",
            " isotonic cone loss = 0.0147\n",
            "1.0679864883422852\n",
            " isotonic cone loss = 0.0142\n",
            "1.0986998081207275\n",
            " isotonic cone loss = 0.0145\n",
            "1.0869544744491577\n",
            " isotonic cone loss = 0.0145\n",
            "1.0868560075759888\n",
            " isotonic cone loss = 0.0147\n",
            "1.1140791177749634\n",
            " isotonic cone loss = 0.0143\n",
            "1.1014896631240845\n",
            " isotonic cone loss = 0.0141\n",
            "1.1292320489883423\n",
            " isotonic cone loss = 0.0143\n",
            "1.045785903930664\n",
            " isotonic cone loss = 0.0142\n",
            "1.111497402191162\n",
            " isotonic cone loss = 0.0144\n",
            "1.0821657180786133\n",
            " isotonic cone loss = 0.0144\n",
            "1.031646966934204\n",
            " isotonic cone loss = 0.0144\n",
            "1.1094975471496582\n",
            " isotonic cone loss = 0.0145\n",
            "1.066236972808838\n",
            " isotonic cone loss = 0.0138\n",
            "1.0599071979522705\n",
            " isotonic cone loss = 0.0144\n",
            "1.0404157638549805\n",
            " isotonic cone loss = 0.0141\n",
            "1.1042627096176147\n",
            " isotonic cone loss = 0.0145\n",
            "1.0542665719985962\n",
            " isotonic cone loss = 0.0146\n",
            "1.0526602268218994\n",
            " isotonic cone loss = 0.0150\n",
            "1.0907254219055176\n",
            " isotonic cone loss = 0.0139\n",
            "1.1559468507766724\n",
            " isotonic cone loss = 0.0141\n",
            "1.180505633354187\n",
            " isotonic cone loss = 0.0143\n",
            "1.0784025192260742\n",
            " isotonic cone loss = 0.0144\n",
            "1.0965584516525269\n",
            " isotonic cone loss = 0.0145\n",
            "1.0076590776443481\n",
            " isotonic cone loss = 0.0147\n",
            "1.022229790687561\n",
            " isotonic cone loss = 0.0143\n",
            "1.0874271392822266\n",
            " isotonic cone loss = 0.0142\n",
            "1.0947718620300293\n",
            " isotonic cone loss = 0.0146\n",
            "1.067662000656128\n",
            " isotonic cone loss = 0.0145\n",
            "1.060462474822998\n",
            " isotonic cone loss = 0.0143\n",
            "1.0933325290679932\n",
            " isotonic cone loss = 0.0147\n",
            "1.1039109230041504\n",
            " isotonic cone loss = 0.0144\n",
            "1.0728095769882202\n",
            " isotonic cone loss = 0.0144\n",
            "1.1234855651855469\n",
            " isotonic cone loss = 0.0142\n",
            "1.0175226926803589\n",
            " isotonic cone loss = 0.0147\n",
            "1.0747710466384888\n",
            " isotonic cone loss = 0.0143\n",
            "1.1019169092178345\n",
            " isotonic cone loss = 0.0144\n",
            "1.0551143884658813\n",
            " isotonic cone loss = 0.0145\n",
            "1.074358582496643\n",
            " isotonic cone loss = 0.0143\n",
            "1.0087249279022217\n",
            " isotonic cone loss = 0.0145\n",
            "1.099259376525879\n",
            " isotonic cone loss = 0.0141\n",
            "1.0990849733352661\n",
            " isotonic cone loss = 0.0146\n",
            "1.0802392959594727\n",
            " isotonic cone loss = 0.0145\n",
            "1.097381591796875\n",
            " isotonic cone loss = 0.0146\n",
            "1.0420960187911987\n",
            " isotonic cone loss = 0.0147\n",
            "1.0145342350006104\n",
            " isotonic cone loss = 0.0142\n",
            "1.0921862125396729\n",
            " isotonic cone loss = 0.0142\n",
            "1.0967589616775513\n",
            " isotonic cone loss = 0.0141\n",
            "1.1137336492538452\n",
            " isotonic cone loss = 0.0142\n",
            "1.01796555519104\n",
            " isotonic cone loss = 0.0145\n",
            "1.0640166997909546\n",
            " isotonic cone loss = 0.0147\n",
            "0.9905165433883667\n",
            " isotonic cone loss = 0.0142\n",
            "1.0762702226638794\n",
            " isotonic cone loss = 0.0139\n",
            "1.1596249341964722\n",
            " isotonic cone loss = 0.0140\n",
            "1.1271215677261353\n",
            " isotonic cone loss = 0.0139\n",
            "1.1153900623321533\n",
            " isotonic cone loss = 0.0142\n",
            "1.0664963722229004\n",
            " isotonic cone loss = 0.0143\n",
            "1.08953058719635\n",
            " isotonic cone loss = 0.0146\n",
            "1.0752841234207153\n",
            " isotonic cone loss = 0.0140\n",
            "1.0355565547943115\n",
            " isotonic cone loss = 0.0144\n",
            "1.0245561599731445\n",
            " isotonic cone loss = 0.0145\n",
            "1.0612245798110962\n",
            " isotonic cone loss = 0.0143\n",
            "1.0217487812042236\n",
            " isotonic cone loss = 0.0143\n",
            "0.9688518047332764\n",
            " isotonic cone loss = 0.0146\n",
            "1.0087034702301025\n",
            " isotonic cone loss = 0.0145\n",
            "1.1147067546844482\n",
            " isotonic cone loss = 0.0142\n",
            "1.042161464691162\n",
            " isotonic cone loss = 0.0141\n",
            "1.0198734998703003\n",
            " isotonic cone loss = 0.0140\n",
            "1.1487300395965576\n",
            " isotonic cone loss = 0.0144\n",
            "1.0257244110107422\n",
            " isotonic cone loss = 0.0142\n",
            "1.046947956085205\n",
            " isotonic cone loss = 0.0145\n",
            "1.159000277519226\n",
            " isotonic cone loss = 0.0144\n",
            "1.0771291255950928\n",
            " isotonic cone loss = 0.0143\n",
            "1.0288981199264526\n",
            " isotonic cone loss = 0.0142\n",
            "1.0871026515960693\n",
            " isotonic cone loss = 0.0140\n",
            "1.0457671880722046\n",
            " isotonic cone loss = 0.0144\n",
            "0.9852784872055054\n",
            " isotonic cone loss = 0.0145\n",
            "1.0024439096450806\n",
            " isotonic cone loss = 0.0144\n",
            "1.090620756149292\n",
            " isotonic cone loss = 0.0144\n",
            "1.0424003601074219\n",
            " isotonic cone loss = 0.0146\n",
            "1.0721594095230103\n",
            " isotonic cone loss = 0.0143\n",
            "1.070029616355896\n",
            " isotonic cone loss = 0.0145\n",
            "1.062874674797058\n",
            " isotonic cone loss = 0.0143\n",
            "1.0574544668197632\n",
            " isotonic cone loss = 0.0147\n",
            "1.150553822517395\n",
            " isotonic cone loss = 0.0144\n",
            "1.019266963005066\n",
            " isotonic cone loss = 0.0145\n",
            "1.1224552392959595\n",
            " isotonic cone loss = 0.0146\n",
            "1.0377765893936157\n",
            " isotonic cone loss = 0.0142\n",
            "1.0482605695724487\n",
            " isotonic cone loss = 0.0144\n",
            "1.1003022193908691\n",
            " isotonic cone loss = 0.0144\n",
            "1.0326917171478271\n",
            " isotonic cone loss = 0.0143\n",
            "1.1560359001159668\n",
            " isotonic cone loss = 0.0147\n",
            "1.074167251586914\n",
            " isotonic cone loss = 0.0144\n",
            "1.029192328453064\n",
            " isotonic cone loss = 0.0142\n",
            "1.092836856842041\n",
            " isotonic cone loss = 0.0142\n",
            "0.9941117167472839\n",
            " isotonic cone loss = 0.0140\n",
            "1.0588583946228027\n",
            " isotonic cone loss = 0.0139\n",
            "1.0722088813781738\n",
            " isotonic cone loss = 0.0143\n",
            "1.0524098873138428\n",
            " isotonic cone loss = 0.0142\n",
            "1.0130739212036133\n",
            " isotonic cone loss = 0.0143\n",
            "1.1964858770370483\n",
            " isotonic cone loss = 0.0143\n",
            "1.0242974758148193\n",
            " isotonic cone loss = 0.0142\n",
            "1.0519263744354248\n",
            " isotonic cone loss = 0.0140\n",
            "1.0561097860336304\n",
            " isotonic cone loss = 0.0143\n",
            "1.1139215230941772\n",
            " isotonic cone loss = 0.0143\n",
            "1.0417836904525757\n",
            " isotonic cone loss = 0.0145\n",
            "1.0186738967895508\n",
            " isotonic cone loss = 0.0145\n",
            "1.0674833059310913\n",
            " isotonic cone loss = 0.0145\n",
            "1.0701900720596313\n",
            " isotonic cone loss = 0.0141\n",
            "1.060226559638977\n",
            " isotonic cone loss = 0.0143\n",
            "0.9910356998443604\n",
            " isotonic cone loss = 0.0144\n",
            "1.0743775367736816\n",
            " isotonic cone loss = 0.0145\n",
            "1.111920952796936\n",
            " isotonic cone loss = 0.0141\n",
            "1.1232846975326538\n",
            " isotonic cone loss = 0.0144\n",
            "1.1113343238830566\n",
            " isotonic cone loss = 0.0148\n",
            "1.0517877340316772\n",
            " isotonic cone loss = 0.0147\n",
            "1.08489191532135\n",
            " isotonic cone loss = 0.0144\n",
            "1.1454793214797974\n",
            " isotonic cone loss = 0.0143\n",
            "1.1481618881225586\n",
            " isotonic cone loss = 0.0146\n",
            "1.0135282278060913\n",
            " isotonic cone loss = 0.0145\n",
            "1.1415233612060547\n",
            " isotonic cone loss = 0.0142\n",
            "1.0391457080841064\n",
            " isotonic cone loss = 0.0144\n",
            "1.0910025835037231\n",
            " isotonic cone loss = 0.0144\n",
            "1.0150007009506226\n",
            " isotonic cone loss = 0.0148\n",
            "1.0630314350128174\n",
            " isotonic cone loss = 0.0141\n",
            "1.0632635354995728\n",
            " isotonic cone loss = 0.0143\n",
            "1.081984043121338\n",
            " isotonic cone loss = 0.0144\n",
            "1.0783449411392212\n",
            " isotonic cone loss = 0.0141\n",
            "1.0007808208465576\n",
            " isotonic cone loss = 0.0144\n",
            "1.105995774269104\n",
            " isotonic cone loss = 0.0143\n",
            "1.0475507974624634\n",
            " isotonic cone loss = 0.0145\n",
            "1.0612345933914185\n",
            " isotonic cone loss = 0.0142\n",
            "1.0800676345825195\n",
            " isotonic cone loss = 0.0144\n",
            "1.1251542568206787\n",
            " isotonic cone loss = 0.0146\n",
            "1.1418253183364868\n",
            " isotonic cone loss = 0.0144\n",
            "1.08119535446167\n",
            " isotonic cone loss = 0.0146\n",
            "1.0923157930374146\n",
            " isotonic cone loss = 0.0144\n",
            "1.0912190675735474\n",
            " isotonic cone loss = 0.0141\n",
            "1.0843080282211304\n",
            " isotonic cone loss = 0.0142\n",
            "1.0312919616699219\n",
            " isotonic cone loss = 0.0142\n",
            "1.132091999053955\n",
            " isotonic cone loss = 0.0138\n",
            "1.0549322366714478\n",
            " isotonic cone loss = 0.0144\n",
            "1.041486382484436\n",
            " isotonic cone loss = 0.0144\n",
            "1.0653350353240967\n",
            " isotonic cone loss = 0.0144\n",
            "1.1217913627624512\n",
            " isotonic cone loss = 0.0143\n",
            "1.0188095569610596\n",
            " isotonic cone loss = 0.0143\n",
            "1.1093450784683228\n",
            " isotonic cone loss = 0.0142\n",
            "1.0651671886444092\n",
            " isotonic cone loss = 0.0139\n",
            "1.0018953084945679\n",
            " isotonic cone loss = 0.0143\n",
            "1.120222806930542\n",
            " isotonic cone loss = 0.0146\n",
            "1.0827972888946533\n",
            " isotonic cone loss = 0.0144\n",
            "1.1100378036499023\n",
            " isotonic cone loss = 0.0143\n",
            "1.0052406787872314\n",
            " isotonic cone loss = 0.0142\n",
            "1.0683523416519165\n",
            " isotonic cone loss = 0.0141\n",
            "1.0549070835113525\n",
            " isotonic cone loss = 0.0148\n",
            "0.9728000164031982\n",
            " isotonic cone loss = 0.0144\n",
            "1.069850206375122\n",
            " isotonic cone loss = 0.0146\n",
            "0.9265909790992737\n",
            " isotonic cone loss = 0.0143\n",
            "1.1125389337539673\n",
            " isotonic cone loss = 0.0143\n",
            "1.022067666053772\n",
            " isotonic cone loss = 0.0141\n",
            "1.0880053043365479\n",
            " isotonic cone loss = 0.0142\n",
            "1.0409284830093384\n",
            " isotonic cone loss = 0.0140\n",
            "1.0079423189163208\n",
            " isotonic cone loss = 0.0142\n",
            "1.1568608283996582\n",
            " isotonic cone loss = 0.0142\n",
            "1.122864842414856\n",
            " isotonic cone loss = 0.0143\n",
            "1.0575722455978394\n",
            " isotonic cone loss = 0.0144\n",
            "1.0269203186035156\n",
            " isotonic cone loss = 0.0144\n",
            "1.0024285316467285\n",
            " isotonic cone loss = 0.0143\n",
            "1.0805768966674805\n",
            " isotonic cone loss = 0.0141\n",
            "1.0960201025009155\n",
            " isotonic cone loss = 0.0140\n",
            "1.0894109010696411\n",
            " isotonic cone loss = 0.0141\n",
            "1.089640498161316\n",
            " isotonic cone loss = 0.0138\n",
            "1.0915815830230713\n",
            " isotonic cone loss = 0.0142\n",
            "1.0529752969741821\n",
            " isotonic cone loss = 0.0146\n",
            "1.0792648792266846\n",
            " isotonic cone loss = 0.0142\n",
            "1.0579817295074463\n",
            " isotonic cone loss = 0.0142\n",
            "1.0781413316726685\n",
            " isotonic cone loss = 0.0141\n",
            "1.1389498710632324\n",
            " isotonic cone loss = 0.0139\n",
            "1.0041275024414062\n",
            " isotonic cone loss = 0.0141\n",
            "1.0630460977554321\n",
            " isotonic cone loss = 0.0142\n",
            "0.9458723068237305\n",
            " isotonic cone loss = 0.0143\n",
            "1.1449657678604126\n",
            " isotonic cone loss = 0.0138\n",
            "1.1257954835891724\n",
            " isotonic cone loss = 0.0142\n",
            "1.0377576351165771\n",
            " isotonic cone loss = 0.0139\n",
            "1.1431206464767456\n",
            " isotonic cone loss = 0.0142\n",
            "1.032210111618042\n",
            " isotonic cone loss = 0.0142\n",
            "0.9816809296607971\n",
            " isotonic cone loss = 0.0144\n",
            "1.0127140283584595\n",
            " isotonic cone loss = 0.0143\n",
            "1.1279703378677368\n",
            " isotonic cone loss = 0.0144\n",
            "1.058658480644226\n",
            " isotonic cone loss = 0.0141\n",
            "0.9910066723823547\n",
            " isotonic cone loss = 0.0139\n",
            "1.1013712882995605\n",
            " isotonic cone loss = 0.0138\n",
            "1.0949320793151855\n",
            " isotonic cone loss = 0.0139\n",
            "1.0484504699707031\n",
            " isotonic cone loss = 0.0143\n",
            "1.0276800394058228\n",
            " isotonic cone loss = 0.0144\n",
            "0.9502615928649902\n",
            " isotonic cone loss = 0.0142\n",
            "1.002801775932312\n",
            " isotonic cone loss = 0.0145\n",
            "1.0865437984466553\n",
            " isotonic cone loss = 0.0141\n",
            "0.9975824356079102\n",
            " isotonic cone loss = 0.0144\n",
            "1.05692720413208\n",
            " isotonic cone loss = 0.0143\n",
            "1.05545973777771\n",
            " isotonic cone loss = 0.0141\n",
            "1.0268898010253906\n",
            " isotonic cone loss = 0.0140\n",
            "0.9810458421707153\n",
            " isotonic cone loss = 0.0139\n",
            "1.0127533674240112\n",
            " isotonic cone loss = 0.0138\n",
            "1.068254828453064\n",
            " isotonic cone loss = 0.0145\n",
            "1.1013875007629395\n",
            " isotonic cone loss = 0.0144\n",
            "0.9929437637329102\n",
            " isotonic cone loss = 0.0144\n",
            "1.0703151226043701\n",
            " isotonic cone loss = 0.0144\n",
            "1.0328476428985596\n",
            " isotonic cone loss = 0.0142\n",
            "0.9888043403625488\n",
            " isotonic cone loss = 0.0140\n",
            "1.0564109086990356\n",
            " isotonic cone loss = 0.0142\n",
            "1.0431112051010132\n",
            " isotonic cone loss = 0.0143\n",
            "1.0470080375671387\n",
            " isotonic cone loss = 0.0141\n",
            "1.0149390697479248\n",
            " isotonic cone loss = 0.0145\n",
            "1.0071951150894165\n",
            " isotonic cone loss = 0.0142\n",
            "1.0306824445724487\n",
            " isotonic cone loss = 0.0145\n",
            "1.0487618446350098\n",
            " isotonic cone loss = 0.0142\n",
            "1.0590459108352661\n",
            " isotonic cone loss = 0.0144\n",
            "1.0540704727172852\n",
            " isotonic cone loss = 0.0144\n",
            "1.0282313823699951\n",
            " isotonic cone loss = 0.0146\n",
            "1.0573667287826538\n",
            " isotonic cone loss = 0.0144\n",
            "1.0185242891311646\n",
            " isotonic cone loss = 0.0146\n",
            "0.9741533398628235\n",
            " isotonic cone loss = 0.0146\n",
            "1.0149447917938232\n",
            " isotonic cone loss = 0.0143\n",
            "1.0879606008529663\n",
            " isotonic cone loss = 0.0145\n",
            "1.0805463790893555\n",
            " isotonic cone loss = 0.0142\n",
            "1.0868061780929565\n",
            " isotonic cone loss = 0.0141\n",
            "1.0389493703842163\n",
            " isotonic cone loss = 0.0140\n",
            "1.0506689548492432\n",
            " isotonic cone loss = 0.0142\n",
            "1.0852082967758179\n",
            " isotonic cone loss = 0.0144\n",
            "1.069340467453003\n",
            " isotonic cone loss = 0.0145\n",
            "0.9716062545776367\n",
            " isotonic cone loss = 0.0140\n",
            "1.0573214292526245\n",
            " isotonic cone loss = 0.0145\n",
            "1.1068838834762573\n",
            " isotonic cone loss = 0.0141\n",
            "1.010467529296875\n",
            " isotonic cone loss = 0.0143\n",
            "1.0413026809692383\n",
            " isotonic cone loss = 0.0147\n",
            "1.0949095487594604\n",
            " isotonic cone loss = 0.0143\n",
            "1.0226434469223022\n",
            " isotonic cone loss = 0.0143\n",
            "1.0758987665176392\n",
            " isotonic cone loss = 0.0143\n",
            "1.0144903659820557\n",
            " isotonic cone loss = 0.0141\n",
            "1.0541706085205078\n",
            " isotonic cone loss = 0.0141\n",
            "1.1346515417099\n",
            " isotonic cone loss = 0.0142\n",
            "0.9608520865440369\n",
            " isotonic cone loss = 0.0142\n",
            "0.9530684351921082\n",
            " isotonic cone loss = 0.0144\n",
            "1.056097149848938\n",
            " isotonic cone loss = 0.0139\n",
            "1.1334474086761475\n",
            " isotonic cone loss = 0.0140\n",
            "1.0284315347671509\n",
            " isotonic cone loss = 0.0144\n",
            "0.9947774410247803\n",
            " isotonic cone loss = 0.0144\n",
            "1.0367035865783691\n",
            " isotonic cone loss = 0.0142\n",
            "0.9909152984619141\n",
            " isotonic cone loss = 0.0141\n",
            "1.115280270576477\n",
            " isotonic cone loss = 0.0146\n",
            "1.0511455535888672\n",
            " isotonic cone loss = 0.0144\n",
            "1.0000396966934204\n",
            " isotonic cone loss = 0.0139\n",
            "1.0340354442596436\n",
            " isotonic cone loss = 0.0140\n",
            "1.0702110528945923\n",
            " isotonic cone loss = 0.0143\n",
            "0.9986353516578674\n",
            " isotonic cone loss = 0.0142\n",
            "1.0714412927627563\n",
            " isotonic cone loss = 0.0143\n",
            "1.1001979112625122\n",
            " isotonic cone loss = 0.0148\n",
            "1.0755974054336548\n",
            " isotonic cone loss = 0.0141\n",
            "1.1617586612701416\n",
            " isotonic cone loss = 0.0142\n",
            "1.0117863416671753\n",
            " isotonic cone loss = 0.0143\n",
            "0.9822511076927185\n",
            " isotonic cone loss = 0.0143\n",
            "0.9782833456993103\n",
            " isotonic cone loss = 0.0145\n",
            "1.0665851831436157\n",
            " isotonic cone loss = 0.0143\n",
            "1.0339702367782593\n",
            " isotonic cone loss = 0.0144\n",
            "1.0791518688201904\n",
            " isotonic cone loss = 0.0143\n",
            "1.0420408248901367\n",
            " isotonic cone loss = 0.0141\n",
            "1.0845110416412354\n",
            " isotonic cone loss = 0.0143\n",
            "1.0675668716430664\n",
            " isotonic cone loss = 0.0143\n",
            "1.0285472869873047\n",
            " isotonic cone loss = 0.0140\n",
            "1.064117670059204\n",
            " isotonic cone loss = 0.0147\n",
            "1.0565630197525024\n",
            " isotonic cone loss = 0.0144\n",
            "1.0647740364074707\n",
            " isotonic cone loss = 0.0144\n",
            "1.0964329242706299\n",
            " isotonic cone loss = 0.0141\n",
            "1.1717445850372314\n",
            " isotonic cone loss = 0.0140\n",
            "0.9635260105133057\n",
            " isotonic cone loss = 0.0144\n",
            "1.0609793663024902\n",
            " isotonic cone loss = 0.0142\n",
            "1.0077136754989624\n",
            " isotonic cone loss = 0.0145\n",
            "1.0196548700332642\n",
            " isotonic cone loss = 0.0145\n",
            "0.9976556897163391\n",
            " isotonic cone loss = 0.0142\n",
            "1.0934277772903442\n",
            " isotonic cone loss = 0.0142\n",
            "1.0186911821365356\n",
            " isotonic cone loss = 0.0142\n",
            "1.0441163778305054\n",
            " isotonic cone loss = 0.0143\n",
            "1.0407848358154297\n",
            " isotonic cone loss = 0.0142\n",
            "1.0448063611984253\n",
            " isotonic cone loss = 0.0142\n",
            "1.0953037738800049\n",
            " isotonic cone loss = 0.0142\n",
            "0.9353037476539612\n",
            " isotonic cone loss = 0.0140\n",
            "1.0420308113098145\n",
            " isotonic cone loss = 0.0142\n",
            "1.1424221992492676\n",
            " isotonic cone loss = 0.0142\n",
            "0.9526450634002686\n",
            " isotonic cone loss = 0.0140\n",
            "1.0913989543914795\n",
            " isotonic cone loss = 0.0142\n",
            "0.998708963394165\n",
            " isotonic cone loss = 0.0140\n",
            "1.0033177137374878\n",
            " isotonic cone loss = 0.0144\n",
            "1.0382181406021118\n",
            " isotonic cone loss = 0.0142\n",
            "0.9886052012443542\n",
            " isotonic cone loss = 0.0142\n",
            "0.9388393759727478\n",
            " isotonic cone loss = 0.0145\n",
            "0.9374434351921082\n",
            " isotonic cone loss = 0.0142\n",
            "1.0172598361968994\n",
            " isotonic cone loss = 0.0143\n",
            "1.0520403385162354\n",
            " isotonic cone loss = 0.0140\n",
            "1.0020707845687866\n",
            " isotonic cone loss = 0.0140\n",
            "1.0253900289535522\n",
            " isotonic cone loss = 0.0142\n",
            "0.9575710892677307\n",
            " isotonic cone loss = 0.0143\n",
            "0.9664710164070129\n",
            " isotonic cone loss = 0.0143\n",
            "0.9678462743759155\n",
            " isotonic cone loss = 0.0143\n",
            "0.9512932896614075\n",
            " isotonic cone loss = 0.0143\n",
            "0.8993157148361206\n",
            " isotonic cone loss = 0.0140\n",
            "1.0293985605239868\n",
            " isotonic cone loss = 0.0140\n",
            "0.9965845942497253\n",
            " isotonic cone loss = 0.0142\n",
            "0.9663151502609253\n",
            " isotonic cone loss = 0.0144\n",
            "0.9959926009178162\n",
            " isotonic cone loss = 0.0145\n",
            "1.0318512916564941\n",
            " isotonic cone loss = 0.0141\n",
            "1.0014386177062988\n",
            " isotonic cone loss = 0.0143\n",
            "0.9717710614204407\n",
            " isotonic cone loss = 0.0143\n",
            "1.0090254545211792\n",
            " isotonic cone loss = 0.0141\n",
            "1.022258996963501\n",
            " isotonic cone loss = 0.0142\n",
            "1.030903697013855\n",
            " isotonic cone loss = 0.0142\n",
            "0.9314485192298889\n",
            " isotonic cone loss = 0.0141\n",
            "1.1395220756530762\n",
            " isotonic cone loss = 0.0142\n",
            "1.01641047000885\n",
            " isotonic cone loss = 0.0143\n",
            "1.031715989112854\n",
            " isotonic cone loss = 0.0140\n",
            "1.0443079471588135\n",
            " isotonic cone loss = 0.0144\n",
            "0.9347230195999146\n",
            " isotonic cone loss = 0.0145\n",
            "1.1019465923309326\n",
            " isotonic cone loss = 0.0143\n",
            "1.0197488069534302\n",
            " isotonic cone loss = 0.0146\n",
            "1.029064416885376\n",
            " isotonic cone loss = 0.0143\n",
            "1.0646253824234009\n",
            " isotonic cone loss = 0.0145\n",
            "1.0014963150024414\n",
            " isotonic cone loss = 0.0142\n",
            "1.011863112449646\n",
            " isotonic cone loss = 0.0143\n",
            "0.9807948470115662\n",
            " isotonic cone loss = 0.0143\n",
            "1.050816535949707\n",
            " isotonic cone loss = 0.0147\n",
            "1.122031807899475\n",
            " isotonic cone loss = 0.0140\n",
            "1.0301791429519653\n",
            " isotonic cone loss = 0.0144\n",
            "0.975951075553894\n",
            " isotonic cone loss = 0.0144\n",
            "1.0422186851501465\n",
            " isotonic cone loss = 0.0140\n",
            "1.0752809047698975\n",
            " isotonic cone loss = 0.0144\n",
            "0.9643945097923279\n",
            " isotonic cone loss = 0.0141\n",
            "1.1314910650253296\n",
            " isotonic cone loss = 0.0142\n",
            "1.0964547395706177\n",
            " isotonic cone loss = 0.0143\n",
            "0.9754430055618286\n",
            " isotonic cone loss = 0.0142\n",
            "1.0732051134109497\n",
            " isotonic cone loss = 0.0142\n",
            "1.051432490348816\n",
            " isotonic cone loss = 0.0142\n",
            "1.0043904781341553\n",
            " isotonic cone loss = 0.0142\n",
            "0.9956667423248291\n",
            " isotonic cone loss = 0.0138\n",
            "0.9550737738609314\n",
            " isotonic cone loss = 0.0142\n",
            "1.0614855289459229\n",
            " isotonic cone loss = 0.0142\n",
            "0.9666405916213989\n",
            " isotonic cone loss = 0.0138\n",
            "1.0240548849105835\n",
            " isotonic cone loss = 0.0141\n",
            "1.0188660621643066\n",
            " isotonic cone loss = 0.0143\n",
            "0.9870167374610901\n",
            " isotonic cone loss = 0.0141\n",
            "1.0078003406524658\n",
            " isotonic cone loss = 0.0139\n",
            "1.12044358253479\n",
            " isotonic cone loss = 0.0138\n",
            "1.0514860153198242\n",
            " isotonic cone loss = 0.0140\n",
            "1.0103169679641724\n",
            " isotonic cone loss = 0.0142\n",
            "0.9996408820152283\n",
            " isotonic cone loss = 0.0144\n",
            "1.008143663406372\n",
            " isotonic cone loss = 0.0144\n",
            "1.0159403085708618\n",
            " isotonic cone loss = 0.0140\n",
            "1.0314644575119019\n",
            " isotonic cone loss = 0.0142\n",
            "1.055964469909668\n",
            " isotonic cone loss = 0.0137\n",
            "0.9930810332298279\n",
            " isotonic cone loss = 0.0140\n",
            "1.035008430480957\n",
            " isotonic cone loss = 0.0142\n",
            "1.0697768926620483\n",
            " isotonic cone loss = 0.0144\n",
            "0.9507105350494385\n",
            " isotonic cone loss = 0.0143\n",
            "1.0317684412002563\n",
            " isotonic cone loss = 0.0146\n",
            "0.9598456025123596\n",
            " isotonic cone loss = 0.0140\n",
            "1.0812652111053467\n",
            " isotonic cone loss = 0.0145\n",
            "0.9661430716514587\n",
            " isotonic cone loss = 0.0142\n",
            "0.9791412949562073\n",
            " isotonic cone loss = 0.0142\n",
            "0.9865889549255371\n",
            " isotonic cone loss = 0.0139\n",
            "1.0112520456314087\n",
            " isotonic cone loss = 0.0142\n",
            "0.9720970392227173\n",
            " isotonic cone loss = 0.0139\n",
            "0.9452807307243347\n",
            " isotonic cone loss = 0.0143\n",
            "1.0087742805480957\n",
            " isotonic cone loss = 0.0142\n",
            "0.9447649717330933\n",
            " isotonic cone loss = 0.0140\n",
            "1.012975811958313\n",
            " isotonic cone loss = 0.0144\n",
            "0.9622016549110413\n",
            " isotonic cone loss = 0.0142\n",
            "1.0925865173339844\n",
            " isotonic cone loss = 0.0142\n",
            "1.0391371250152588\n",
            " isotonic cone loss = 0.0138\n",
            "1.0642290115356445\n",
            " isotonic cone loss = 0.0142\n",
            "0.9327024221420288\n",
            " isotonic cone loss = 0.0143\n",
            "0.9433965682983398\n",
            " isotonic cone loss = 0.0141\n",
            "0.9740936756134033\n",
            " isotonic cone loss = 0.0140\n",
            "1.027748465538025\n",
            " isotonic cone loss = 0.0144\n",
            "1.018602728843689\n",
            " isotonic cone loss = 0.0143\n",
            "0.9227592945098877\n",
            " isotonic cone loss = 0.0142\n",
            "1.075508713722229\n",
            " isotonic cone loss = 0.0145\n",
            "0.9988152980804443\n",
            " isotonic cone loss = 0.0143\n",
            "1.0170072317123413\n",
            " isotonic cone loss = 0.0142\n",
            "1.0361557006835938\n",
            " isotonic cone loss = 0.0141\n",
            "0.9901629686355591\n",
            " isotonic cone loss = 0.0145\n",
            "0.990882158279419\n",
            " isotonic cone loss = 0.0142\n",
            "0.9586564302444458\n",
            " isotonic cone loss = 0.0145\n",
            "1.0644581317901611\n",
            " isotonic cone loss = 0.0144\n",
            "1.0684870481491089\n",
            " isotonic cone loss = 0.0142\n",
            "1.061721682548523\n",
            " isotonic cone loss = 0.0144\n",
            "0.9521694779396057\n",
            " isotonic cone loss = 0.0146\n",
            "0.926986038684845\n",
            " isotonic cone loss = 0.0143\n",
            "1.0305880308151245\n",
            " isotonic cone loss = 0.0145\n",
            "1.0303654670715332\n",
            " isotonic cone loss = 0.0141\n",
            "0.9677507281303406\n",
            " isotonic cone loss = 0.0144\n",
            "1.0294244289398193\n",
            " isotonic cone loss = 0.0143\n",
            "1.0088428258895874\n",
            " isotonic cone loss = 0.0143\n",
            "1.0495291948318481\n",
            " isotonic cone loss = 0.0140\n",
            "1.0480901002883911\n",
            " isotonic cone loss = 0.0143\n",
            "1.0158252716064453\n",
            " isotonic cone loss = 0.0141\n",
            "1.1464594602584839\n",
            " isotonic cone loss = 0.0140\n",
            "1.0673400163650513\n",
            " isotonic cone loss = 0.0143\n",
            "1.004536509513855\n",
            " isotonic cone loss = 0.0146\n",
            "0.9740284085273743\n",
            " isotonic cone loss = 0.0141\n",
            "0.9907607436180115\n",
            " isotonic cone loss = 0.0143\n",
            "0.9984781742095947\n",
            " isotonic cone loss = 0.0142\n",
            "0.932905912399292\n",
            " isotonic cone loss = 0.0138\n",
            "0.938266932964325\n",
            " isotonic cone loss = 0.0142\n",
            "0.9855062961578369\n",
            " isotonic cone loss = 0.0142\n",
            "0.9909396171569824\n",
            " isotonic cone loss = 0.0137\n",
            "1.128400206565857\n",
            " isotonic cone loss = 0.0139\n",
            "0.9820827841758728\n",
            " isotonic cone loss = 0.0140\n",
            "0.9728842377662659\n",
            " isotonic cone loss = 0.0141\n",
            "0.9806151986122131\n",
            " isotonic cone loss = 0.0141\n",
            "0.9713741540908813\n",
            " isotonic cone loss = 0.0139\n",
            "0.9878536462783813\n",
            " isotonic cone loss = 0.0140\n",
            "1.0043236017227173\n",
            " isotonic cone loss = 0.0143\n",
            "0.9829222559928894\n",
            " isotonic cone loss = 0.0142\n",
            "0.9943717122077942\n",
            " isotonic cone loss = 0.0143\n",
            "1.0389024019241333\n",
            " isotonic cone loss = 0.0143\n",
            "1.0311737060546875\n",
            " isotonic cone loss = 0.0141\n",
            "0.9986008405685425\n",
            " isotonic cone loss = 0.0139\n",
            "1.0755767822265625\n",
            " isotonic cone loss = 0.0141\n",
            "1.1037641763687134\n",
            " isotonic cone loss = 0.0138\n",
            "1.0631638765335083\n",
            " isotonic cone loss = 0.0143\n",
            "1.0335277318954468\n",
            " isotonic cone loss = 0.0145\n",
            "0.9822449684143066\n",
            " isotonic cone loss = 0.0145\n",
            "1.0306134223937988\n",
            " isotonic cone loss = 0.0140\n",
            "1.0769712924957275\n",
            " isotonic cone loss = 0.0140\n",
            "0.9677011370658875\n",
            " isotonic cone loss = 0.0139\n",
            "0.9979718923568726\n",
            " isotonic cone loss = 0.0142\n",
            "1.0525634288787842\n",
            " isotonic cone loss = 0.0141\n",
            "1.0498218536376953\n",
            " isotonic cone loss = 0.0143\n",
            "1.0219502449035645\n",
            " isotonic cone loss = 0.0139\n",
            "1.032732367515564\n",
            " isotonic cone loss = 0.0139\n",
            "0.9695706367492676\n",
            " isotonic cone loss = 0.0141\n",
            "0.9609514474868774\n",
            " isotonic cone loss = 0.0140\n",
            "0.9409538507461548\n",
            " isotonic cone loss = 0.0140\n",
            "1.0074952840805054\n",
            " isotonic cone loss = 0.0144\n",
            "0.9153794646263123\n",
            " isotonic cone loss = 0.0143\n",
            "0.9738239049911499\n",
            " isotonic cone loss = 0.0139\n",
            "0.9015867114067078\n",
            " isotonic cone loss = 0.0139\n",
            "0.9078317284584045\n",
            " isotonic cone loss = 0.0142\n",
            "1.08004629611969\n",
            " isotonic cone loss = 0.0140\n",
            "1.019349217414856\n",
            " isotonic cone loss = 0.0143\n",
            "0.9887727499008179\n",
            " isotonic cone loss = 0.0140\n",
            "1.0032124519348145\n",
            " isotonic cone loss = 0.0142\n",
            "1.0339394807815552\n",
            " isotonic cone loss = 0.0141\n",
            "0.998867392539978\n",
            " isotonic cone loss = 0.0141\n",
            "0.9401774406433105\n",
            " isotonic cone loss = 0.0140\n",
            "1.0471830368041992\n",
            " isotonic cone loss = 0.0142\n",
            "0.9291201233863831\n",
            " isotonic cone loss = 0.0144\n",
            "0.9945577383041382\n",
            " isotonic cone loss = 0.0144\n",
            "0.9801766872406006\n",
            " isotonic cone loss = 0.0141\n",
            "0.9779500961303711\n",
            " isotonic cone loss = 0.0141\n",
            "1.1012321710586548\n",
            " isotonic cone loss = 0.0140\n",
            "1.0282487869262695\n",
            " isotonic cone loss = 0.0140\n",
            "0.9709162712097168\n",
            " isotonic cone loss = 0.0142\n",
            "1.0829033851623535\n",
            " isotonic cone loss = 0.0141\n",
            "0.9635475873947144\n",
            " isotonic cone loss = 0.0144\n",
            "0.9535446166992188\n",
            " isotonic cone loss = 0.0140\n",
            "0.9528920650482178\n",
            " isotonic cone loss = 0.0140\n",
            "0.9808086156845093\n",
            " isotonic cone loss = 0.0137\n",
            "0.9762285351753235\n",
            " isotonic cone loss = 0.0140\n",
            "1.0256818532943726\n",
            " isotonic cone loss = 0.0141\n",
            "0.956082284450531\n",
            " isotonic cone loss = 0.0142\n",
            "0.9008670449256897\n",
            " isotonic cone loss = 0.0144\n",
            "1.033360481262207\n",
            " isotonic cone loss = 0.0143\n",
            "1.092166781425476\n",
            " isotonic cone loss = 0.0137\n",
            "0.9582938551902771\n",
            " isotonic cone loss = 0.0141\n",
            "0.9214346408843994\n",
            " isotonic cone loss = 0.0139\n",
            "0.9727190136909485\n",
            " isotonic cone loss = 0.0141\n",
            "1.0274986028671265\n",
            " isotonic cone loss = 0.0142\n",
            "0.953658938407898\n",
            " isotonic cone loss = 0.0141\n",
            "0.9954227805137634\n",
            " isotonic cone loss = 0.0142\n",
            "0.9701594710350037\n",
            " isotonic cone loss = 0.0141\n",
            "0.937393069267273\n",
            " isotonic cone loss = 0.0138\n",
            "1.0078165531158447\n",
            " isotonic cone loss = 0.0139\n",
            "0.9497235417366028\n",
            " isotonic cone loss = 0.0142\n",
            "0.9420394897460938\n",
            " isotonic cone loss = 0.0141\n",
            "0.9178757071495056\n",
            " isotonic cone loss = 0.0143\n",
            "0.8915808796882629\n",
            " isotonic cone loss = 0.0142\n",
            "1.0734543800354004\n",
            " isotonic cone loss = 0.0140\n",
            "1.0044584274291992\n",
            " isotonic cone loss = 0.0139\n",
            "1.0345265865325928\n",
            " isotonic cone loss = 0.0140\n",
            "1.1361771821975708\n",
            " isotonic cone loss = 0.0140\n",
            "1.0180827379226685\n",
            " isotonic cone loss = 0.0138\n",
            "0.9338998198509216\n",
            " isotonic cone loss = 0.0142\n",
            "0.9931055307388306\n",
            " isotonic cone loss = 0.0141\n",
            "0.9801005721092224\n",
            " isotonic cone loss = 0.0141\n",
            "0.9001863598823547\n",
            " isotonic cone loss = 0.0142\n",
            "0.9899493455886841\n",
            " isotonic cone loss = 0.0145\n",
            "1.0219618082046509\n",
            " isotonic cone loss = 0.0139\n",
            "0.9320328831672668\n",
            " isotonic cone loss = 0.0139\n",
            "0.9818103909492493\n",
            " isotonic cone loss = 0.0142\n",
            "0.869053065776825\n",
            " isotonic cone loss = 0.0142\n",
            "0.8647121787071228\n",
            " isotonic cone loss = 0.0144\n",
            "0.9923883676528931\n",
            " isotonic cone loss = 0.0142\n",
            "0.9228278994560242\n",
            " isotonic cone loss = 0.0140\n",
            "0.8905230164527893\n",
            " isotonic cone loss = 0.0138\n",
            "1.0260236263275146\n",
            " isotonic cone loss = 0.0139\n",
            "1.0418903827667236\n",
            " isotonic cone loss = 0.0140\n",
            "1.0718342065811157\n",
            " isotonic cone loss = 0.0144\n",
            "1.0033711194992065\n",
            " isotonic cone loss = 0.0142\n",
            "0.964002788066864\n",
            " isotonic cone loss = 0.0142\n",
            "0.985548198223114\n",
            " isotonic cone loss = 0.0140\n",
            "1.096252679824829\n",
            " isotonic cone loss = 0.0142\n",
            "0.9976661801338196\n",
            " isotonic cone loss = 0.0139\n",
            "1.0558547973632812\n",
            " isotonic cone loss = 0.0139\n",
            "1.081741213798523\n",
            " isotonic cone loss = 0.0141\n",
            "1.0150954723358154\n",
            " isotonic cone loss = 0.0142\n",
            "0.8717407584190369\n",
            " isotonic cone loss = 0.0140\n",
            "0.9769660830497742\n",
            " isotonic cone loss = 0.0144\n",
            "0.9297896027565002\n",
            " isotonic cone loss = 0.0141\n",
            "0.9992356300354004\n",
            " isotonic cone loss = 0.0143\n",
            "1.0037860870361328\n",
            " isotonic cone loss = 0.0140\n",
            "1.0819520950317383\n",
            " isotonic cone loss = 0.0139\n",
            "0.9726455807685852\n",
            " isotonic cone loss = 0.0141\n",
            "1.0000938177108765\n",
            " isotonic cone loss = 0.0142\n",
            "0.9739850759506226\n",
            " isotonic cone loss = 0.0140\n",
            "1.0073128938674927\n",
            " isotonic cone loss = 0.0140\n",
            "0.9222795963287354\n",
            " isotonic cone loss = 0.0138\n",
            "0.9645124673843384\n",
            " isotonic cone loss = 0.0141\n",
            "0.9650464653968811\n",
            " isotonic cone loss = 0.0141\n",
            "0.9833679795265198\n",
            " isotonic cone loss = 0.0143\n",
            "0.900897204875946\n",
            " isotonic cone loss = 0.0140\n",
            "0.9556903839111328\n",
            " isotonic cone loss = 0.0142\n",
            "0.881307065486908\n",
            " isotonic cone loss = 0.0138\n",
            "1.051137089729309\n",
            " isotonic cone loss = 0.0144\n",
            "1.0409605503082275\n",
            " isotonic cone loss = 0.0141\n",
            "0.9558488130569458\n",
            " isotonic cone loss = 0.0141\n",
            "0.9635127186775208\n",
            " isotonic cone loss = 0.0143\n",
            "1.056473970413208\n",
            " isotonic cone loss = 0.0141\n",
            "0.9203153848648071\n",
            " isotonic cone loss = 0.0144\n",
            "0.9027018547058105\n",
            " isotonic cone loss = 0.0142\n",
            "1.0293296575546265\n",
            " isotonic cone loss = 0.0139\n",
            "0.9279674291610718\n",
            " isotonic cone loss = 0.0140\n",
            "0.9914243221282959\n",
            " isotonic cone loss = 0.0141\n",
            "1.016844630241394\n",
            " isotonic cone loss = 0.0142\n",
            "0.9786311984062195\n",
            " isotonic cone loss = 0.0144\n",
            "0.9359638690948486\n",
            " isotonic cone loss = 0.0144\n",
            "0.9498059153556824\n",
            " isotonic cone loss = 0.0141\n",
            "0.8969974517822266\n",
            " isotonic cone loss = 0.0141\n",
            "0.9681581854820251\n",
            " isotonic cone loss = 0.0139\n",
            "0.9369726181030273\n",
            " isotonic cone loss = 0.0142\n",
            "0.8964982032775879\n",
            " isotonic cone loss = 0.0140\n",
            "1.0190974473953247\n",
            " isotonic cone loss = 0.0142\n",
            "0.908313512802124\n",
            " isotonic cone loss = 0.0144\n",
            "0.9600684642791748\n",
            " isotonic cone loss = 0.0142\n",
            "1.0044804811477661\n",
            " isotonic cone loss = 0.0142\n",
            "0.9894012808799744\n",
            " isotonic cone loss = 0.0142\n",
            "1.0115102529525757\n",
            " isotonic cone loss = 0.0143\n",
            "1.0050393342971802\n",
            " isotonic cone loss = 0.0143\n",
            "1.0249594449996948\n",
            " isotonic cone loss = 0.0146\n",
            "0.873505711555481\n",
            " isotonic cone loss = 0.0142\n",
            "0.9313139915466309\n",
            " isotonic cone loss = 0.0142\n",
            "0.9867669939994812\n",
            " isotonic cone loss = 0.0139\n",
            "1.0937812328338623\n",
            " isotonic cone loss = 0.0139\n",
            "0.9704403281211853\n",
            " isotonic cone loss = 0.0143\n",
            "1.0233705043792725\n",
            " isotonic cone loss = 0.0143\n",
            "0.9466387629508972\n",
            " isotonic cone loss = 0.0140\n",
            "1.0265311002731323\n",
            " isotonic cone loss = 0.0144\n",
            "0.9902006387710571\n",
            " isotonic cone loss = 0.0139\n",
            "1.0043081045150757\n",
            " isotonic cone loss = 0.0140\n",
            "1.0460141897201538\n",
            " isotonic cone loss = 0.0143\n",
            "1.043550729751587\n",
            " isotonic cone loss = 0.0141\n",
            "0.9735895395278931\n",
            " isotonic cone loss = 0.0142\n",
            "0.9655222296714783\n",
            " isotonic cone loss = 0.0139\n",
            "0.9760626554489136\n",
            " isotonic cone loss = 0.0143\n",
            "0.9796523451805115\n",
            " isotonic cone loss = 0.0139\n",
            "0.9655532836914062\n",
            " isotonic cone loss = 0.0141\n",
            "0.9449718594551086\n",
            " isotonic cone loss = 0.0137\n",
            "1.049818992614746\n",
            " isotonic cone loss = 0.0144\n",
            "1.0028036832809448\n",
            " isotonic cone loss = 0.0140\n",
            "0.9806298613548279\n",
            " isotonic cone loss = 0.0140\n",
            "0.9726240038871765\n",
            " isotonic cone loss = 0.0138\n",
            "0.9772735238075256\n",
            " isotonic cone loss = 0.0142\n",
            "0.8727200031280518\n",
            " isotonic cone loss = 0.0140\n",
            "1.0091172456741333\n",
            " isotonic cone loss = 0.0138\n",
            "0.9391396045684814\n",
            " isotonic cone loss = 0.0139\n",
            "0.9305484294891357\n",
            " isotonic cone loss = 0.0137\n",
            "1.0082815885543823\n",
            " isotonic cone loss = 0.0141\n",
            "0.9725010991096497\n",
            " isotonic cone loss = 0.0142\n",
            "0.8954721093177795\n",
            " isotonic cone loss = 0.0141\n",
            "1.0497304201126099\n",
            " isotonic cone loss = 0.0143\n",
            "0.9788705706596375\n",
            " isotonic cone loss = 0.0141\n",
            "0.8735138177871704\n",
            " isotonic cone loss = 0.0140\n",
            "0.9874169230461121\n",
            " isotonic cone loss = 0.0139\n",
            "1.0347439050674438\n",
            " isotonic cone loss = 0.0140\n",
            "0.9414820075035095\n",
            " isotonic cone loss = 0.0141\n",
            "0.9997100234031677\n",
            " isotonic cone loss = 0.0141\n",
            "0.98027104139328\n",
            " isotonic cone loss = 0.0142\n",
            "1.04618501663208\n",
            " isotonic cone loss = 0.0141\n",
            "0.9350485801696777\n",
            " isotonic cone loss = 0.0137\n",
            "0.9174513220787048\n",
            " isotonic cone loss = 0.0142\n",
            "1.0751361846923828\n",
            " isotonic cone loss = 0.0143\n",
            "0.9823527336120605\n",
            " isotonic cone loss = 0.0142\n",
            "1.009997010231018\n",
            " isotonic cone loss = 0.0143\n",
            "0.9247967600822449\n",
            " isotonic cone loss = 0.0142\n",
            "0.9842115640640259\n",
            " isotonic cone loss = 0.0139\n",
            "1.0146994590759277\n",
            " isotonic cone loss = 0.0142\n",
            "0.9419751167297363\n",
            " isotonic cone loss = 0.0137\n",
            "0.9220125079154968\n",
            " isotonic cone loss = 0.0139\n",
            "0.992658793926239\n",
            " isotonic cone loss = 0.0142\n",
            "0.8844659924507141\n",
            " isotonic cone loss = 0.0138\n",
            "0.9104744791984558\n",
            " isotonic cone loss = 0.0141\n",
            "0.9862030148506165\n",
            " isotonic cone loss = 0.0140\n",
            "1.05522620677948\n",
            " isotonic cone loss = 0.0141\n",
            "1.0278315544128418\n",
            " isotonic cone loss = 0.0142\n",
            "0.8743969202041626\n",
            " isotonic cone loss = 0.0143\n",
            "1.0294387340545654\n",
            " isotonic cone loss = 0.0140\n",
            "0.9765772223472595\n",
            " isotonic cone loss = 0.0140\n",
            "1.0512961149215698\n",
            " isotonic cone loss = 0.0144\n",
            "0.9547190070152283\n",
            " isotonic cone loss = 0.0139\n",
            "0.9215915203094482\n",
            " isotonic cone loss = 0.0139\n",
            "0.9254422187805176\n",
            " isotonic cone loss = 0.0141\n",
            "0.9126092195510864\n",
            " isotonic cone loss = 0.0140\n",
            "0.828737199306488\n",
            " isotonic cone loss = 0.0144\n",
            "0.8606122136116028\n",
            " isotonic cone loss = 0.0138\n",
            "1.0165833234786987\n",
            " isotonic cone loss = 0.0142\n",
            "0.9651724100112915\n",
            " isotonic cone loss = 0.0141\n",
            "0.9723542928695679\n",
            " isotonic cone loss = 0.0142\n",
            "0.9014150500297546\n",
            " isotonic cone loss = 0.0139\n",
            "0.9431320428848267\n",
            " isotonic cone loss = 0.0142\n",
            "0.9668655395507812\n",
            " isotonic cone loss = 0.0141\n",
            "0.969612717628479\n",
            " isotonic cone loss = 0.0140\n",
            "0.942029595375061\n",
            " isotonic cone loss = 0.0142\n",
            "0.8735523223876953\n",
            " isotonic cone loss = 0.0139\n",
            "0.9959806799888611\n",
            " isotonic cone loss = 0.0139\n",
            "0.9042778611183167\n",
            " isotonic cone loss = 0.0139\n",
            "0.8523629903793335\n",
            " isotonic cone loss = 0.0141\n",
            "0.940279483795166\n",
            " isotonic cone loss = 0.0142\n",
            "0.9456227421760559\n",
            " isotonic cone loss = 0.0142\n",
            "0.9369796514511108\n",
            " isotonic cone loss = 0.0141\n",
            "0.8089438676834106\n",
            " isotonic cone loss = 0.0144\n",
            "0.944151759147644\n",
            " isotonic cone loss = 0.0138\n",
            "0.9284952282905579\n",
            " isotonic cone loss = 0.0140\n",
            "0.9446418881416321\n",
            " isotonic cone loss = 0.0141\n",
            "0.9635571241378784\n",
            " isotonic cone loss = 0.0141\n",
            "0.9264571666717529\n",
            " isotonic cone loss = 0.0141\n",
            "0.9286178350448608\n",
            " isotonic cone loss = 0.0143\n",
            "1.0242341756820679\n",
            " isotonic cone loss = 0.0141\n",
            "0.9904720783233643\n",
            " isotonic cone loss = 0.0138\n",
            "0.9547492265701294\n",
            " isotonic cone loss = 0.0139\n",
            "0.7865583300590515\n",
            " isotonic cone loss = 0.0139\n",
            "0.9219432473182678\n",
            " isotonic cone loss = 0.0141\n",
            "0.9545302987098694\n",
            " isotonic cone loss = 0.0138\n",
            "0.9275741577148438\n",
            " isotonic cone loss = 0.0137\n",
            "0.9904153943061829\n",
            " isotonic cone loss = 0.0139\n",
            "0.9182692766189575\n",
            " isotonic cone loss = 0.0137\n",
            "1.03118896484375\n",
            " isotonic cone loss = 0.0139\n",
            "0.8811990022659302\n",
            " isotonic cone loss = 0.0140\n",
            "0.9383203983306885\n",
            " isotonic cone loss = 0.0141\n",
            "0.9071412682533264\n",
            " isotonic cone loss = 0.0139\n",
            "0.9485414624214172\n",
            " isotonic cone loss = 0.0141\n",
            "0.8826969861984253\n",
            " isotonic cone loss = 0.0140\n",
            "0.9316672682762146\n",
            " isotonic cone loss = 0.0142\n",
            "0.9729633331298828\n",
            " isotonic cone loss = 0.0141\n",
            "0.8941596150398254\n",
            " isotonic cone loss = 0.0141\n",
            "0.9492045640945435\n",
            " isotonic cone loss = 0.0142\n",
            "0.8680907487869263\n",
            " isotonic cone loss = 0.0142\n",
            "0.9604567289352417\n",
            " isotonic cone loss = 0.0139\n",
            "1.0127888917922974\n",
            " isotonic cone loss = 0.0140\n",
            "0.9452149868011475\n",
            " isotonic cone loss = 0.0138\n",
            "0.9402178525924683\n",
            " isotonic cone loss = 0.0140\n",
            "0.9830126762390137\n",
            " isotonic cone loss = 0.0141\n",
            "0.906814694404602\n",
            " isotonic cone loss = 0.0140\n",
            "0.9533661603927612\n",
            " isotonic cone loss = 0.0141\n",
            "0.9586024880409241\n",
            " isotonic cone loss = 0.0141\n",
            "0.8421356678009033\n",
            " isotonic cone loss = 0.0141\n",
            "1.0225396156311035\n",
            " isotonic cone loss = 0.0140\n",
            "0.8754013776779175\n",
            " isotonic cone loss = 0.0139\n",
            "0.9258080720901489\n",
            " isotonic cone loss = 0.0141\n",
            "1.0073238611221313\n",
            " isotonic cone loss = 0.0140\n",
            "0.8967959880828857\n",
            " isotonic cone loss = 0.0140\n",
            "1.0053915977478027\n",
            " isotonic cone loss = 0.0141\n",
            "0.9231177568435669\n",
            " isotonic cone loss = 0.0141\n",
            "0.9527449011802673\n",
            " isotonic cone loss = 0.0140\n",
            "1.0253820419311523\n",
            " isotonic cone loss = 0.0140\n",
            "0.8782851099967957\n",
            " isotonic cone loss = 0.0140\n",
            "0.8570155501365662\n",
            " isotonic cone loss = 0.0146\n",
            "0.9928151965141296\n",
            " isotonic cone loss = 0.0142\n",
            "0.9314438700675964\n",
            " isotonic cone loss = 0.0142\n",
            "0.9345250725746155\n",
            " isotonic cone loss = 0.0140\n",
            "0.9177064895629883\n",
            " isotonic cone loss = 0.0139\n",
            "0.8943693041801453\n",
            " isotonic cone loss = 0.0137\n",
            "0.9795753359794617\n",
            " isotonic cone loss = 0.0139\n",
            "1.0035940408706665\n",
            " isotonic cone loss = 0.0137\n",
            "0.9875633120536804\n",
            " isotonic cone loss = 0.0140\n",
            "0.9433147311210632\n",
            " isotonic cone loss = 0.0141\n",
            "0.9497972726821899\n",
            " isotonic cone loss = 0.0141\n",
            "0.9167104959487915\n",
            " isotonic cone loss = 0.0145\n",
            "0.959979236125946\n",
            " isotonic cone loss = 0.0142\n",
            "1.0104691982269287\n",
            " isotonic cone loss = 0.0140\n",
            "0.9864547848701477\n",
            " isotonic cone loss = 0.0139\n",
            "1.0633183717727661\n",
            " isotonic cone loss = 0.0140\n",
            "0.9209097027778625\n",
            " isotonic cone loss = 0.0141\n",
            "0.8038982152938843\n",
            " isotonic cone loss = 0.0143\n",
            "0.8924468159675598\n",
            " isotonic cone loss = 0.0141\n",
            "0.8169731497764587\n",
            " isotonic cone loss = 0.0141\n",
            "1.0199029445648193\n",
            " isotonic cone loss = 0.0144\n",
            "0.899083137512207\n",
            " isotonic cone loss = 0.0139\n",
            "0.9438090920448303\n",
            " isotonic cone loss = 0.0143\n",
            "1.00163996219635\n",
            " isotonic cone loss = 0.0142\n",
            "1.0239797830581665\n",
            " isotonic cone loss = 0.0140\n",
            "0.947525143623352\n",
            " isotonic cone loss = 0.0140\n",
            "0.9814770221710205\n",
            " isotonic cone loss = 0.0144\n",
            "0.9867245554924011\n",
            " isotonic cone loss = 0.0141\n",
            "0.9526729583740234\n",
            " isotonic cone loss = 0.0138\n",
            "0.8926867246627808\n",
            " isotonic cone loss = 0.0138\n",
            "0.861038088798523\n",
            " isotonic cone loss = 0.0140\n",
            "0.9510918259620667\n",
            " isotonic cone loss = 0.0144\n",
            "0.9372262954711914\n",
            " isotonic cone loss = 0.0142\n",
            "0.9474345445632935\n",
            " isotonic cone loss = 0.0144\n",
            "0.8048560619354248\n",
            " isotonic cone loss = 0.0141\n",
            "0.8957391381263733\n",
            " isotonic cone loss = 0.0138\n",
            "0.9355460405349731\n",
            " isotonic cone loss = 0.0144\n",
            "1.0754938125610352\n",
            " isotonic cone loss = 0.0143\n",
            "0.9099918007850647\n",
            " isotonic cone loss = 0.0141\n",
            "0.9557578563690186\n",
            " isotonic cone loss = 0.0142\n",
            "0.9547634720802307\n",
            " isotonic cone loss = 0.0142\n",
            "0.9452049732208252\n",
            " isotonic cone loss = 0.0143\n",
            "1.052248239517212\n",
            " isotonic cone loss = 0.0141\n",
            "0.9645065665245056\n",
            " isotonic cone loss = 0.0140\n",
            "0.9513026475906372\n",
            " isotonic cone loss = 0.0142\n",
            "0.9955489039421082\n",
            " isotonic cone loss = 0.0140\n",
            "0.9469093084335327\n",
            " isotonic cone loss = 0.0142\n",
            "0.9765830636024475\n",
            " isotonic cone loss = 0.0143\n",
            "0.8671658039093018\n",
            " isotonic cone loss = 0.0146\n",
            "0.8947537541389465\n",
            " isotonic cone loss = 0.0141\n",
            "0.9414582252502441\n",
            " isotonic cone loss = 0.0144\n",
            "0.9442605972290039\n",
            " isotonic cone loss = 0.0137\n",
            "0.9335993528366089\n",
            " isotonic cone loss = 0.0142\n",
            "0.9580784440040588\n",
            " isotonic cone loss = 0.0139\n",
            "0.9260398745536804\n",
            " isotonic cone loss = 0.0137\n",
            "0.9485814571380615\n",
            " isotonic cone loss = 0.0139\n",
            "0.857315182685852\n",
            " isotonic cone loss = 0.0141\n",
            "0.8975597620010376\n",
            " isotonic cone loss = 0.0142\n",
            "0.9478707313537598\n",
            " isotonic cone loss = 0.0146\n",
            "0.942309558391571\n",
            " isotonic cone loss = 0.0144\n",
            "0.8202863931655884\n",
            " isotonic cone loss = 0.0142\n",
            "0.8527017831802368\n",
            " isotonic cone loss = 0.0142\n",
            "1.0144871473312378\n",
            " isotonic cone loss = 0.0140\n",
            "0.9329060316085815\n",
            " isotonic cone loss = 0.0141\n",
            "0.8667875528335571\n",
            " isotonic cone loss = 0.0139\n",
            "0.9322795271873474\n",
            " isotonic cone loss = 0.0143\n",
            "0.9153333902359009\n",
            " isotonic cone loss = 0.0144\n",
            "0.8439052700996399\n",
            " isotonic cone loss = 0.0139\n",
            "1.0138651132583618\n",
            " isotonic cone loss = 0.0143\n",
            "0.8535158038139343\n",
            " isotonic cone loss = 0.0138\n",
            "1.003719687461853\n",
            " isotonic cone loss = 0.0139\n",
            "0.9166341423988342\n",
            " isotonic cone loss = 0.0136\n",
            "0.9829903841018677\n",
            " isotonic cone loss = 0.0139\n",
            "0.944464385509491\n",
            " isotonic cone loss = 0.0144\n",
            "0.9315970540046692\n",
            " isotonic cone loss = 0.0143\n",
            "0.9252064228057861\n",
            " isotonic cone loss = 0.0140\n",
            "0.9016433954238892\n",
            " isotonic cone loss = 0.0140\n",
            "0.86025071144104\n",
            " isotonic cone loss = 0.0141\n",
            "0.8421207666397095\n",
            " isotonic cone loss = 0.0140\n",
            "0.9783506989479065\n",
            " isotonic cone loss = 0.0139\n",
            "0.8589662313461304\n",
            " isotonic cone loss = 0.0142\n",
            "0.8245565891265869\n",
            " isotonic cone loss = 0.0140\n",
            "1.0383520126342773\n",
            " isotonic cone loss = 0.0140\n",
            "1.0155445337295532\n",
            " isotonic cone loss = 0.0143\n",
            "0.8834134340286255\n",
            " isotonic cone loss = 0.0139\n",
            "0.9103604555130005\n",
            " isotonic cone loss = 0.0141\n",
            "0.9418891072273254\n",
            " isotonic cone loss = 0.0139\n",
            "0.9415174722671509\n",
            " isotonic cone loss = 0.0142\n",
            "0.8778901100158691\n",
            " isotonic cone loss = 0.0143\n",
            "0.7789978981018066\n",
            " isotonic cone loss = 0.0140\n",
            "0.8893256187438965\n",
            " isotonic cone loss = 0.0143\n",
            "0.9055895209312439\n",
            " isotonic cone loss = 0.0140\n",
            "0.8935286998748779\n",
            " isotonic cone loss = 0.0139\n",
            "0.9124318361282349\n",
            " isotonic cone loss = 0.0142\n",
            "0.95419842004776\n",
            " isotonic cone loss = 0.0144\n",
            "0.9018649458885193\n",
            " isotonic cone loss = 0.0141\n",
            "0.8393791317939758\n",
            " isotonic cone loss = 0.0142\n",
            "0.8966362476348877\n",
            " isotonic cone loss = 0.0143\n",
            "0.8871108889579773\n",
            " isotonic cone loss = 0.0141\n",
            "0.9544356465339661\n",
            " isotonic cone loss = 0.0141\n",
            "0.9357366561889648\n",
            " isotonic cone loss = 0.0141\n",
            "0.9247416853904724\n",
            " isotonic cone loss = 0.0142\n",
            "0.9073280096054077\n",
            " isotonic cone loss = 0.0141\n",
            "0.977957010269165\n",
            " isotonic cone loss = 0.0143\n",
            "0.8814935088157654\n",
            " isotonic cone loss = 0.0140\n",
            "0.8561290502548218\n",
            " isotonic cone loss = 0.0142\n",
            "0.8879545331001282\n",
            " isotonic cone loss = 0.0140\n",
            "0.9729490876197815\n",
            " isotonic cone loss = 0.0141\n",
            "0.9854811429977417\n",
            " isotonic cone loss = 0.0140\n",
            "0.8551411032676697\n",
            " isotonic cone loss = 0.0137\n",
            "0.9459879398345947\n",
            " isotonic cone loss = 0.0142\n",
            "0.8675340414047241\n",
            " isotonic cone loss = 0.0139\n",
            "0.8382218480110168\n",
            " isotonic cone loss = 0.0142\n",
            "0.9813180565834045\n",
            " isotonic cone loss = 0.0139\n",
            "0.8575881123542786\n",
            " isotonic cone loss = 0.0140\n",
            "0.9078590273857117\n",
            " isotonic cone loss = 0.0140\n",
            "0.9026784896850586\n",
            " isotonic cone loss = 0.0140\n",
            "0.9222999215126038\n",
            " isotonic cone loss = 0.0140\n",
            "0.8683566451072693\n",
            " isotonic cone loss = 0.0142\n",
            "0.89422208070755\n",
            " isotonic cone loss = 0.0140\n",
            "0.7870488166809082\n",
            " isotonic cone loss = 0.0140\n",
            "0.8233216404914856\n",
            " isotonic cone loss = 0.0144\n",
            "0.8737002015113831\n",
            " isotonic cone loss = 0.0140\n",
            "0.836098849773407\n",
            " isotonic cone loss = 0.0139\n",
            "0.7962436079978943\n",
            " isotonic cone loss = 0.0137\n",
            "0.9661059975624084\n",
            " isotonic cone loss = 0.0141\n",
            "0.9529104828834534\n",
            " isotonic cone loss = 0.0148\n",
            "0.9288214445114136\n",
            " isotonic cone loss = 0.0141\n",
            "0.9625258445739746\n",
            " isotonic cone loss = 0.0141\n",
            "0.9428287148475647\n",
            " isotonic cone loss = 0.0139\n",
            "0.9058434963226318\n",
            " isotonic cone loss = 0.0141\n",
            "0.8732012510299683\n",
            " isotonic cone loss = 0.0142\n",
            "1.0309975147247314\n",
            " isotonic cone loss = 0.0139\n",
            "0.8165138363838196\n",
            " isotonic cone loss = 0.0141\n",
            "0.9508169293403625\n",
            " isotonic cone loss = 0.0142\n",
            "0.9935014843940735\n",
            " isotonic cone loss = 0.0140\n",
            "0.9380124807357788\n",
            " isotonic cone loss = 0.0139\n",
            "0.9499725699424744\n",
            " isotonic cone loss = 0.0138\n",
            "0.9724103808403015\n",
            " isotonic cone loss = 0.0141\n",
            "0.9375820159912109\n",
            " isotonic cone loss = 0.0144\n",
            "0.8771765232086182\n",
            " isotonic cone loss = 0.0141\n",
            "0.8684096336364746\n",
            " isotonic cone loss = 0.0141\n",
            "1.018012523651123\n",
            " isotonic cone loss = 0.0140\n",
            "0.9094501733779907\n",
            " isotonic cone loss = 0.0140\n",
            "0.9476555585861206\n",
            " isotonic cone loss = 0.0141\n",
            "0.9173147678375244\n",
            " isotonic cone loss = 0.0142\n",
            "0.9450293183326721\n",
            " isotonic cone loss = 0.0141\n",
            "1.0036276578903198\n",
            " isotonic cone loss = 0.0139\n",
            "1.075062870979309\n",
            " isotonic cone loss = 0.0139\n",
            "0.941015362739563\n",
            " isotonic cone loss = 0.0137\n",
            "0.8814247250556946\n",
            " isotonic cone loss = 0.0139\n",
            "0.8373340368270874\n",
            " isotonic cone loss = 0.0139\n",
            "0.9862917065620422\n",
            " isotonic cone loss = 0.0140\n",
            "0.8553159832954407\n",
            " isotonic cone loss = 0.0140\n",
            "0.8018398880958557\n",
            " isotonic cone loss = 0.0141\n",
            "0.9097200036048889\n",
            " isotonic cone loss = 0.0143\n",
            "0.8868209719657898\n",
            " isotonic cone loss = 0.0141\n",
            "0.9747155904769897\n",
            " isotonic cone loss = 0.0139\n",
            "0.8883635997772217\n",
            " isotonic cone loss = 0.0137\n",
            "0.8903988003730774\n",
            " isotonic cone loss = 0.0137\n",
            "0.9334056377410889\n",
            " isotonic cone loss = 0.0139\n",
            "0.8678531050682068\n",
            " isotonic cone loss = 0.0143\n",
            "0.969027042388916\n",
            " isotonic cone loss = 0.0140\n",
            "0.9361571669578552\n",
            " isotonic cone loss = 0.0143\n",
            "0.9669928550720215\n",
            " isotonic cone loss = 0.0141\n",
            "0.8189496397972107\n",
            " isotonic cone loss = 0.0141\n",
            "0.8954236507415771\n",
            " isotonic cone loss = 0.0140\n",
            "0.9131496548652649\n",
            " isotonic cone loss = 0.0138\n",
            "0.8435675501823425\n",
            " isotonic cone loss = 0.0142\n",
            "0.9492064118385315\n",
            " isotonic cone loss = 0.0140\n",
            "0.8036013841629028\n",
            " isotonic cone loss = 0.0141\n",
            "0.8769620656967163\n",
            " isotonic cone loss = 0.0137\n",
            "0.9297226667404175\n",
            " isotonic cone loss = 0.0142\n",
            "0.9422010779380798\n",
            " isotonic cone loss = 0.0139\n",
            "0.8273215889930725\n",
            " isotonic cone loss = 0.0144\n",
            "0.8518996834754944\n",
            " isotonic cone loss = 0.0142\n",
            "0.9338204860687256\n",
            " isotonic cone loss = 0.0141\n",
            "0.8959803581237793\n",
            " isotonic cone loss = 0.0143\n",
            "0.8815397620201111\n",
            " isotonic cone loss = 0.0142\n",
            "0.7273364067077637\n",
            " isotonic cone loss = 0.0143\n",
            "0.9476178884506226\n",
            " isotonic cone loss = 0.0137\n",
            "0.9434557557106018\n",
            " isotonic cone loss = 0.0141\n",
            "0.9555259346961975\n",
            " isotonic cone loss = 0.0141\n",
            "0.8313525915145874\n",
            " isotonic cone loss = 0.0140\n",
            "0.8711761832237244\n",
            " isotonic cone loss = 0.0140\n",
            "0.9550672173500061\n",
            " isotonic cone loss = 0.0140\n",
            "0.9501599669456482\n",
            " isotonic cone loss = 0.0141\n",
            "0.8768646121025085\n",
            " isotonic cone loss = 0.0141\n",
            "0.9344121813774109\n",
            " isotonic cone loss = 0.0142\n",
            "0.888399600982666\n",
            " isotonic cone loss = 0.0139\n",
            "0.8530402183532715\n",
            " isotonic cone loss = 0.0140\n",
            "0.806481122970581\n",
            " isotonic cone loss = 0.0143\n",
            "0.8874852657318115\n",
            " isotonic cone loss = 0.0141\n",
            "0.9455583095550537\n",
            " isotonic cone loss = 0.0139\n",
            "0.8098710179328918\n",
            " isotonic cone loss = 0.0140\n",
            "0.8681469559669495\n",
            " isotonic cone loss = 0.0137\n",
            "0.9242717027664185\n",
            " isotonic cone loss = 0.0140\n",
            "1.0030107498168945\n",
            " isotonic cone loss = 0.0138\n",
            "0.9316506385803223\n",
            " isotonic cone loss = 0.0139\n",
            "0.9706094861030579\n",
            " isotonic cone loss = 0.0140\n",
            "0.884370744228363\n",
            " isotonic cone loss = 0.0142\n",
            "0.9074890613555908\n",
            " isotonic cone loss = 0.0144\n",
            "0.89716637134552\n",
            " isotonic cone loss = 0.0144\n",
            "0.9317838549613953\n",
            " isotonic cone loss = 0.0139\n",
            "0.9416490197181702\n",
            " isotonic cone loss = 0.0139\n",
            "0.9118896722793579\n",
            " isotonic cone loss = 0.0140\n",
            "0.9148401618003845\n",
            " isotonic cone loss = 0.0140\n",
            "0.9097808003425598\n",
            " isotonic cone loss = 0.0142\n",
            "0.9370085000991821\n",
            " isotonic cone loss = 0.0139\n",
            "0.9676564335823059\n",
            " isotonic cone loss = 0.0143\n",
            "0.9546117782592773\n",
            " isotonic cone loss = 0.0141\n",
            "0.8710319399833679\n",
            " isotonic cone loss = 0.0143\n",
            "0.9418724775314331\n",
            " isotonic cone loss = 0.0140\n",
            "0.9252724647521973\n",
            " isotonic cone loss = 0.0141\n",
            "0.8677061796188354\n",
            " isotonic cone loss = 0.0138\n",
            "0.8777876496315002\n",
            " isotonic cone loss = 0.0139\n",
            "0.9925228357315063\n",
            " isotonic cone loss = 0.0139\n",
            "0.859868586063385\n",
            " isotonic cone loss = 0.0140\n",
            "0.861079216003418\n",
            " isotonic cone loss = 0.0139\n",
            "0.9443604350090027\n",
            " isotonic cone loss = 0.0141\n",
            "0.8442073464393616\n",
            " isotonic cone loss = 0.0141\n",
            "0.9196054935455322\n",
            " isotonic cone loss = 0.0139\n",
            "0.9157528281211853\n",
            " isotonic cone loss = 0.0142\n",
            "0.9127228260040283\n",
            " isotonic cone loss = 0.0142\n",
            "0.8636569380760193\n",
            " isotonic cone loss = 0.0136\n",
            "0.8643835783004761\n",
            " isotonic cone loss = 0.0141\n",
            "0.9246032238006592\n",
            " isotonic cone loss = 0.0139\n",
            "0.8954563140869141\n",
            " isotonic cone loss = 0.0138\n",
            "0.8635563254356384\n",
            " isotonic cone loss = 0.0138\n",
            "0.9198966026306152\n",
            " isotonic cone loss = 0.0143\n",
            "0.8884352445602417\n",
            " isotonic cone loss = 0.0142\n",
            "0.8654625415802002\n",
            " isotonic cone loss = 0.0142\n",
            "0.8691635131835938\n",
            " isotonic cone loss = 0.0138\n",
            "0.8354277610778809\n",
            " isotonic cone loss = 0.0139\n",
            "0.9036055207252502\n",
            " isotonic cone loss = 0.0139\n",
            "0.9471924304962158\n",
            " isotonic cone loss = 0.0139\n",
            "0.9124080538749695\n",
            " isotonic cone loss = 0.0140\n",
            "0.8930802941322327\n",
            " isotonic cone loss = 0.0139\n",
            "0.8709412813186646\n",
            " isotonic cone loss = 0.0138\n",
            "0.8751096129417419\n",
            " isotonic cone loss = 0.0139\n",
            "0.8830319046974182\n",
            " isotonic cone loss = 0.0140\n",
            "0.9250867962837219\n",
            " isotonic cone loss = 0.0138\n",
            "0.8062402009963989\n",
            " isotonic cone loss = 0.0140\n",
            "0.8194738030433655\n",
            " isotonic cone loss = 0.0143\n",
            "0.8379088640213013\n",
            " isotonic cone loss = 0.0139\n",
            "0.8340225219726562\n",
            " isotonic cone loss = 0.0141\n",
            "0.7764841914176941\n",
            " isotonic cone loss = 0.0142\n",
            "0.9054800271987915\n",
            " isotonic cone loss = 0.0138\n",
            "0.9139752388000488\n",
            " isotonic cone loss = 0.0143\n",
            "0.9078227281570435\n",
            " isotonic cone loss = 0.0143\n",
            "0.8342928290367126\n",
            " isotonic cone loss = 0.0139\n",
            "0.9370499849319458\n",
            " isotonic cone loss = 0.0142\n",
            "0.8240074515342712\n",
            " isotonic cone loss = 0.0140\n",
            "0.8222292065620422\n",
            " isotonic cone loss = 0.0137\n",
            "0.931690514087677\n",
            " isotonic cone loss = 0.0139\n",
            "0.78776615858078\n",
            " isotonic cone loss = 0.0139\n",
            "0.8571494221687317\n",
            " isotonic cone loss = 0.0139\n",
            "0.8189022541046143\n",
            " isotonic cone loss = 0.0137\n",
            "0.974733293056488\n",
            " isotonic cone loss = 0.0139\n",
            "0.8482860326766968\n",
            " isotonic cone loss = 0.0139\n",
            "0.8732149600982666\n",
            " isotonic cone loss = 0.0139\n",
            "0.8618181347846985\n",
            " isotonic cone loss = 0.0141\n",
            "0.9385546445846558\n",
            " isotonic cone loss = 0.0142\n",
            "0.8593474626541138\n",
            " isotonic cone loss = 0.0140\n",
            "0.9120246767997742\n",
            " isotonic cone loss = 0.0139\n",
            "0.864375114440918\n",
            " isotonic cone loss = 0.0140\n",
            "0.9035329818725586\n",
            " isotonic cone loss = 0.0140\n",
            "0.8513565063476562\n",
            " isotonic cone loss = 0.0139\n",
            "0.9287815093994141\n",
            " isotonic cone loss = 0.0141\n",
            "0.795836329460144\n",
            " isotonic cone loss = 0.0138\n",
            "0.8523706197738647\n",
            " isotonic cone loss = 0.0139\n",
            "0.7840490937232971\n",
            " isotonic cone loss = 0.0139\n",
            "0.8575618267059326\n",
            " isotonic cone loss = 0.0142\n",
            "0.8246495127677917\n",
            " isotonic cone loss = 0.0142\n",
            "0.9186747670173645\n",
            " isotonic cone loss = 0.0142\n",
            "0.7818781733512878\n",
            " isotonic cone loss = 0.0142\n",
            "0.7963906526565552\n",
            " isotonic cone loss = 0.0137\n",
            "0.8098591566085815\n",
            " isotonic cone loss = 0.0140\n",
            "0.9070161581039429\n",
            " isotonic cone loss = 0.0139\n",
            "0.9315308928489685\n",
            " isotonic cone loss = 0.0139\n",
            "0.7505034804344177\n",
            " isotonic cone loss = 0.0140\n",
            "0.9345713257789612\n",
            " isotonic cone loss = 0.0142\n",
            "0.9069520831108093\n",
            " isotonic cone loss = 0.0139\n",
            "0.8415059447288513\n",
            " isotonic cone loss = 0.0142\n",
            "0.8708004951477051\n",
            " isotonic cone loss = 0.0138\n",
            "0.8726761341094971\n",
            " isotonic cone loss = 0.0139\n",
            "0.8933828473091125\n",
            " isotonic cone loss = 0.0142\n",
            "0.8687503337860107\n",
            " isotonic cone loss = 0.0140\n",
            "0.8745080232620239\n",
            " isotonic cone loss = 0.0141\n",
            "0.8464476466178894\n",
            " isotonic cone loss = 0.0139\n",
            "0.8339929580688477\n",
            " isotonic cone loss = 0.0139\n",
            "0.7837508320808411\n",
            " isotonic cone loss = 0.0140\n",
            "0.7505285739898682\n",
            " isotonic cone loss = 0.0141\n",
            "0.8883442282676697\n",
            " isotonic cone loss = 0.0145\n",
            "0.8653991222381592\n",
            " isotonic cone loss = 0.0140\n",
            "0.9895413517951965\n",
            " isotonic cone loss = 0.0142\n",
            "0.8074213266372681\n",
            " isotonic cone loss = 0.0140\n",
            "0.8856532573699951\n",
            " isotonic cone loss = 0.0143\n",
            "0.7816545963287354\n",
            " isotonic cone loss = 0.0142\n",
            "0.8201318979263306\n",
            " isotonic cone loss = 0.0140\n",
            "0.8446052670478821\n",
            " isotonic cone loss = 0.0142\n",
            "0.8938769102096558\n",
            " isotonic cone loss = 0.0143\n",
            "0.885301411151886\n",
            " isotonic cone loss = 0.0142\n",
            "0.9018030762672424\n",
            " isotonic cone loss = 0.0139\n",
            "0.9015658497810364\n",
            " isotonic cone loss = 0.0140\n",
            "0.8165562152862549\n",
            " isotonic cone loss = 0.0139\n",
            "0.789462149143219\n",
            " isotonic cone loss = 0.0144\n",
            "0.8160499930381775\n",
            " isotonic cone loss = 0.0139\n",
            "0.8907467126846313\n",
            " isotonic cone loss = 0.0139\n",
            "0.7752189040184021\n",
            " isotonic cone loss = 0.0140\n",
            "0.7753245234489441\n",
            " isotonic cone loss = 0.0137\n",
            "0.8201804161071777\n",
            " isotonic cone loss = 0.0140\n",
            "0.9569995403289795\n",
            " isotonic cone loss = 0.0140\n",
            "0.8851919174194336\n",
            " isotonic cone loss = 0.0139\n",
            "0.8787083029747009\n",
            " isotonic cone loss = 0.0143\n",
            "0.8557758331298828\n",
            " isotonic cone loss = 0.0141\n",
            "0.9319173097610474\n",
            " isotonic cone loss = 0.0143\n",
            "0.7899038195610046\n",
            " isotonic cone loss = 0.0142\n",
            "0.9333180785179138\n",
            " isotonic cone loss = 0.0144\n",
            "0.8212905526161194\n",
            " isotonic cone loss = 0.0141\n",
            "0.8373094797134399\n",
            " isotonic cone loss = 0.0143\n",
            "0.8890650868415833\n",
            " isotonic cone loss = 0.0139\n",
            "0.8050471544265747\n",
            " isotonic cone loss = 0.0141\n",
            "0.9221838116645813\n",
            " isotonic cone loss = 0.0140\n",
            "0.9752572178840637\n",
            " isotonic cone loss = 0.0140\n",
            "0.891482949256897\n",
            " isotonic cone loss = 0.0140\n",
            "0.8814842104911804\n",
            " isotonic cone loss = 0.0140\n",
            "0.8833586573600769\n",
            " isotonic cone loss = 0.0139\n",
            "0.8708803057670593\n",
            " isotonic cone loss = 0.0135\n",
            "0.9279696941375732\n",
            " isotonic cone loss = 0.0142\n",
            "0.9380549192428589\n",
            " isotonic cone loss = 0.0140\n",
            "0.8279168605804443\n",
            " isotonic cone loss = 0.0139\n",
            "0.8170523643493652\n",
            " isotonic cone loss = 0.0136\n",
            "0.784424364566803\n",
            " isotonic cone loss = 0.0138\n",
            "0.8472355008125305\n",
            " isotonic cone loss = 0.0136\n",
            "0.8958430886268616\n",
            " isotonic cone loss = 0.0138\n",
            "0.9457146525382996\n",
            " isotonic cone loss = 0.0138\n",
            "0.8535569310188293\n",
            " isotonic cone loss = 0.0143\n",
            "0.9593858122825623\n",
            " isotonic cone loss = 0.0140\n",
            "0.8823606967926025\n",
            " isotonic cone loss = 0.0140\n",
            "0.9553425312042236\n",
            " isotonic cone loss = 0.0140\n",
            "0.8882480263710022\n",
            " isotonic cone loss = 0.0139\n",
            "0.8856160640716553\n",
            " isotonic cone loss = 0.0140\n",
            "0.898589551448822\n",
            " isotonic cone loss = 0.0139\n",
            "0.8805315494537354\n",
            " isotonic cone loss = 0.0138\n",
            "0.8434275984764099\n",
            " isotonic cone loss = 0.0141\n",
            "0.9017277956008911\n",
            " isotonic cone loss = 0.0140\n",
            "0.8950722217559814\n",
            " isotonic cone loss = 0.0141\n",
            "0.8724638223648071\n",
            " isotonic cone loss = 0.0139\n",
            "0.8541632890701294\n",
            " isotonic cone loss = 0.0142\n",
            "0.9582706689834595\n",
            " isotonic cone loss = 0.0137\n",
            "0.9804452061653137\n",
            " isotonic cone loss = 0.0138\n",
            "0.8643862009048462\n",
            " isotonic cone loss = 0.0139\n",
            "0.8648172616958618\n",
            " isotonic cone loss = 0.0139\n",
            "0.9124951362609863\n",
            " isotonic cone loss = 0.0139\n",
            "0.7881344556808472\n",
            " isotonic cone loss = 0.0138\n",
            "0.7774385809898376\n",
            " isotonic cone loss = 0.0138\n",
            "0.8355050683021545\n",
            " isotonic cone loss = 0.0135\n",
            "0.9187909364700317\n",
            " isotonic cone loss = 0.0139\n",
            "0.8541518449783325\n",
            " isotonic cone loss = 0.0138\n",
            "0.905940055847168\n",
            " isotonic cone loss = 0.0136\n",
            "0.9555261135101318\n",
            " isotonic cone loss = 0.0137\n",
            "0.8737058639526367\n",
            " isotonic cone loss = 0.0141\n",
            "0.8926828503608704\n",
            " isotonic cone loss = 0.0138\n",
            "0.8639426231384277\n",
            " isotonic cone loss = 0.0140\n",
            "0.888683021068573\n",
            " isotonic cone loss = 0.0142\n",
            "0.8680486679077148\n",
            " isotonic cone loss = 0.0139\n",
            "0.9407510757446289\n",
            " isotonic cone loss = 0.0142\n",
            "0.7628888487815857\n",
            " isotonic cone loss = 0.0139\n",
            "0.8289682269096375\n",
            " isotonic cone loss = 0.0139\n",
            "0.81465083360672\n",
            " isotonic cone loss = 0.0144\n",
            "0.8311804533004761\n",
            " isotonic cone loss = 0.0139\n",
            "0.936956524848938\n",
            " isotonic cone loss = 0.0141\n",
            "0.8682199716567993\n",
            " isotonic cone loss = 0.0139\n",
            "0.7190343141555786\n",
            " isotonic cone loss = 0.0139\n",
            "0.8352298736572266\n",
            " isotonic cone loss = 0.0141\n",
            "0.849299430847168\n",
            " isotonic cone loss = 0.0138\n",
            "0.8177165985107422\n",
            " isotonic cone loss = 0.0141\n",
            "0.7692388892173767\n",
            " isotonic cone loss = 0.0139\n",
            "0.7131220102310181\n",
            " isotonic cone loss = 0.0140\n",
            "0.9336373209953308\n",
            " isotonic cone loss = 0.0142\n",
            "0.8899736404418945\n",
            " isotonic cone loss = 0.0140\n",
            "0.7872821092605591\n",
            " isotonic cone loss = 0.0141\n",
            "0.8903493881225586\n",
            " isotonic cone loss = 0.0140\n",
            "0.8727922439575195\n",
            " isotonic cone loss = 0.0141\n",
            "0.8337405920028687\n",
            " isotonic cone loss = 0.0141\n",
            "0.8548111915588379\n",
            " isotonic cone loss = 0.0140\n",
            "0.9550303220748901\n",
            " isotonic cone loss = 0.0141\n",
            "0.853630006313324\n",
            " isotonic cone loss = 0.0139\n",
            "0.80239337682724\n",
            " isotonic cone loss = 0.0137\n",
            "0.9031631946563721\n",
            " isotonic cone loss = 0.0140\n",
            "0.7893730401992798\n",
            " isotonic cone loss = 0.0142\n",
            "0.900654673576355\n",
            " isotonic cone loss = 0.0140\n",
            "0.747866153717041\n",
            " isotonic cone loss = 0.0139\n",
            "0.8331504464149475\n",
            " isotonic cone loss = 0.0138\n",
            "0.8095923662185669\n",
            " isotonic cone loss = 0.0141\n",
            "0.8100714087486267\n",
            " isotonic cone loss = 0.0139\n",
            "0.7855095863342285\n",
            " isotonic cone loss = 0.0139\n",
            "0.9185678958892822\n",
            " isotonic cone loss = 0.0138\n",
            "0.8419240117073059\n",
            " isotonic cone loss = 0.0138\n",
            "0.6966551542282104\n",
            " isotonic cone loss = 0.0139\n",
            "0.8218113780021667\n",
            " isotonic cone loss = 0.0138\n",
            "0.7643666863441467\n",
            " isotonic cone loss = 0.0140\n",
            "0.9074453711509705\n",
            " isotonic cone loss = 0.0139\n",
            "0.9448795914649963\n",
            " isotonic cone loss = 0.0139\n",
            "0.8892762660980225\n",
            " isotonic cone loss = 0.0141\n",
            "0.7954723834991455\n",
            " isotonic cone loss = 0.0140\n",
            "0.7968427538871765\n",
            " isotonic cone loss = 0.0141\n",
            "0.9221833944320679\n",
            " isotonic cone loss = 0.0141\n",
            "0.8697819709777832\n",
            " isotonic cone loss = 0.0139\n",
            "0.7869297862052917\n",
            " isotonic cone loss = 0.0143\n",
            "0.8474083542823792\n",
            " isotonic cone loss = 0.0142\n",
            "0.8723048567771912\n",
            " isotonic cone loss = 0.0143\n",
            "0.8703667521476746\n",
            " isotonic cone loss = 0.0140\n",
            "0.7697338461875916\n",
            " isotonic cone loss = 0.0143\n",
            "0.8143724799156189\n",
            " isotonic cone loss = 0.0141\n",
            "0.8302478194236755\n",
            " isotonic cone loss = 0.0139\n",
            "0.8394352197647095\n",
            " isotonic cone loss = 0.0141\n",
            "0.8173441886901855\n",
            " isotonic cone loss = 0.0141\n",
            "0.854647696018219\n",
            " isotonic cone loss = 0.0141\n",
            "0.8293837904930115\n",
            " isotonic cone loss = 0.0140\n",
            "0.8234743475914001\n",
            " isotonic cone loss = 0.0141\n",
            "0.8646800518035889\n",
            " isotonic cone loss = 0.0140\n",
            "0.8363835215568542\n",
            " isotonic cone loss = 0.0140\n",
            "0.9077847599983215\n",
            " isotonic cone loss = 0.0140\n",
            "0.9782532453536987\n",
            " isotonic cone loss = 0.0139\n",
            "0.7915542125701904\n",
            " isotonic cone loss = 0.0141\n",
            "0.8293151259422302\n",
            " isotonic cone loss = 0.0143\n",
            "0.8587191700935364\n",
            " isotonic cone loss = 0.0139\n",
            "0.8079046607017517\n",
            " isotonic cone loss = 0.0139\n",
            "0.9504877924919128\n",
            " isotonic cone loss = 0.0141\n",
            "0.8348544239997864\n",
            " isotonic cone loss = 0.0136\n",
            "0.884306788444519\n",
            " isotonic cone loss = 0.0139\n",
            "0.9556168913841248\n",
            " isotonic cone loss = 0.0141\n",
            "0.7959076762199402\n",
            " isotonic cone loss = 0.0140\n",
            "0.8193150758743286\n",
            " isotonic cone loss = 0.0139\n",
            "0.7493682503700256\n",
            " isotonic cone loss = 0.0141\n",
            "0.8051472902297974\n",
            " isotonic cone loss = 0.0139\n",
            "0.8002858757972717\n",
            " isotonic cone loss = 0.0139\n",
            "0.8299015164375305\n",
            " isotonic cone loss = 0.0140\n",
            "0.7853542566299438\n",
            " isotonic cone loss = 0.0139\n",
            "0.8263158798217773\n",
            " isotonic cone loss = 0.0137\n",
            "0.7425661087036133\n",
            " isotonic cone loss = 0.0138\n",
            "0.7770327925682068\n",
            " isotonic cone loss = 0.0138\n",
            "0.8178700804710388\n",
            " isotonic cone loss = 0.0139\n",
            "0.7390846610069275\n",
            " isotonic cone loss = 0.0142\n",
            "0.9467036128044128\n",
            " isotonic cone loss = 0.0143\n",
            "0.803978681564331\n",
            " isotonic cone loss = 0.0141\n",
            "0.7861445546150208\n",
            " isotonic cone loss = 0.0140\n",
            "0.7499276399612427\n",
            " isotonic cone loss = 0.0139\n",
            "0.8631405234336853\n",
            " isotonic cone loss = 0.0141\n",
            "0.8147658109664917\n",
            " isotonic cone loss = 0.0138\n",
            "0.7345269322395325\n",
            " isotonic cone loss = 0.0142\n",
            "0.8442023992538452\n",
            " isotonic cone loss = 0.0141\n",
            "0.9695280194282532\n",
            " isotonic cone loss = 0.0137\n",
            "0.7796353697776794\n",
            " isotonic cone loss = 0.0137\n",
            "0.9489651918411255\n",
            " isotonic cone loss = 0.0141\n",
            "0.7046154141426086\n",
            " isotonic cone loss = 0.0140\n",
            "0.8674009442329407\n",
            " isotonic cone loss = 0.0139\n",
            "0.7428568005561829\n",
            " isotonic cone loss = 0.0139\n",
            "0.7562099695205688\n",
            " isotonic cone loss = 0.0139\n",
            "0.7834656834602356\n",
            " isotonic cone loss = 0.0140\n",
            "0.8640965819358826\n",
            " isotonic cone loss = 0.0139\n",
            "0.8087777495384216\n",
            " isotonic cone loss = 0.0141\n",
            "0.8658788800239563\n",
            " isotonic cone loss = 0.0143\n",
            "0.8959128856658936\n",
            " isotonic cone loss = 0.0142\n",
            "0.810271680355072\n",
            " isotonic cone loss = 0.0141\n",
            "0.7372052073478699\n",
            " isotonic cone loss = 0.0138\n",
            "0.8101633787155151\n",
            " isotonic cone loss = 0.0138\n",
            "0.8036139011383057\n",
            " isotonic cone loss = 0.0139\n",
            "0.7852362394332886\n",
            " isotonic cone loss = 0.0139\n",
            "0.7690116167068481\n",
            " isotonic cone loss = 0.0140\n",
            "0.6675328612327576\n",
            " isotonic cone loss = 0.0138\n",
            "0.7724603414535522\n",
            " isotonic cone loss = 0.0139\n",
            "0.8044754862785339\n",
            " isotonic cone loss = 0.0141\n",
            "0.8393528461456299\n",
            " isotonic cone loss = 0.0139\n",
            "1.007723331451416\n",
            " isotonic cone loss = 0.0141\n",
            "0.8229712247848511\n",
            " isotonic cone loss = 0.0141\n",
            "0.8002912998199463\n",
            " isotonic cone loss = 0.0142\n",
            "0.8177233338356018\n",
            " isotonic cone loss = 0.0144\n",
            "0.7829089164733887\n",
            " isotonic cone loss = 0.0141\n",
            "0.8693138957023621\n",
            " isotonic cone loss = 0.0143\n",
            "0.9273813366889954\n",
            " isotonic cone loss = 0.0140\n",
            "0.8821020126342773\n",
            " isotonic cone loss = 0.0142\n",
            "0.8784822821617126\n",
            " isotonic cone loss = 0.0139\n",
            "0.767682671546936\n",
            " isotonic cone loss = 0.0141\n",
            "0.8428690433502197\n",
            " isotonic cone loss = 0.0141\n",
            "0.8416263461112976\n",
            " isotonic cone loss = 0.0141\n",
            "0.923297107219696\n",
            " isotonic cone loss = 0.0137\n",
            "0.7999398112297058\n",
            " isotonic cone loss = 0.0139\n",
            "0.800194263458252\n",
            " isotonic cone loss = 0.0139\n",
            "0.789970338344574\n",
            " isotonic cone loss = 0.0140\n",
            "0.8290318250656128\n",
            " isotonic cone loss = 0.0141\n",
            "0.7543691396713257\n",
            " isotonic cone loss = 0.0138\n",
            "0.8816271424293518\n",
            " isotonic cone loss = 0.0139\n",
            "0.7238953113555908\n",
            " isotonic cone loss = 0.0136\n",
            "0.8313630223274231\n",
            " isotonic cone loss = 0.0135\n",
            "0.8839527368545532\n",
            " isotonic cone loss = 0.0137\n",
            "0.7601842880249023\n",
            " isotonic cone loss = 0.0140\n",
            "0.7817693948745728\n",
            " isotonic cone loss = 0.0143\n",
            "0.8081719279289246\n",
            " isotonic cone loss = 0.0143\n",
            "0.7976342439651489\n",
            " isotonic cone loss = 0.0142\n",
            "0.7458096146583557\n",
            " isotonic cone loss = 0.0142\n",
            "0.7712604999542236\n",
            " isotonic cone loss = 0.0144\n",
            "0.7435188293457031\n",
            " isotonic cone loss = 0.0140\n",
            "0.8705418705940247\n",
            " isotonic cone loss = 0.0139\n",
            "0.8891066312789917\n",
            " isotonic cone loss = 0.0141\n",
            "0.9325953722000122\n",
            " isotonic cone loss = 0.0138\n",
            "0.8600772619247437\n",
            " isotonic cone loss = 0.0138\n",
            "0.8777730464935303\n",
            " isotonic cone loss = 0.0140\n",
            "0.7542932033538818\n",
            " isotonic cone loss = 0.0140\n",
            "0.8080798387527466\n",
            " isotonic cone loss = 0.0140\n",
            "0.9294122457504272\n",
            " isotonic cone loss = 0.0140\n",
            "0.939624011516571\n",
            " isotonic cone loss = 0.0142\n",
            "0.8360623121261597\n",
            " isotonic cone loss = 0.0141\n",
            "0.8163725137710571\n",
            " isotonic cone loss = 0.0141\n",
            "0.8742427229881287\n",
            " isotonic cone loss = 0.0140\n",
            "0.8656516075134277\n",
            " isotonic cone loss = 0.0138\n",
            "0.8744710087776184\n",
            " isotonic cone loss = 0.0139\n",
            "0.905460774898529\n",
            " isotonic cone loss = 0.0134\n",
            "0.7510076761245728\n",
            " isotonic cone loss = 0.0139\n",
            "0.82138991355896\n",
            " isotonic cone loss = 0.0140\n",
            "0.7425593137741089\n",
            " isotonic cone loss = 0.0140\n",
            "0.9315861463546753\n",
            " isotonic cone loss = 0.0137\n",
            "0.8042380809783936\n",
            " isotonic cone loss = 0.0140\n",
            "0.8688396215438843\n",
            " isotonic cone loss = 0.0137\n",
            "0.7367309927940369\n",
            " isotonic cone loss = 0.0139\n",
            "0.6810158491134644\n",
            " isotonic cone loss = 0.0138\n",
            "0.7251303791999817\n",
            " isotonic cone loss = 0.0139\n",
            "0.8534303903579712\n",
            " isotonic cone loss = 0.0141\n",
            "0.8210412859916687\n",
            " isotonic cone loss = 0.0137\n",
            "0.7921693921089172\n",
            " isotonic cone loss = 0.0138\n",
            "0.7436270713806152\n",
            " isotonic cone loss = 0.0137\n",
            "0.8687747120857239\n",
            " isotonic cone loss = 0.0140\n",
            "0.7513872981071472\n",
            " isotonic cone loss = 0.0139\n",
            "0.773740291595459\n",
            " isotonic cone loss = 0.0139\n",
            "0.9042673110961914\n",
            " isotonic cone loss = 0.0140\n",
            "0.8455594778060913\n",
            " isotonic cone loss = 0.0142\n",
            "0.8233757615089417\n",
            " isotonic cone loss = 0.0139\n",
            "0.773730993270874\n",
            " isotonic cone loss = 0.0142\n",
            "0.8499082922935486\n",
            " isotonic cone loss = 0.0140\n",
            "0.779188871383667\n",
            " isotonic cone loss = 0.0140\n",
            "0.8834707736968994\n",
            " isotonic cone loss = 0.0142\n",
            "0.8887192606925964\n",
            " isotonic cone loss = 0.0139\n",
            "0.7054154276847839\n",
            " isotonic cone loss = 0.0139\n",
            "0.8354029059410095\n",
            " isotonic cone loss = 0.0141\n",
            "0.8407014012336731\n",
            " isotonic cone loss = 0.0138\n",
            "0.7826157212257385\n",
            " isotonic cone loss = 0.0139\n",
            "0.8750126957893372\n",
            " isotonic cone loss = 0.0138\n",
            "0.8470422029495239\n",
            " isotonic cone loss = 0.0139\n",
            "0.8163675665855408\n",
            " isotonic cone loss = 0.0139\n",
            "0.7923782467842102\n",
            " isotonic cone loss = 0.0139\n",
            "0.8339195251464844\n",
            " isotonic cone loss = 0.0139\n",
            "0.731812059879303\n",
            " isotonic cone loss = 0.0142\n",
            "0.8691322207450867\n",
            " isotonic cone loss = 0.0138\n",
            "0.8086239695549011\n",
            " isotonic cone loss = 0.0139\n",
            "0.8026536703109741\n",
            " isotonic cone loss = 0.0140\n",
            "0.7857173085212708\n",
            " isotonic cone loss = 0.0142\n",
            "0.7368180155754089\n",
            " isotonic cone loss = 0.0139\n",
            "0.8497909307479858\n",
            " isotonic cone loss = 0.0139\n",
            "0.8508280515670776\n",
            " isotonic cone loss = 0.0137\n",
            "0.759850025177002\n",
            " isotonic cone loss = 0.0137\n",
            "0.7250775098800659\n",
            " isotonic cone loss = 0.0140\n",
            "0.7371383309364319\n",
            " isotonic cone loss = 0.0139\n",
            "0.850824236869812\n",
            " isotonic cone loss = 0.0142\n",
            "0.8642276525497437\n",
            " isotonic cone loss = 0.0139\n",
            "0.727548360824585\n",
            " isotonic cone loss = 0.0141\n",
            "0.8046655058860779\n",
            " isotonic cone loss = 0.0138\n",
            "0.8422797322273254\n",
            " isotonic cone loss = 0.0137\n",
            "0.7970882654190063\n",
            " isotonic cone loss = 0.0141\n",
            "0.7360443472862244\n",
            " isotonic cone loss = 0.0139\n",
            "0.770167350769043\n",
            " isotonic cone loss = 0.0138\n",
            "0.8322924375534058\n",
            " isotonic cone loss = 0.0143\n",
            "0.770067036151886\n",
            " isotonic cone loss = 0.0140\n",
            "0.826854944229126\n",
            " isotonic cone loss = 0.0144\n",
            "0.7708742022514343\n",
            " isotonic cone loss = 0.0141\n",
            "0.7956283092498779\n",
            " isotonic cone loss = 0.0138\n",
            "0.807640016078949\n",
            " isotonic cone loss = 0.0139\n",
            "0.8423169255256653\n",
            " isotonic cone loss = 0.0141\n",
            "0.7888508439064026\n",
            " isotonic cone loss = 0.0140\n",
            "0.831658124923706\n",
            " isotonic cone loss = 0.0141\n",
            "0.8695843815803528\n",
            " isotonic cone loss = 0.0140\n",
            "0.8520479798316956\n",
            " isotonic cone loss = 0.0138\n",
            "0.7980709671974182\n",
            " isotonic cone loss = 0.0139\n",
            "0.7447028160095215\n",
            " isotonic cone loss = 0.0140\n",
            "0.8246201276779175\n",
            " isotonic cone loss = 0.0139\n",
            "0.7514006495475769\n",
            " isotonic cone loss = 0.0141\n",
            "0.8840000629425049\n",
            " isotonic cone loss = 0.0142\n",
            "0.8875635862350464\n",
            " isotonic cone loss = 0.0139\n",
            "0.7405990362167358\n",
            " isotonic cone loss = 0.0137\n",
            "0.8213162422180176\n",
            " isotonic cone loss = 0.0135\n",
            "0.8764452934265137\n",
            " isotonic cone loss = 0.0140\n",
            "0.8418959975242615\n",
            " isotonic cone loss = 0.0140\n",
            "0.8024296164512634\n",
            " isotonic cone loss = 0.0142\n",
            "0.7250367403030396\n",
            " isotonic cone loss = 0.0139\n",
            "0.9219594597816467\n",
            " isotonic cone loss = 0.0140\n",
            "0.9000722765922546\n",
            " isotonic cone loss = 0.0139\n",
            "0.7437556982040405\n",
            " isotonic cone loss = 0.0139\n",
            "0.7236331105232239\n",
            " isotonic cone loss = 0.0138\n",
            "0.7934248447418213\n",
            " isotonic cone loss = 0.0138\n",
            "0.8675360679626465\n",
            " isotonic cone loss = 0.0137\n",
            "0.8138303756713867\n",
            " isotonic cone loss = 0.0137\n",
            "0.8297930359840393\n",
            " isotonic cone loss = 0.0136\n",
            "0.7896056771278381\n",
            " isotonic cone loss = 0.0138\n",
            "0.6751190423965454\n",
            " isotonic cone loss = 0.0137\n",
            "0.8424248099327087\n",
            " isotonic cone loss = 0.0141\n",
            "0.7237135767936707\n",
            " isotonic cone loss = 0.0141\n",
            "0.745639979839325\n",
            " isotonic cone loss = 0.0136\n",
            "0.7342947721481323\n",
            " isotonic cone loss = 0.0136\n",
            "0.7906660437583923\n",
            " isotonic cone loss = 0.0139\n",
            "0.734335720539093\n",
            " isotonic cone loss = 0.0139\n",
            "0.9449720978736877\n",
            " isotonic cone loss = 0.0139\n",
            "0.8656773567199707\n",
            " isotonic cone loss = 0.0139\n",
            "0.7581325173377991\n",
            " isotonic cone loss = 0.0139\n",
            "0.7908815145492554\n",
            " isotonic cone loss = 0.0139\n",
            "0.7912742495536804\n",
            " isotonic cone loss = 0.0138\n",
            "0.7191144824028015\n",
            " isotonic cone loss = 0.0138\n",
            "0.6654757261276245\n",
            " isotonic cone loss = 0.0141\n",
            "0.7578616142272949\n",
            " isotonic cone loss = 0.0139\n",
            "0.7806118726730347\n",
            " isotonic cone loss = 0.0143\n",
            "0.7119742035865784\n",
            " isotonic cone loss = 0.0140\n",
            "0.7840343117713928\n",
            " isotonic cone loss = 0.0138\n",
            "0.7376656532287598\n",
            " isotonic cone loss = 0.0141\n",
            "0.8190548419952393\n",
            " isotonic cone loss = 0.0137\n",
            "0.7605886459350586\n",
            " isotonic cone loss = 0.0143\n",
            "0.7298591732978821\n",
            " isotonic cone loss = 0.0141\n",
            "0.8262548446655273\n",
            " isotonic cone loss = 0.0142\n",
            "0.7045536041259766\n",
            " isotonic cone loss = 0.0142\n",
            "0.7883221507072449\n",
            " isotonic cone loss = 0.0142\n",
            "0.6766156554222107\n",
            " isotonic cone loss = 0.0141\n",
            "0.8120410442352295\n",
            " isotonic cone loss = 0.0136\n",
            "0.6779361367225647\n",
            " isotonic cone loss = 0.0138\n",
            "0.9192144274711609\n",
            " isotonic cone loss = 0.0136\n",
            "0.7303463220596313\n",
            " isotonic cone loss = 0.0142\n",
            "0.5604969263076782\n",
            " isotonic cone loss = 0.0140\n",
            "0.7860844135284424\n",
            " isotonic cone loss = 0.0142\n",
            "0.6540005207061768\n",
            " isotonic cone loss = 0.0138\n",
            "0.8880522847175598\n",
            " isotonic cone loss = 0.0141\n",
            "0.740612268447876\n",
            " isotonic cone loss = 0.0141\n",
            "0.6732458472251892\n",
            " isotonic cone loss = 0.0136\n",
            "0.827560544013977\n",
            " isotonic cone loss = 0.0141\n",
            "0.6897249221801758\n",
            " isotonic cone loss = 0.0138\n",
            "0.8453727960586548\n",
            " isotonic cone loss = 0.0138\n",
            "0.749197244644165\n",
            " isotonic cone loss = 0.0139\n",
            "0.767297625541687\n",
            " isotonic cone loss = 0.0138\n",
            "0.7064510583877563\n",
            " isotonic cone loss = 0.0140\n",
            "0.8053568005561829\n",
            " isotonic cone loss = 0.0138\n",
            "0.8444564342498779\n",
            " isotonic cone loss = 0.0138\n",
            "0.8116808533668518\n",
            " isotonic cone loss = 0.0137\n",
            "0.6718093752861023\n",
            " isotonic cone loss = 0.0138\n",
            "0.7029281854629517\n",
            " isotonic cone loss = 0.0135\n",
            "0.8479349613189697\n",
            " isotonic cone loss = 0.0141\n",
            "0.780524492263794\n",
            " isotonic cone loss = 0.0137\n",
            "0.6969302296638489\n",
            " isotonic cone loss = 0.0138\n",
            "0.8501372933387756\n",
            " isotonic cone loss = 0.0136\n",
            "0.7838915586471558\n",
            " isotonic cone loss = 0.0137\n",
            "0.8325175046920776\n",
            " isotonic cone loss = 0.0138\n",
            "0.8306331038475037\n",
            " isotonic cone loss = 0.0139\n",
            "0.8083069324493408\n",
            " isotonic cone loss = 0.0139\n",
            "0.7227782607078552\n",
            " isotonic cone loss = 0.0142\n",
            "0.7456527948379517\n",
            " isotonic cone loss = 0.0139\n",
            "0.8260759711265564\n",
            " isotonic cone loss = 0.0141\n",
            "0.7320218682289124\n",
            " isotonic cone loss = 0.0141\n",
            "0.831770658493042\n",
            " isotonic cone loss = 0.0141\n",
            "0.6586441993713379\n",
            " isotonic cone loss = 0.0140\n",
            "0.8294432163238525\n",
            " isotonic cone loss = 0.0140\n",
            "0.8093159198760986\n",
            " isotonic cone loss = 0.0140\n",
            "0.8647476434707642\n",
            " isotonic cone loss = 0.0138\n",
            "0.7692743539810181\n",
            " isotonic cone loss = 0.0141\n",
            "0.8114452958106995\n",
            " isotonic cone loss = 0.0139\n",
            "0.7555053234100342\n",
            " isotonic cone loss = 0.0139\n",
            "0.751068651676178\n",
            " isotonic cone loss = 0.0139\n",
            "0.7378972172737122\n",
            " isotonic cone loss = 0.0142\n",
            "0.8385803699493408\n",
            " isotonic cone loss = 0.0139\n",
            "0.7236385345458984\n",
            " isotonic cone loss = 0.0137\n",
            "0.8385794162750244\n",
            " isotonic cone loss = 0.0139\n",
            "0.7773871421813965\n",
            " isotonic cone loss = 0.0136\n",
            "0.8946084380149841\n",
            " isotonic cone loss = 0.0139\n",
            "0.8198709487915039\n",
            " isotonic cone loss = 0.0139\n",
            "0.8896051645278931\n",
            " isotonic cone loss = 0.0139\n",
            "0.6576101779937744\n",
            " isotonic cone loss = 0.0139\n",
            "0.6830227971076965\n",
            " isotonic cone loss = 0.0140\n",
            "0.8431152701377869\n",
            " isotonic cone loss = 0.0140\n",
            "0.7986509203910828\n",
            " isotonic cone loss = 0.0138\n",
            "0.7787410616874695\n",
            " isotonic cone loss = 0.0137\n",
            "0.7505548596382141\n",
            " isotonic cone loss = 0.0137\n",
            "0.744778573513031\n",
            " isotonic cone loss = 0.0140\n",
            "0.712501049041748\n",
            " isotonic cone loss = 0.0142\n",
            "0.6831933856010437\n",
            " isotonic cone loss = 0.0140\n",
            "0.7560266852378845\n",
            " isotonic cone loss = 0.0139\n",
            "0.8013715147972107\n",
            " isotonic cone loss = 0.0137\n",
            "0.7900746464729309\n",
            " isotonic cone loss = 0.0143\n",
            "0.8223787546157837\n",
            " isotonic cone loss = 0.0139\n",
            "0.7719299793243408\n",
            " isotonic cone loss = 0.0140\n",
            "0.8647200465202332\n",
            " isotonic cone loss = 0.0138\n",
            "0.740626335144043\n",
            " isotonic cone loss = 0.0139\n",
            "0.8032403588294983\n",
            " isotonic cone loss = 0.0140\n",
            "0.6951602101325989\n",
            " isotonic cone loss = 0.0139\n",
            "0.7243734002113342\n",
            " isotonic cone loss = 0.0137\n",
            "0.8119286894798279\n",
            " isotonic cone loss = 0.0141\n",
            "0.7532970905303955\n",
            " isotonic cone loss = 0.0138\n",
            "0.750117301940918\n",
            " isotonic cone loss = 0.0141\n",
            "0.7255828380584717\n",
            " isotonic cone loss = 0.0140\n",
            "0.7774797081947327\n",
            " isotonic cone loss = 0.0143\n",
            "0.8009120225906372\n",
            " isotonic cone loss = 0.0140\n",
            "0.6695007681846619\n",
            " isotonic cone loss = 0.0140\n",
            "0.8321059346199036\n",
            " isotonic cone loss = 0.0138\n",
            "0.7329869866371155\n",
            " isotonic cone loss = 0.0141\n",
            "0.8115517497062683\n",
            " isotonic cone loss = 0.0141\n",
            "0.6751735210418701\n",
            " isotonic cone loss = 0.0138\n",
            "0.7509212493896484\n",
            " isotonic cone loss = 0.0142\n",
            "0.7761269211769104\n",
            " isotonic cone loss = 0.0141\n",
            "0.8040246963500977\n",
            " isotonic cone loss = 0.0139\n",
            "0.7857702970504761\n",
            " isotonic cone loss = 0.0140\n",
            "0.8173548579216003\n",
            " isotonic cone loss = 0.0141\n",
            "0.7062656283378601\n",
            " isotonic cone loss = 0.0139\n",
            "0.8057879209518433\n",
            " isotonic cone loss = 0.0138\n",
            "0.7142875790596008\n",
            " isotonic cone loss = 0.0137\n",
            "0.7123553156852722\n",
            " isotonic cone loss = 0.0140\n",
            "0.842095673084259\n",
            " isotonic cone loss = 0.0139\n",
            "0.7864338159561157\n",
            " isotonic cone loss = 0.0139\n",
            "0.8076611161231995\n",
            " isotonic cone loss = 0.0139\n",
            "0.8601193428039551\n",
            " isotonic cone loss = 0.0140\n",
            "0.7022365927696228\n",
            " isotonic cone loss = 0.0137\n",
            "0.806108295917511\n",
            " isotonic cone loss = 0.0137\n",
            "0.8250435590744019\n",
            " isotonic cone loss = 0.0141\n",
            "0.8132364749908447\n",
            " isotonic cone loss = 0.0137\n",
            "0.7367892265319824\n",
            " isotonic cone loss = 0.0140\n",
            "0.6448774337768555\n",
            " isotonic cone loss = 0.0141\n",
            "0.8182470798492432\n",
            " isotonic cone loss = 0.0139\n",
            "0.8486231565475464\n",
            " isotonic cone loss = 0.0139\n",
            "0.6856900453567505\n",
            " isotonic cone loss = 0.0138\n",
            "0.7294050455093384\n",
            " isotonic cone loss = 0.0137\n",
            "0.757238507270813\n",
            " isotonic cone loss = 0.0137\n",
            "0.883203387260437\n",
            " isotonic cone loss = 0.0138\n",
            "0.6868993043899536\n",
            " isotonic cone loss = 0.0139\n",
            "0.7514743804931641\n",
            " isotonic cone loss = 0.0138\n",
            "0.8855293989181519\n",
            " isotonic cone loss = 0.0138\n",
            "0.7854297757148743\n",
            " isotonic cone loss = 0.0139\n",
            "0.7813901901245117\n",
            " isotonic cone loss = 0.0138\n",
            "0.7562062740325928\n",
            " isotonic cone loss = 0.0137\n",
            "0.7366666197776794\n",
            " isotonic cone loss = 0.0137\n",
            "0.7740835547447205\n",
            " isotonic cone loss = 0.0138\n",
            "0.7204180359840393\n",
            " isotonic cone loss = 0.0135\n",
            "0.7679272890090942\n",
            " isotonic cone loss = 0.0139\n",
            "0.6854537129402161\n",
            " isotonic cone loss = 0.0136\n",
            "0.7447845935821533\n",
            " isotonic cone loss = 0.0139\n",
            "0.7991552948951721\n",
            " isotonic cone loss = 0.0139\n",
            "0.7733129262924194\n",
            " isotonic cone loss = 0.0139\n",
            "0.6025569438934326\n",
            " isotonic cone loss = 0.0141\n",
            "0.689964234828949\n",
            " isotonic cone loss = 0.0141\n",
            "0.8004426956176758\n",
            " isotonic cone loss = 0.0139\n",
            "0.8572112917900085\n",
            " isotonic cone loss = 0.0137\n",
            "0.785259485244751\n",
            " isotonic cone loss = 0.0137\n",
            "0.7247297167778015\n",
            " isotonic cone loss = 0.0140\n",
            "0.8230899572372437\n",
            " isotonic cone loss = 0.0141\n",
            "0.7601315975189209\n",
            " isotonic cone loss = 0.0138\n",
            "0.731255054473877\n",
            " isotonic cone loss = 0.0140\n",
            "0.6824247241020203\n",
            " isotonic cone loss = 0.0137\n",
            "0.8645204305648804\n",
            " isotonic cone loss = 0.0139\n",
            "0.7379899621009827\n",
            " isotonic cone loss = 0.0139\n",
            "0.7511635422706604\n",
            " isotonic cone loss = 0.0140\n",
            "0.6233384013175964\n",
            " isotonic cone loss = 0.0141\n",
            "0.5826042890548706\n",
            " isotonic cone loss = 0.0141\n",
            "0.7951238751411438\n",
            " isotonic cone loss = 0.0142\n",
            "0.7395822405815125\n",
            " isotonic cone loss = 0.0141\n",
            "0.7640045881271362\n",
            " isotonic cone loss = 0.0141\n",
            "0.8464853763580322\n",
            " isotonic cone loss = 0.0138\n",
            "0.8860759735107422\n",
            " isotonic cone loss = 0.0140\n",
            "0.7004967331886292\n",
            " isotonic cone loss = 0.0142\n",
            "0.6785082221031189\n",
            " isotonic cone loss = 0.0137\n",
            "0.7737699151039124\n",
            " isotonic cone loss = 0.0139\n",
            "0.6802241206169128\n",
            " isotonic cone loss = 0.0137\n",
            "0.8002514839172363\n",
            " isotonic cone loss = 0.0138\n",
            "0.8271570205688477\n",
            " isotonic cone loss = 0.0137\n",
            "0.7885361909866333\n",
            " isotonic cone loss = 0.0139\n",
            "0.8155035972595215\n",
            " isotonic cone loss = 0.0141\n",
            "0.8794469833374023\n",
            " isotonic cone loss = 0.0140\n",
            "0.8712736368179321\n",
            " isotonic cone loss = 0.0140\n",
            "0.8482275009155273\n",
            " isotonic cone loss = 0.0139\n",
            "0.6515210270881653\n",
            " isotonic cone loss = 0.0139\n",
            "0.6291511058807373\n",
            " isotonic cone loss = 0.0137\n",
            "0.6090466380119324\n",
            " isotonic cone loss = 0.0138\n",
            "0.8143762946128845\n",
            " isotonic cone loss = 0.0138\n",
            "0.7031179070472717\n",
            " isotonic cone loss = 0.0138\n",
            "0.7846949100494385\n",
            " isotonic cone loss = 0.0137\n",
            "0.7420077323913574\n",
            " isotonic cone loss = 0.0139\n",
            "0.7697691321372986\n",
            " isotonic cone loss = 0.0138\n",
            "0.6933470368385315\n",
            " isotonic cone loss = 0.0137\n",
            "0.8024341464042664\n",
            " isotonic cone loss = 0.0140\n",
            "0.7915367484092712\n",
            " isotonic cone loss = 0.0139\n",
            "0.7066323757171631\n",
            " isotonic cone loss = 0.0139\n",
            "0.7544440031051636\n",
            " isotonic cone loss = 0.0142\n",
            "0.6984256505966187\n",
            " isotonic cone loss = 0.0143\n",
            "0.7355265021324158\n",
            " isotonic cone loss = 0.0139\n",
            "0.8406845331192017\n",
            " isotonic cone loss = 0.0141\n",
            "0.7874996066093445\n",
            " isotonic cone loss = 0.0139\n",
            "0.6888959407806396\n",
            " isotonic cone loss = 0.0139\n",
            "0.8308826088905334\n",
            " isotonic cone loss = 0.0141\n",
            "0.7985059022903442\n",
            " isotonic cone loss = 0.0140\n",
            "0.6990401744842529\n",
            " isotonic cone loss = 0.0139\n",
            "0.6944109797477722\n",
            " isotonic cone loss = 0.0139\n",
            "0.789356529712677\n",
            " isotonic cone loss = 0.0136\n",
            "0.6462720632553101\n",
            " isotonic cone loss = 0.0136\n",
            "0.7431954741477966\n",
            " isotonic cone loss = 0.0139\n",
            "0.7477341294288635\n",
            " isotonic cone loss = 0.0140\n",
            "0.710034191608429\n",
            " isotonic cone loss = 0.0138\n",
            "0.7175285220146179\n",
            " isotonic cone loss = 0.0141\n",
            "0.673761248588562\n",
            " isotonic cone loss = 0.0140\n",
            "0.8592110276222229\n",
            " isotonic cone loss = 0.0140\n",
            "0.7321907877922058\n",
            " isotonic cone loss = 0.0140\n",
            "0.7261194586753845\n",
            " isotonic cone loss = 0.0139\n",
            "0.6978946924209595\n",
            " isotonic cone loss = 0.0141\n",
            "0.6702308058738708\n",
            " isotonic cone loss = 0.0139\n",
            "0.6773461699485779\n",
            " isotonic cone loss = 0.0139\n",
            "0.6853103041648865\n",
            " isotonic cone loss = 0.0138\n",
            "0.8160386681556702\n",
            " isotonic cone loss = 0.0136\n",
            "0.6833603978157043\n",
            " isotonic cone loss = 0.0140\n",
            "0.6777048110961914\n",
            " isotonic cone loss = 0.0139\n",
            "0.8173644542694092\n",
            " isotonic cone loss = 0.0139\n",
            "0.6952958106994629\n",
            " isotonic cone loss = 0.0140\n",
            "0.7386816143989563\n",
            " isotonic cone loss = 0.0141\n",
            "0.8083757758140564\n",
            " isotonic cone loss = 0.0140\n",
            "0.7791271209716797\n",
            " isotonic cone loss = 0.0139\n",
            "0.7806335687637329\n",
            " isotonic cone loss = 0.0138\n",
            "0.79914790391922\n",
            " isotonic cone loss = 0.0144\n",
            "0.6912012696266174\n",
            " isotonic cone loss = 0.0140\n",
            "0.7179914712905884\n",
            " isotonic cone loss = 0.0140\n",
            "0.7659319043159485\n",
            " isotonic cone loss = 0.0135\n",
            "0.8322169184684753\n",
            " isotonic cone loss = 0.0137\n",
            "0.6907908916473389\n",
            " isotonic cone loss = 0.0142\n",
            "0.783746600151062\n",
            " isotonic cone loss = 0.0139\n",
            "0.8131691217422485\n",
            " isotonic cone loss = 0.0138\n",
            "0.755261242389679\n",
            " isotonic cone loss = 0.0140\n",
            "0.7197693586349487\n",
            " isotonic cone loss = 0.0140\n",
            "0.6949037909507751\n",
            " isotonic cone loss = 0.0140\n",
            "0.7124100923538208\n",
            " isotonic cone loss = 0.0138\n",
            "0.6860024929046631\n",
            " isotonic cone loss = 0.0138\n",
            "0.7689395546913147\n",
            " isotonic cone loss = 0.0141\n",
            "0.6695742011070251\n",
            " isotonic cone loss = 0.0138\n",
            "0.7353863716125488\n",
            " isotonic cone loss = 0.0137\n",
            "0.695088267326355\n",
            " isotonic cone loss = 0.0138\n",
            "0.7855654954910278\n",
            " isotonic cone loss = 0.0141\n",
            "0.7043474316596985\n",
            " isotonic cone loss = 0.0142\n",
            "0.7578322291374207\n",
            " isotonic cone loss = 0.0140\n",
            "0.7839598655700684\n",
            " isotonic cone loss = 0.0140\n",
            "0.8757904767990112\n",
            " isotonic cone loss = 0.0139\n",
            "0.6886195540428162\n",
            " isotonic cone loss = 0.0138\n",
            "0.7898101806640625\n",
            " isotonic cone loss = 0.0137\n",
            "0.7574595808982849\n",
            " isotonic cone loss = 0.0138\n",
            "0.7866012454032898\n",
            " isotonic cone loss = 0.0139\n",
            "0.7639088034629822\n",
            " isotonic cone loss = 0.0139\n",
            "0.7555779218673706\n",
            " isotonic cone loss = 0.0139\n",
            "0.7009064555168152\n",
            " isotonic cone loss = 0.0139\n",
            "0.5884859561920166\n",
            " isotonic cone loss = 0.0139\n",
            "0.7458146214485168\n",
            " isotonic cone loss = 0.0135\n",
            "0.7782001495361328\n",
            " isotonic cone loss = 0.0135\n",
            "0.8116797208786011\n",
            " isotonic cone loss = 0.0136\n",
            "0.6801479458808899\n",
            " isotonic cone loss = 0.0141\n",
            "0.7193955183029175\n",
            " isotonic cone loss = 0.0142\n",
            "0.7051836848258972\n",
            " isotonic cone loss = 0.0140\n",
            "0.7770818471908569\n",
            " isotonic cone loss = 0.0139\n",
            "0.8499569892883301\n",
            " isotonic cone loss = 0.0137\n",
            "0.7195906639099121\n",
            " isotonic cone loss = 0.0138\n",
            "0.7849895358085632\n",
            " isotonic cone loss = 0.0137\n",
            "0.817700207233429\n",
            " isotonic cone loss = 0.0140\n",
            "0.7591652870178223\n",
            " isotonic cone loss = 0.0142\n",
            "0.7446545362472534\n",
            " isotonic cone loss = 0.0135\n",
            "0.7444723844528198\n",
            " isotonic cone loss = 0.0138\n",
            "0.731793999671936\n",
            " isotonic cone loss = 0.0135\n",
            "0.743385374546051\n",
            " isotonic cone loss = 0.0137\n",
            "0.7410137057304382\n",
            " isotonic cone loss = 0.0141\n",
            "0.7853701114654541\n",
            " isotonic cone loss = 0.0136\n",
            "0.7544474005699158\n",
            " isotonic cone loss = 0.0142\n",
            "0.6336867809295654\n",
            " isotonic cone loss = 0.0140\n",
            "0.7627614736557007\n",
            " isotonic cone loss = 0.0138\n",
            "0.7127507925033569\n",
            " isotonic cone loss = 0.0137\n",
            "0.663454532623291\n",
            " isotonic cone loss = 0.0139\n",
            "0.7475545406341553\n",
            " isotonic cone loss = 0.0138\n",
            "0.779163658618927\n",
            " isotonic cone loss = 0.0141\n",
            "0.7965928912162781\n",
            " isotonic cone loss = 0.0141\n",
            "0.5985841155052185\n",
            " isotonic cone loss = 0.0138\n",
            "0.7880657911300659\n",
            " isotonic cone loss = 0.0138\n",
            "0.7588074803352356\n",
            " isotonic cone loss = 0.0139\n",
            "0.8020306825637817\n",
            " isotonic cone loss = 0.0138\n",
            "0.7222228050231934\n",
            " isotonic cone loss = 0.0141\n",
            "0.6889269948005676\n",
            " isotonic cone loss = 0.0137\n",
            "0.7414536476135254\n",
            " isotonic cone loss = 0.0139\n",
            "0.7466716766357422\n",
            " isotonic cone loss = 0.0136\n",
            "0.6643008589744568\n",
            " isotonic cone loss = 0.0138\n",
            "0.7513021230697632\n",
            " isotonic cone loss = 0.0139\n",
            "0.6791636943817139\n",
            " isotonic cone loss = 0.0137\n",
            "0.6815369129180908\n",
            " isotonic cone loss = 0.0139\n",
            "0.6823605895042419\n",
            " isotonic cone loss = 0.0140\n",
            "0.8053066730499268\n",
            " isotonic cone loss = 0.0138\n",
            "0.797242283821106\n",
            " isotonic cone loss = 0.0141\n",
            "0.6980811357498169\n",
            " isotonic cone loss = 0.0134\n",
            "0.7527536749839783\n",
            " isotonic cone loss = 0.0141\n",
            "0.7462987303733826\n",
            " isotonic cone loss = 0.0144\n",
            "0.8178342580795288\n",
            " isotonic cone loss = 0.0143\n",
            "0.610369861125946\n",
            " isotonic cone loss = 0.0140\n",
            "0.7034290432929993\n",
            " isotonic cone loss = 0.0141\n",
            "0.6533205509185791\n",
            " isotonic cone loss = 0.0142\n",
            "0.6876155138015747\n",
            " isotonic cone loss = 0.0140\n",
            "0.6802732348442078\n",
            " isotonic cone loss = 0.0137\n",
            "0.6676818132400513\n",
            " isotonic cone loss = 0.0138\n",
            "0.7320602536201477\n",
            " isotonic cone loss = 0.0139\n",
            "0.6433838605880737\n",
            " isotonic cone loss = 0.0142\n",
            "0.5528980493545532\n",
            " isotonic cone loss = 0.0142\n",
            "0.6999239921569824\n",
            " isotonic cone loss = 0.0140\n",
            "0.7108858227729797\n",
            " isotonic cone loss = 0.0139\n",
            "0.6885561347007751\n",
            " isotonic cone loss = 0.0141\n",
            "0.6922839283943176\n",
            " isotonic cone loss = 0.0138\n",
            "0.6906514167785645\n",
            " isotonic cone loss = 0.0137\n",
            "0.6736627221107483\n",
            " isotonic cone loss = 0.0140\n",
            "0.685411274433136\n",
            " isotonic cone loss = 0.0139\n",
            "0.5379957556724548\n",
            " isotonic cone loss = 0.0140\n",
            "0.725874662399292\n",
            " isotonic cone loss = 0.0140\n",
            "0.7661880850791931\n",
            " isotonic cone loss = 0.0139\n",
            "0.7716860771179199\n",
            " isotonic cone loss = 0.0143\n",
            "0.7657265663146973\n",
            " isotonic cone loss = 0.0141\n",
            "0.7373082041740417\n",
            " isotonic cone loss = 0.0136\n",
            "0.8212796449661255\n",
            " isotonic cone loss = 0.0142\n",
            "0.5840100049972534\n",
            " isotonic cone loss = 0.0136\n",
            "0.6987617015838623\n",
            " isotonic cone loss = 0.0137\n",
            "0.6646862030029297\n",
            " isotonic cone loss = 0.0139\n",
            "0.7600921392440796\n",
            " isotonic cone loss = 0.0141\n",
            "0.7058849334716797\n",
            " isotonic cone loss = 0.0141\n",
            "0.6456072330474854\n",
            " isotonic cone loss = 0.0141\n",
            "0.5650227665901184\n",
            " isotonic cone loss = 0.0140\n",
            "0.7256109118461609\n",
            " isotonic cone loss = 0.0136\n",
            "0.6171282529830933\n",
            " isotonic cone loss = 0.0137\n",
            "0.7826906442642212\n",
            " isotonic cone loss = 0.0137\n",
            "0.778251588344574\n",
            " isotonic cone loss = 0.0137\n",
            "0.6332914233207703\n",
            " isotonic cone loss = 0.0142\n",
            "0.7429801225662231\n",
            " isotonic cone loss = 0.0143\n",
            "0.7415239810943604\n",
            " isotonic cone loss = 0.0140\n",
            "0.7238202095031738\n",
            " isotonic cone loss = 0.0139\n",
            "0.6296238303184509\n",
            " isotonic cone loss = 0.0137\n",
            "0.6904094219207764\n",
            " isotonic cone loss = 0.0139\n",
            "0.7577542066574097\n",
            " isotonic cone loss = 0.0140\n",
            "0.8446176648139954\n",
            " isotonic cone loss = 0.0137\n",
            "0.6655305624008179\n",
            " isotonic cone loss = 0.0139\n",
            "0.7682920694351196\n",
            " isotonic cone loss = 0.0133\n",
            "0.7136920094490051\n",
            " isotonic cone loss = 0.0137\n",
            "0.7409082055091858\n",
            " isotonic cone loss = 0.0139\n",
            "0.6534200310707092\n",
            " isotonic cone loss = 0.0138\n",
            "0.7148739099502563\n",
            " isotonic cone loss = 0.0136\n",
            "0.7225054502487183\n",
            " isotonic cone loss = 0.0138\n",
            "0.6884568333625793\n",
            " isotonic cone loss = 0.0137\n",
            "0.6736992001533508\n",
            " isotonic cone loss = 0.0141\n",
            "0.8367834687232971\n",
            " isotonic cone loss = 0.0141\n",
            "0.6320914030075073\n",
            " isotonic cone loss = 0.0140\n",
            "0.7377086281776428\n",
            " isotonic cone loss = 0.0141\n",
            "0.6954879760742188\n",
            " isotonic cone loss = 0.0139\n",
            "0.6641116738319397\n",
            " isotonic cone loss = 0.0138\n",
            "0.7269040942192078\n",
            " isotonic cone loss = 0.0141\n",
            "0.8149632215499878\n",
            " isotonic cone loss = 0.0139\n",
            "0.883384644985199\n",
            " isotonic cone loss = 0.0139\n",
            "0.7651147842407227\n",
            " isotonic cone loss = 0.0140\n",
            "0.7348540425300598\n",
            " isotonic cone loss = 0.0141\n",
            "0.6826781630516052\n",
            " isotonic cone loss = 0.0139\n",
            "0.774878203868866\n",
            " isotonic cone loss = 0.0138\n",
            "0.7114401459693909\n",
            " isotonic cone loss = 0.0141\n",
            "0.7734625935554504\n",
            " isotonic cone loss = 0.0137\n",
            "0.6859134435653687\n",
            " isotonic cone loss = 0.0138\n",
            "0.6663203239440918\n",
            " isotonic cone loss = 0.0136\n",
            "0.8465366959571838\n",
            " isotonic cone loss = 0.0141\n",
            "0.6274906396865845\n",
            " isotonic cone loss = 0.0137\n",
            "0.7464457154273987\n",
            " isotonic cone loss = 0.0139\n",
            "0.7400997281074524\n",
            " isotonic cone loss = 0.0140\n",
            "0.6886627674102783\n",
            " isotonic cone loss = 0.0139\n",
            "0.5550642013549805\n",
            " isotonic cone loss = 0.0137\n",
            "0.6560691595077515\n",
            " isotonic cone loss = 0.0137\n",
            "0.6651507019996643\n",
            " isotonic cone loss = 0.0138\n",
            "0.5999259948730469\n",
            " isotonic cone loss = 0.0137\n",
            "0.6873538494110107\n",
            " isotonic cone loss = 0.0135\n",
            "0.7113239765167236\n",
            " isotonic cone loss = 0.0138\n",
            "0.8014434576034546\n",
            " isotonic cone loss = 0.0137\n",
            "0.6548563838005066\n",
            " isotonic cone loss = 0.0137\n",
            "0.8267309665679932\n",
            " isotonic cone loss = 0.0136\n",
            "0.6776139736175537\n",
            " isotonic cone loss = 0.0139\n",
            "0.6720702052116394\n",
            " isotonic cone loss = 0.0140\n",
            "0.7921164035797119\n",
            " isotonic cone loss = 0.0141\n",
            "0.7551082968711853\n",
            " isotonic cone loss = 0.0138\n",
            "0.8067991137504578\n",
            " isotonic cone loss = 0.0137\n",
            "0.7705655097961426\n",
            " isotonic cone loss = 0.0140\n",
            "0.53635174036026\n",
            " isotonic cone loss = 0.0139\n",
            "0.5990859866142273\n",
            " isotonic cone loss = 0.0141\n",
            "0.7246639132499695\n",
            " isotonic cone loss = 0.0139\n",
            "0.7856351137161255\n",
            " isotonic cone loss = 0.0139\n",
            "0.611105740070343\n",
            " isotonic cone loss = 0.0138\n",
            "0.652732253074646\n",
            " isotonic cone loss = 0.0139\n",
            "0.7777186632156372\n",
            " isotonic cone loss = 0.0138\n",
            "0.6367654800415039\n",
            " isotonic cone loss = 0.0138\n",
            "0.657136857509613\n",
            " isotonic cone loss = 0.0140\n",
            "0.6657041311264038\n",
            " isotonic cone loss = 0.0139\n",
            "0.6840670108795166\n",
            " isotonic cone loss = 0.0136\n",
            "0.7832592129707336\n",
            " isotonic cone loss = 0.0136\n",
            "0.5789026021957397\n",
            " isotonic cone loss = 0.0139\n",
            "0.6195788979530334\n",
            " isotonic cone loss = 0.0140\n",
            "0.6358916759490967\n",
            " isotonic cone loss = 0.0140\n",
            "0.674781322479248\n",
            " isotonic cone loss = 0.0139\n",
            "0.7116594314575195\n",
            " isotonic cone loss = 0.0138\n",
            "0.6145638823509216\n",
            " isotonic cone loss = 0.0141\n",
            "0.6699662208557129\n",
            " isotonic cone loss = 0.0139\n",
            "0.6621934771537781\n",
            " isotonic cone loss = 0.0142\n",
            "0.7200338840484619\n",
            " isotonic cone loss = 0.0141\n",
            "0.7022377252578735\n",
            " isotonic cone loss = 0.0139\n",
            "0.5351799130439758\n",
            " isotonic cone loss = 0.0137\n",
            "0.5866907835006714\n",
            " isotonic cone loss = 0.0143\n",
            "0.6737037301063538\n",
            " isotonic cone loss = 0.0140\n",
            "0.5499758720397949\n",
            " isotonic cone loss = 0.0140\n",
            "0.6516551971435547\n",
            " isotonic cone loss = 0.0140\n",
            "0.6938591599464417\n",
            " isotonic cone loss = 0.0140\n",
            "0.6726090908050537\n",
            " isotonic cone loss = 0.0139\n",
            "0.5810161232948303\n",
            " isotonic cone loss = 0.0140\n",
            "0.7837548851966858\n",
            " isotonic cone loss = 0.0138\n",
            "0.6173502802848816\n",
            " isotonic cone loss = 0.0138\n",
            "0.5939598679542542\n",
            " isotonic cone loss = 0.0140\n",
            "0.6784889698028564\n",
            " isotonic cone loss = 0.0139\n",
            "0.6537802815437317\n",
            " isotonic cone loss = 0.0142\n",
            "0.6386050581932068\n",
            " isotonic cone loss = 0.0140\n",
            "0.7510392069816589\n",
            " isotonic cone loss = 0.0139\n",
            "0.7275959253311157\n",
            " isotonic cone loss = 0.0137\n",
            "0.6687630414962769\n",
            " isotonic cone loss = 0.0138\n",
            "0.6534271240234375\n",
            " isotonic cone loss = 0.0141\n",
            "0.7508964538574219\n",
            " isotonic cone loss = 0.0141\n",
            "0.5929620862007141\n",
            " isotonic cone loss = 0.0140\n",
            "0.6128115653991699\n",
            " isotonic cone loss = 0.0143\n",
            "0.6394422054290771\n",
            " isotonic cone loss = 0.0139\n",
            "0.767880916595459\n",
            " isotonic cone loss = 0.0138\n",
            "0.6243253350257874\n",
            " isotonic cone loss = 0.0136\n",
            "0.5571956634521484\n",
            " isotonic cone loss = 0.0142\n",
            "0.5811232328414917\n",
            " isotonic cone loss = 0.0141\n",
            "0.7870477437973022\n",
            " isotonic cone loss = 0.0141\n",
            "0.6083687543869019\n",
            " isotonic cone loss = 0.0139\n",
            "0.7702414393424988\n",
            " isotonic cone loss = 0.0138\n",
            "0.7352006435394287\n",
            " isotonic cone loss = 0.0139\n",
            "0.7924594283103943\n",
            " isotonic cone loss = 0.0139\n",
            "0.6344875693321228\n",
            " isotonic cone loss = 0.0141\n",
            "0.7735427618026733\n",
            " isotonic cone loss = 0.0140\n",
            "0.7329357862472534\n",
            " isotonic cone loss = 0.0135\n",
            "0.6735421419143677\n",
            " isotonic cone loss = 0.0138\n",
            "0.7275617718696594\n",
            " isotonic cone loss = 0.0141\n",
            "0.7516131401062012\n",
            " isotonic cone loss = 0.0139\n",
            "0.5983645915985107\n",
            " isotonic cone loss = 0.0142\n",
            "0.6919350624084473\n",
            " isotonic cone loss = 0.0141\n",
            "0.7540645003318787\n",
            " isotonic cone loss = 0.0139\n",
            "0.7058019042015076\n",
            " isotonic cone loss = 0.0137\n",
            "0.754374086856842\n",
            " isotonic cone loss = 0.0137\n",
            "0.6708532571792603\n",
            " isotonic cone loss = 0.0140\n",
            "0.6416943669319153\n",
            " isotonic cone loss = 0.0140\n",
            "0.8480656743049622\n",
            " isotonic cone loss = 0.0140\n",
            "0.6289597153663635\n",
            " isotonic cone loss = 0.0142\n",
            "0.666946530342102\n",
            " isotonic cone loss = 0.0142\n",
            "0.6565154194831848\n",
            " isotonic cone loss = 0.0140\n",
            "0.6660465002059937\n",
            " isotonic cone loss = 0.0138\n",
            "0.6537821888923645\n",
            " isotonic cone loss = 0.0138\n",
            "0.6469535827636719\n",
            " isotonic cone loss = 0.0138\n",
            "0.7473628520965576\n",
            " isotonic cone loss = 0.0138\n",
            "0.6243025064468384\n",
            " isotonic cone loss = 0.0137\n",
            "0.7081722617149353\n",
            " isotonic cone loss = 0.0141\n",
            "0.6584668755531311\n",
            " isotonic cone loss = 0.0137\n",
            "0.7066265940666199\n",
            " isotonic cone loss = 0.0141\n",
            "0.6843329668045044\n",
            " isotonic cone loss = 0.0142\n",
            "0.6448085904121399\n",
            " isotonic cone loss = 0.0140\n",
            "0.7142643928527832\n",
            " isotonic cone loss = 0.0139\n",
            "0.6888411045074463\n",
            " isotonic cone loss = 0.0140\n",
            "0.6832765936851501\n",
            " isotonic cone loss = 0.0138\n",
            "0.6394357681274414\n",
            " isotonic cone loss = 0.0138\n",
            "0.5925642848014832\n",
            " isotonic cone loss = 0.0139\n",
            "0.6982012987136841\n",
            " isotonic cone loss = 0.0141\n",
            "0.6358723640441895\n",
            " isotonic cone loss = 0.0140\n",
            "0.7394552230834961\n",
            " isotonic cone loss = 0.0140\n",
            "0.7098133563995361\n",
            " isotonic cone loss = 0.0137\n",
            "0.7253148555755615\n",
            " isotonic cone loss = 0.0137\n",
            "0.7890885472297668\n",
            " isotonic cone loss = 0.0138\n",
            "0.5864512920379639\n",
            " isotonic cone loss = 0.0137\n",
            "0.6510999798774719\n",
            " isotonic cone loss = 0.0140\n",
            "0.6770592927932739\n",
            " isotonic cone loss = 0.0139\n",
            "0.5643087029457092\n",
            " isotonic cone loss = 0.0140\n",
            "0.7377720475196838\n",
            " isotonic cone loss = 0.0142\n",
            "0.695441484451294\n",
            " isotonic cone loss = 0.0139\n",
            "0.6569945216178894\n",
            " isotonic cone loss = 0.0139\n",
            "0.542799174785614\n",
            " isotonic cone loss = 0.0136\n",
            "0.6932613849639893\n",
            " isotonic cone loss = 0.0141\n",
            "0.6556647419929504\n",
            " isotonic cone loss = 0.0142\n",
            "0.5545654296875\n",
            " isotonic cone loss = 0.0139\n",
            "0.5460711121559143\n",
            " isotonic cone loss = 0.0136\n",
            "0.6958038210868835\n",
            " isotonic cone loss = 0.0140\n",
            "0.6402330994606018\n",
            " isotonic cone loss = 0.0141\n",
            "0.7421446442604065\n",
            " isotonic cone loss = 0.0136\n",
            "0.6651338934898376\n",
            " isotonic cone loss = 0.0137\n",
            "0.5447385907173157\n",
            " isotonic cone loss = 0.0134\n",
            "0.711139976978302\n",
            " isotonic cone loss = 0.0138\n",
            "0.7728714346885681\n",
            " isotonic cone loss = 0.0140\n",
            "0.7493895888328552\n",
            " isotonic cone loss = 0.0137\n",
            "0.6303033232688904\n",
            " isotonic cone loss = 0.0139\n",
            "0.7206427454948425\n",
            " isotonic cone loss = 0.0138\n",
            "0.5382658243179321\n",
            " isotonic cone loss = 0.0139\n",
            "0.5432037115097046\n",
            " isotonic cone loss = 0.0141\n",
            "0.6330518126487732\n",
            " isotonic cone loss = 0.0140\n",
            "0.7102578282356262\n",
            " isotonic cone loss = 0.0137\n",
            "0.6664075255393982\n",
            " isotonic cone loss = 0.0136\n",
            "0.7187776565551758\n",
            " isotonic cone loss = 0.0136\n",
            "0.6886506080627441\n",
            " isotonic cone loss = 0.0137\n",
            "0.557878851890564\n",
            " isotonic cone loss = 0.0140\n",
            "0.6637280583381653\n",
            " isotonic cone loss = 0.0137\n",
            "0.7131562829017639\n",
            " isotonic cone loss = 0.0140\n",
            "0.7196792960166931\n",
            " isotonic cone loss = 0.0138\n",
            "0.7007200717926025\n",
            " isotonic cone loss = 0.0139\n",
            "0.6245378255844116\n",
            " isotonic cone loss = 0.0139\n",
            "0.62795490026474\n",
            " isotonic cone loss = 0.0139\n",
            "0.680401086807251\n",
            " isotonic cone loss = 0.0138\n",
            "0.6278077960014343\n",
            " isotonic cone loss = 0.0140\n",
            "0.6333932876586914\n",
            " isotonic cone loss = 0.0138\n",
            "0.6637541055679321\n",
            " isotonic cone loss = 0.0140\n",
            "0.6932095885276794\n",
            " isotonic cone loss = 0.0135\n",
            "0.747929573059082\n",
            " isotonic cone loss = 0.0137\n",
            "0.5123635530471802\n",
            " isotonic cone loss = 0.0137\n",
            "0.6358476877212524\n",
            " isotonic cone loss = 0.0140\n",
            "0.5673916935920715\n",
            " isotonic cone loss = 0.0142\n",
            "0.6284931302070618\n",
            " isotonic cone loss = 0.0140\n",
            "0.6384459137916565\n",
            " isotonic cone loss = 0.0139\n",
            "0.7556591033935547\n",
            " isotonic cone loss = 0.0143\n",
            "0.5692923069000244\n",
            " isotonic cone loss = 0.0142\n",
            "0.6172088980674744\n",
            " isotonic cone loss = 0.0141\n",
            "0.7424576282501221\n",
            " isotonic cone loss = 0.0138\n",
            "0.563710629940033\n",
            " isotonic cone loss = 0.0137\n",
            "0.6623430252075195\n",
            " isotonic cone loss = 0.0141\n",
            "0.628136396408081\n",
            " isotonic cone loss = 0.0140\n",
            "0.6569322943687439\n",
            " isotonic cone loss = 0.0139\n",
            "0.6340334415435791\n",
            " isotonic cone loss = 0.0142\n",
            "0.6031074523925781\n",
            " isotonic cone loss = 0.0138\n",
            "0.5982468128204346\n",
            " isotonic cone loss = 0.0136\n",
            "0.564258337020874\n",
            " isotonic cone loss = 0.0140\n",
            "0.6327775120735168\n",
            " isotonic cone loss = 0.0138\n",
            "0.6709486246109009\n",
            " isotonic cone loss = 0.0142\n",
            "0.630691409111023\n",
            " isotonic cone loss = 0.0139\n",
            "0.7454490661621094\n",
            " isotonic cone loss = 0.0138\n",
            "0.6931895613670349\n",
            " isotonic cone loss = 0.0138\n",
            "0.7606745362281799\n",
            " isotonic cone loss = 0.0140\n",
            "0.6223440170288086\n",
            " isotonic cone loss = 0.0142\n",
            "0.6701216101646423\n",
            " isotonic cone loss = 0.0139\n",
            "0.6352910399436951\n",
            " isotonic cone loss = 0.0138\n",
            "0.6776217818260193\n",
            " isotonic cone loss = 0.0137\n",
            "0.6796977519989014\n",
            " isotonic cone loss = 0.0141\n",
            "0.6674739718437195\n",
            " isotonic cone loss = 0.0139\n",
            "0.6278748512268066\n",
            " isotonic cone loss = 0.0140\n",
            "0.6462329626083374\n",
            " isotonic cone loss = 0.0136\n",
            "0.5817164182662964\n",
            " isotonic cone loss = 0.0140\n",
            "0.6052718758583069\n",
            " isotonic cone loss = 0.0138\n",
            "0.6945530772209167\n",
            " isotonic cone loss = 0.0135\n",
            "0.6452386379241943\n",
            " isotonic cone loss = 0.0139\n",
            "0.703350305557251\n",
            " isotonic cone loss = 0.0136\n",
            "0.6308064460754395\n",
            " isotonic cone loss = 0.0141\n",
            "0.8124008774757385\n",
            " isotonic cone loss = 0.0139\n",
            "0.6402674913406372\n",
            " isotonic cone loss = 0.0139\n",
            "0.7645559310913086\n",
            " isotonic cone loss = 0.0136\n",
            "0.6420702934265137\n",
            " isotonic cone loss = 0.0141\n",
            "0.6324148178100586\n",
            " isotonic cone loss = 0.0140\n",
            "0.7584577798843384\n",
            " isotonic cone loss = 0.0141\n",
            "0.5986723303794861\n",
            " isotonic cone loss = 0.0140\n",
            "0.6992394328117371\n",
            " isotonic cone loss = 0.0140\n",
            "0.5859324336051941\n",
            " isotonic cone loss = 0.0141\n",
            "0.5697881579399109\n",
            " isotonic cone loss = 0.0138\n",
            "0.7369340658187866\n",
            " isotonic cone loss = 0.0141\n",
            "0.6102912425994873\n",
            " isotonic cone loss = 0.0137\n",
            "0.5382574200630188\n",
            " isotonic cone loss = 0.0140\n",
            "0.5998747944831848\n",
            " isotonic cone loss = 0.0141\n",
            "0.5421665906906128\n",
            " isotonic cone loss = 0.0139\n",
            "0.6497887969017029\n",
            " isotonic cone loss = 0.0137\n",
            "0.6142950057983398\n",
            " isotonic cone loss = 0.0140\n",
            "0.6372185945510864\n",
            " isotonic cone loss = 0.0139\n",
            "0.663038432598114\n",
            " isotonic cone loss = 0.0139\n",
            "0.6335592865943909\n",
            " isotonic cone loss = 0.0139\n",
            "0.5414499640464783\n",
            " isotonic cone loss = 0.0140\n",
            "0.5676621198654175\n",
            " isotonic cone loss = 0.0138\n",
            "0.6443273425102234\n",
            " isotonic cone loss = 0.0139\n",
            "0.7361428737640381\n",
            " isotonic cone loss = 0.0141\n",
            "0.6704033017158508\n",
            " isotonic cone loss = 0.0141\n",
            "0.6866801381111145\n",
            " isotonic cone loss = 0.0140\n",
            "0.5790889263153076\n",
            " isotonic cone loss = 0.0141\n",
            "0.6679343581199646\n",
            " isotonic cone loss = 0.0138\n",
            "0.6695791482925415\n",
            " isotonic cone loss = 0.0141\n",
            "0.6062120795249939\n",
            " isotonic cone loss = 0.0142\n",
            "0.5447130799293518\n",
            " isotonic cone loss = 0.0140\n",
            "0.6261547803878784\n",
            " isotonic cone loss = 0.0140\n",
            "0.6883416771888733\n",
            " isotonic cone loss = 0.0142\n",
            "0.7833654880523682\n",
            " isotonic cone loss = 0.0140\n",
            "0.7384978532791138\n",
            " isotonic cone loss = 0.0136\n",
            "0.7114017605781555\n",
            " isotonic cone loss = 0.0140\n",
            "0.7233720421791077\n",
            " isotonic cone loss = 0.0137\n",
            "0.6262655258178711\n",
            " isotonic cone loss = 0.0138\n",
            "0.7147582769393921\n",
            " isotonic cone loss = 0.0141\n",
            "0.6142252087593079\n",
            " isotonic cone loss = 0.0141\n",
            "0.6080657839775085\n",
            " isotonic cone loss = 0.0137\n",
            "0.5919532179832458\n",
            " isotonic cone loss = 0.0138\n",
            "0.6821324229240417\n",
            " isotonic cone loss = 0.0134\n",
            "0.6096280217170715\n",
            " isotonic cone loss = 0.0137\n",
            "0.7518883943557739\n",
            " isotonic cone loss = 0.0138\n",
            "0.7050077319145203\n",
            " isotonic cone loss = 0.0137\n",
            "0.7318145632743835\n",
            " isotonic cone loss = 0.0136\n",
            "0.643696665763855\n",
            " isotonic cone loss = 0.0140\n",
            "0.7669792771339417\n",
            " isotonic cone loss = 0.0140\n",
            "0.6634678244590759\n",
            " isotonic cone loss = 0.0138\n",
            "0.5869482755661011\n",
            " isotonic cone loss = 0.0140\n",
            "0.6109253168106079\n",
            " isotonic cone loss = 0.0140\n",
            "0.5305032134056091\n",
            " isotonic cone loss = 0.0140\n",
            "0.6589850187301636\n",
            " isotonic cone loss = 0.0138\n",
            "0.686133861541748\n",
            " isotonic cone loss = 0.0138\n",
            "0.6113110780715942\n",
            " isotonic cone loss = 0.0139\n",
            "0.6726700067520142\n",
            " isotonic cone loss = 0.0137\n",
            "0.582064688205719\n",
            " isotonic cone loss = 0.0138\n",
            "0.6051005721092224\n",
            " isotonic cone loss = 0.0136\n",
            "0.5275133848190308\n",
            " isotonic cone loss = 0.0140\n",
            "0.6973506212234497\n",
            " isotonic cone loss = 0.0138\n",
            "0.6644067764282227\n",
            " isotonic cone loss = 0.0140\n",
            "0.6305645704269409\n",
            " isotonic cone loss = 0.0138\n",
            "0.6199868321418762\n",
            " isotonic cone loss = 0.0137\n",
            "0.717928946018219\n",
            " isotonic cone loss = 0.0136\n",
            "0.5774611830711365\n",
            " isotonic cone loss = 0.0137\n",
            "0.6277879476547241\n",
            " isotonic cone loss = 0.0136\n",
            "0.6438878178596497\n",
            " isotonic cone loss = 0.0138\n",
            "0.5747417211532593\n",
            " isotonic cone loss = 0.0139\n",
            "0.661819338798523\n",
            " isotonic cone loss = 0.0141\n",
            "0.5546740889549255\n",
            " isotonic cone loss = 0.0137\n",
            "0.6646803617477417\n",
            " isotonic cone loss = 0.0139\n",
            "0.5750288963317871\n",
            " isotonic cone loss = 0.0137\n",
            "0.6530116200447083\n",
            " isotonic cone loss = 0.0138\n",
            "0.49993789196014404\n",
            " isotonic cone loss = 0.0135\n",
            "0.5686763525009155\n",
            " isotonic cone loss = 0.0140\n",
            "0.6342365741729736\n",
            " isotonic cone loss = 0.0138\n",
            "0.5501573085784912\n",
            " isotonic cone loss = 0.0139\n",
            "0.6085085868835449\n",
            " isotonic cone loss = 0.0139\n",
            "0.6268645524978638\n",
            " isotonic cone loss = 0.0138\n",
            "0.6085386872291565\n",
            " isotonic cone loss = 0.0136\n",
            "0.6117494702339172\n",
            " isotonic cone loss = 0.0139\n",
            "0.6752599477767944\n",
            " isotonic cone loss = 0.0137\n",
            "0.752126157283783\n",
            " isotonic cone loss = 0.0139\n",
            "0.6757224798202515\n",
            " isotonic cone loss = 0.0138\n",
            "0.5787774324417114\n",
            " isotonic cone loss = 0.0139\n",
            "0.6850887537002563\n",
            " isotonic cone loss = 0.0139\n",
            "0.7956949472427368\n",
            " isotonic cone loss = 0.0140\n",
            "0.5302038788795471\n",
            " isotonic cone loss = 0.0139\n",
            "0.5663155317306519\n",
            " isotonic cone loss = 0.0137\n",
            "0.7327598333358765\n",
            " isotonic cone loss = 0.0138\n",
            "0.7001821398735046\n",
            " isotonic cone loss = 0.0141\n",
            "0.6291305422782898\n",
            " isotonic cone loss = 0.0140\n",
            "0.626326858997345\n",
            " isotonic cone loss = 0.0143\n",
            "0.6140410900115967\n",
            " isotonic cone loss = 0.0140\n",
            "0.5990645289421082\n",
            " isotonic cone loss = 0.0140\n",
            "0.6300561428070068\n",
            " isotonic cone loss = 0.0141\n",
            "0.6026347279548645\n",
            " isotonic cone loss = 0.0138\n",
            "0.5764822959899902\n",
            " isotonic cone loss = 0.0139\n",
            "0.5252804756164551\n",
            " isotonic cone loss = 0.0137\n",
            "0.7250586748123169\n",
            " isotonic cone loss = 0.0138\n",
            "0.6100180745124817\n",
            " isotonic cone loss = 0.0138\n",
            "0.6477133631706238\n",
            " isotonic cone loss = 0.0136\n",
            "0.5879383087158203\n",
            " isotonic cone loss = 0.0139\n",
            "0.588273286819458\n",
            " isotonic cone loss = 0.0138\n",
            "0.5226885080337524\n",
            " isotonic cone loss = 0.0137\n",
            "0.6244227290153503\n",
            " isotonic cone loss = 0.0143\n",
            "0.7068794965744019\n",
            " isotonic cone loss = 0.0138\n",
            "0.5985277891159058\n",
            " isotonic cone loss = 0.0136\n",
            "0.5476228594779968\n",
            " isotonic cone loss = 0.0139\n",
            "0.7269613742828369\n",
            " isotonic cone loss = 0.0138\n",
            "0.5037434101104736\n",
            " isotonic cone loss = 0.0139\n",
            "0.6594989895820618\n",
            " isotonic cone loss = 0.0138\n",
            "0.6965510845184326\n",
            " isotonic cone loss = 0.0141\n",
            "0.5287830829620361\n",
            " isotonic cone loss = 0.0140\n",
            "0.5515378713607788\n",
            " isotonic cone loss = 0.0140\n",
            "0.6729375123977661\n",
            " isotonic cone loss = 0.0141\n",
            "0.6870431900024414\n",
            " isotonic cone loss = 0.0138\n",
            "0.7226946353912354\n",
            " isotonic cone loss = 0.0136\n",
            "0.6592011451721191\n",
            " isotonic cone loss = 0.0139\n",
            "0.6269201636314392\n",
            " isotonic cone loss = 0.0140\n",
            "0.6264699101448059\n",
            " isotonic cone loss = 0.0141\n",
            "0.5770255327224731\n",
            " isotonic cone loss = 0.0140\n",
            "0.6900120377540588\n",
            " isotonic cone loss = 0.0140\n",
            "0.5636483430862427\n",
            " isotonic cone loss = 0.0142\n",
            "0.7073171138763428\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4105206561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4105206561.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2411969591.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mx_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# stages are ExplorerEngineerStage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2411969591.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, residual)\u001b[0m\n\u001b[1;32m    471\u001b[0m     ):\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengineer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2411969591.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2411969591.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynmix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#apply mixing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 6) Train / eval functions\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb[0], yb[0]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits , loss = model(xb,yb)\n",
        "        B, T, V = logits.shape\n",
        "        loss = loss\n",
        "        # Backprop\n",
        "        ## Compute auxiliary belief persistence loss\n",
        "        loss_rank = 1e-3 * rank_future_sequence_loss_soft(logits, yb)\n",
        "        loss_order= 1e-3 * ordered_future_loss(logits, yb)\n",
        "\n",
        "        print(f\" isotonic cone loss = {loss_rank.item() + loss_order.item():.4f}\")\n",
        "\n",
        "        # Total loss (tune the scale if needed)\n",
        "        loss = loss  + loss_rank + loss_order\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        print(loss.item())\n",
        "        total_loss += loss.item()\n",
        "        losses.append(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "# 7) Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss = train_epoch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nf2029jSJl9P",
        "outputId": "c4af2337-1cf2-4c96-d7d0-53887a23fa72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a70d627b2d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c_XtPYjAb2M",
        "outputId": "8d45f40f-3be0-4596-b20e-fedba622f02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: What light through yonder window breaks?\n",
            "Juliet, Doth you love me? But, must I; now I, gentle preventh:\n",
            "No seenators, give me his son that many days\n",
            "Supply the rapier's death; yea will tell him\n",
            "Go meeds and sulphur and make flatter of\n",
            "Which in his execution, her supple left his shape.\n",
            "\n",
            "Lord Marshal:\n",
            "Tell him fis limitation him to your daughter.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "I thank your grace well resolved his authority.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "O, that may famish, boy and so quickly he\n",
            "Sufficial brings, lies unthour duty both bends\n",
            "Like to his native punished my about,\n",
            "That bear this simple of this churchyard in,\n",
            "And as but righted bloody thing is right.\n",
            "If the duke is see it, seeing, the like a cime\n",
            "Sus from dista beguiles that the deceived\n",
            "For meeds not oft them poison, nor death\n",
            "To please the compass to better tempt.\n",
            "\n",
            "ISABELLA:\n",
            "Stay the duke in a creet to live well;\n",
            "He means the market-flaw thy degentle ear:\n",
            "Clarence death doth keep the ground the splder.\n",
            "White dream, and her heir to bed their grief,\n",
            "But thou art fitteth for his general slain.\n",
            "\n",
            "ROSEO:\n",
            "We must do so, distresseth not a ray of nature.\n",
            "Sirrah, be it rude, he must sit by the skies\n",
            "In hateful branch slain of my brother's love:\n",
            "I will not speak; no more than my past magiht\n",
            "Than reveal for unswoer: live look'd upon me,\n",
            "I  will enjoy and the matter been the store\n",
            "That never with the princely Claudio die,\n",
            "As crack'd by a boastard.\n",
            "\n",
            "CORIOLANUS:\n",
            "He that before this wise that valia, ho,\n",
            "Staying it up sulties himself.\n",
            "\n",
            "LEONTES:\n",
            "On his office!\n",
            "\n",
            "HERMIONE:\n",
            "And when the Capulets give you the coast\n",
            "In love to look upon this house to bed;\n",
            "The traitor speak, he shall know no let.\n",
            "\n",
            "GREMIO:\n",
            "Nay, praise our consenties from Lancaster.\n",
            "\n",
            "GRETHARISCE:\n",
            "O bloody Maria, by a half of less in postime.\n",
            "But by any death the king, the liokest of age,\n",
            "In chariot is out. Sir, I have drawn and thee!\n",
            "\n",
            "BEONTE:\n",
            "Onle a villain, but answer, the melty sake:\n",
            "And here my life can grievous lord a horse.\n",
            "Cousin, not out the citiz\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "def decode_chars(token_ids, itos):\n",
        "    \"\"\"\n",
        "    Decodes a list of character token IDs into a string.\n",
        "    \"\"\"\n",
        "    return ''.join([itos[i] for i in token_ids])\n",
        "\n",
        "def encode_chars(text, stoi):\n",
        "    \"\"\"\n",
        "    Encodes a string into a list of token IDs, one per character.\n",
        "    \"\"\"\n",
        "    return [stoi.get(c, 0) for c in text]\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def decode_sequence_char(\n",
        "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
        "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Character-level decoding from a prompt using the model's logits.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    start_ids = encode_chars(prompt, stoi)\n",
        "    idx = torch.tensor([start_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        context = idx[:, -block_size:]\n",
        "        logits, _ = model(context,None)\n",
        "        last_logits = logits[:, -1, :]\n",
        "\n",
        "\n",
        "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
        "\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "    return decode_chars(idx[0].tolist(), itos)\n",
        "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi = meta[\"stoi\"]\n",
        "itos = meta[\"itos\"]\n",
        "\n",
        "prompt = \"ROMEO: What light through yonder window breaks?\\nJuliet, Doth you love me?\"\n",
        "generated = decode_sequence_char(\n",
        "    model=model,\n",
        "    stoi=stoi,\n",
        "    itos=itos,\n",
        "    prompt=prompt,\n",
        "    max_new_tokens=1900,\n",
        "    block_size=1024,\n",
        "    use_fenchel=False,\n",
        "    tau=1.5,\n",
        "    fenchel_iters=2,\n",
        "    temperature=0.8\n",
        ")\n",
        "\n",
        "print(generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "VRHZquqsWBbL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def decode_chars(token_ids, itos):\n",
        "    \"\"\"\n",
        "    Decodes a list of character token IDs into a string.\n",
        "    \"\"\"\n",
        "    return ''.join([itos[i] for i in token_ids])\n",
        "\n",
        "def encode_chars(text, stoi):\n",
        "    \"\"\"\n",
        "    Encodes a string into a list of token IDs, one per character.\n",
        "    \"\"\"\n",
        "    return [stoi.get(c, 0) for c in text]\n",
        "def decode_sequence_char_greedy(model, stoi, itos, prompt, max_new_tokens=100, block_size=256):\n",
        "    \"\"\"\n",
        "    Deterministic (greedy) character-level decoding for exact memorization checks.\n",
        "    This avoids torch.multinomial() entirely.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    start_ids = encode_chars(prompt, stoi)\n",
        "    idx = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        context = idx[:, -block_size:]\n",
        "        logits, _ = model(context, None)\n",
        "        last_logits = logits[:, -1, :]\n",
        "        next_token = torch.argmax(last_logits, dim=-1, keepdim=True)  # greedy\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "    return decode_chars(idx[0].tolist(), itos)\n",
        "\n",
        "# Usage:\n",
        "prompt = \"Good, speak to the mariners. Fall to, yarely, or we run ourselves aground. Bestir, bestir!\"\n",
        "generated = decode_sequence_char_greedy(\n",
        "    model=model,\n",
        "    stoi=stoi,\n",
        "    itos=itos,\n",
        "    prompt=prompt,\n",
        "    max_new_tokens=2048,\n",
        "    block_size=1024\n",
        ")\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmCa0c92XsUk",
        "outputId": "42a05351-4846-4659-b19c-e1d78d9eefba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good, speak to the mariners. Fall to, yarely, or we run ourselves aground. Bestir, bestir!\n",
            "\n",
            "LUCIO:\n",
            "And there will be so shall be so my son of the\n",
            "country's death, and the stream of the streams of\n",
            "the senators of the duke of the duke of the duker\n",
            "that the second of the duke of the duke.\n",
            "\n",
            "SAMPSON:\n",
            "What, what is the country's soul of the duke?\n",
            "\n",
            "SAMPSON:\n",
            "So shall be so, and the senator of the seas.\n",
            "\n",
            "SAMPSON:\n",
            "I will be so, and the stream of the stream of the\n",
            "country the senators of the senators of the seas\n",
            "And will be so the senators of the prince,\n",
            "That which with the second of the seas\n",
            "And set the senators of the prince of the sea,\n",
            "And what they are better than the sea\n",
            "Than the preservant of the senators of the sea,\n",
            "And what they are the court of the seas\n",
            "And shall be so better than the second sea,\n",
            "And with the secret soul of the prince,\n",
            "And what they are better than the senators,\n",
            "And then the senators of the court of the sea,\n",
            "And there will be so strange and the sea\n",
            "That which the senators of the duke of the sea,\n",
            "And there all the course of the seas\n",
            "And see the senators of the present seat,\n",
            "And there will be so many man of the sea,\n",
            "And there will be so many and severeign.\n",
            "\n",
            "LUCIO:\n",
            "Ay, and the seas of the season of the seas\n",
            "And send the seas of the season of the sea,\n",
            "And shall be so many servant of the sea,\n",
            "And shall I send to the senators of the death,\n",
            "That which shall be so much a treason and the death?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Then we must be so many soul of the sea,\n",
            "And with the senators of the duke of the duke.\n",
            "\n",
            "GREGORY:\n",
            "And what is the seas of the season of the sea,\n",
            "And there is a sense of the season of the sea,\n",
            "And there will be so the senators of the death,\n",
            "And there will be so dear and the duke\n",
            "That was the court and seven and the state\n",
            "That we may seem to the senators of the sea,\n",
            "And there will be some son of the seas\n",
            "And send the seas of the duke of the sea,\n",
            "And shall be so better than the season of the death,\n",
            "That we may see the senators of the duke of the\n",
            "That we will be the secret soul of the seas of the\n",
            "That we will be the constable of the duke of the\n",
            "That was the court of the seas of the duke of the\n",
            "Th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz8NUsW4Qdfj",
        "outputId": "6617c9bf-7a84-4f62-9213-3922767e8248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6614656"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fIPU3pjCQcqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEZgPXclyKlb",
        "outputId": "a13082b8-2ff3-4cfb-9e51-a8f9fff9499b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jacobian rank (final token, projected): 66\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import grad\n",
        "'''\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        embeddings = torch.cat([self.transformer.wte[i](idx)  for i in range(self.n_head)], dim=-1)\n",
        "        embeddings = embeddings + pos_emb\n",
        "        x = self.transformer.drop(embeddings)\n",
        "        x_orig = x.clone()\n",
        "        for stage in self.transformer.h:  # stages are ExplorerEngineerStage\n",
        "          x = stage(x, x_orig)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "'''\n",
        "\n",
        "\n",
        "def compute_jacobian_rank(model, idx, projection_dim=324):\n",
        "    \"\"\"\n",
        "    Compute the rank of the Jacobian of projected logits w.r.t. input embeddings\n",
        "    for the final token position only, with output space optionally projected.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = idx.device\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    b, t = idx.size()\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "    emb = torch.cat([model.transformer.wte[i](idx)  for i in range(model.n_head)], dim=-1)\n",
        "    emb = emb + pos_emb\n",
        "    emb.requires_grad_(True)\n",
        "\n",
        "\n",
        "    # Forward pass using embedding input\n",
        "    def forward_emb(emb_input):\n",
        "        xorig = emb_input.clone()\n",
        "        x = model.transformer.drop(emb_input)\n",
        "        accum = torch.zeros_like(x)\n",
        "        for i, block in enumerate(model.transformer.h):\n",
        "            x= block(x,xorig)\n",
        "\n",
        "        x = model.transformer.ln_f(x)\n",
        "        logits = model.lm_head(x)\n",
        "        return logits[:, -1, :projection_dim]  # final token, projected\n",
        "\n",
        "    output = forward_emb(emb)\n",
        "    jacobian_rows = []\n",
        "\n",
        "    for i in range(output.shape[-1]):\n",
        "        grad_output = torch.zeros_like(output)\n",
        "        grad_output[:, i] = 1.0\n",
        "        grad_i = grad(output, emb, grad_outputs=grad_output, retain_graph=True)[0]\n",
        "        row = grad_i[:, -1, :].detach().cpu().numpy()  # final token, last dim\n",
        "        jacobian_rows.append(row.squeeze())\n",
        "\n",
        "    J = np.stack(jacobian_rows, axis=0)  # shape: [proj_dim, emb_dim]\n",
        "    rank = np.linalg.matrix_rank(J)\n",
        "    return rank\n",
        "\n",
        "\n",
        "def compute_drift_trajectories(model, idx):\n",
        "    \"\"\"\n",
        "    Compute drift vectors Î”h = h_{l+1} - h_l across transformer layers\n",
        "    at each token position.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = idx.device\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    b, t = idx.size()\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "    emb = torch.cat([model.transformer.wte[i](idx)  for i in range(model.n_head)], dim=-1)\n",
        "    x = emb + pos_emb\n",
        "    xorig = x.clone()\n",
        "\n",
        "    layers = []\n",
        "    dy = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for block in model.transformer.h:\n",
        "            x= block(x,xorig)\n",
        "            layers.append(x.clone())\n",
        "\n",
        "    drifts = [layers[i+1] - layers[i] for i in range(len(layers)-1)]\n",
        "    drift_norms = [d.norm(dim=-1).mean(dim=-1).cpu().numpy() for d in drifts]\n",
        "    return drift_norms\n",
        "\n",
        "\n",
        "def plot_drift(drift_norms):\n",
        "    \"\"\"\n",
        "    Plot average drift norm per layer.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot([np.mean(d) for d in drift_norms], marker='o')\n",
        "    plt.title(\"Average Drift Norm Across Layers\")\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(\"â€–Î”hâ€– mean\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_ids = torch.randint(0, model.config.vocab_size, (1, model.config.n_head*2)).to(device)\n",
        "\n",
        "jac_rank = compute_jacobian_rank(model, input_ids)\n",
        "drift_norms = compute_drift_trajectories(model, input_ids)\n",
        "plot_drift(drift_norms)\n",
        "\n",
        "print(\"Jacobian rank (final token, projected):\", jac_rank)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKXFCMBzyKlc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def collect_drift_matrix(model, idx):\n",
        "    \"\"\"\n",
        "    Compute drift vectors Î”h = h_{l+1} - h_l across transformer layers\n",
        "    at each token position.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = idx.device\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    b, t = idx.size()\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "    # Get embeddings, set requires_grad\n",
        "    pos_emb = model.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "    emb = torch.cat([model.transformer.wte[i](idx)  for i in range(model.n_head)], dim=-1)\n",
        "    x = emb + pos_emb\n",
        "    xorig = x.clone()\n",
        "\n",
        "    states = []\n",
        "    dy = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for block in model.transformer.h:\n",
        "            x= block(x,xorig)\n",
        "            states.append(x.clone())\n",
        "\n",
        "    drifts = [states[i+1] - states[i] for i in range(len(states)-1)]  # list of tensors [B,T,C]\n",
        "    drift_matrix = torch.cat([d.reshape(-1, d.shape[-1]).cpu() for d in drifts], dim=0)\n",
        "    return drift_matrix  # shape: [N, D]\n",
        "\n",
        "def run_drift_pca(drift_matrix, k=40):\n",
        "    \"\"\"\n",
        "    Run PCA and report explained variance for top-k components.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=k)\n",
        "    pca.fit(drift_matrix.numpy())\n",
        "    explained = pca.explained_variance_ratio_\n",
        "    return explained, pca\n",
        "\n",
        "def plot_explained_variance(explained):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(np.cumsum(explained) * 100, marker='o')\n",
        "    plt.xlabel(\"Principal Component\")\n",
        "    plt.ylabel(\"Cumulative Variance Explained (%)\")\n",
        "    plt.title(\"Drift Trajectory PCA: Explained Variance\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZThjWOQeyKlc"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "import seaborn as sns\n",
        "\n",
        "def get_projected_residuals(drift_matrix, pca):\n",
        "    \"\"\"\n",
        "    Project Î”h onto PCA space and get residuals.\n",
        "    \"\"\"\n",
        "    proj = pca.transform(drift_matrix.numpy())\n",
        "    recon = pca.inverse_transform(proj)\n",
        "    residuals = drift_matrix.numpy() - recon\n",
        "    return proj, residuals\n",
        "\n",
        "def fit_gmm(proj_data, k=4):\n",
        "    \"\"\"\n",
        "    Fit GMM to PCA-projected drift vectors to identify latent regimes.\n",
        "    \"\"\"\n",
        "    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=0)\n",
        "    gmm.fit(proj_data)\n",
        "    labels = gmm.predict(proj_data)\n",
        "    return gmm, labels\n",
        "\n",
        "def plot_gmm_clusters(proj_data, labels):\n",
        "    \"\"\"\n",
        "    Plot GMM clustering over first 2 PCA components.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=proj_data[:, 0], y=proj_data[:, 1], hue=labels, palette=\"tab10\", s=10)\n",
        "    plt.title(\"Latent Regimes from Drift PCA (GMM Clusters)\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUPNo1flyKlc",
        "outputId": "7155fabd-2238-4941-ceb9-5748d67c7634"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Inputs\n",
        "input_ids = torch.randint(0, model.config.vocab_size, (1, model.config.n_head*2)).to(device)\n",
        "\n",
        "# Drift matrix\n",
        "drift_matrix = collect_drift_matrix(model, input_ids)\n",
        "\n",
        "# PCA and variance\n",
        "explained, pca = run_drift_pca(drift_matrix, k=40)\n",
        "plot_explained_variance(explained)\n",
        "\n",
        "# Residuals and GMM regime fit\n",
        "proj, residuals = get_projected_residuals(drift_matrix, pca)\n",
        "gmm, labels = fit_gmm(proj, k=4)\n",
        "plot_gmm_clusters(proj, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_2KT6IcyKlc",
        "outputId": "f2594511-2436-4ad0-bde3-66c33d7b6911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Baseline] Val loss: 1.8266\n",
            "\n",
            "--- Per-Layer Ablation Report ---\n",
            "Ablate Layer  0: Loss = 3.8196 | Î” = 1.9930\n",
            "Ablate Layer  1: Loss = 1.8639 | Î” = 0.0372\n",
            "Ablate Layer  2: Loss = 2.3641 | Î” = 0.5375\n",
            "Ablate Layer  3: Loss = 2.9427 | Î” = 1.1161\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from contextlib import contextmanager\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==============================\n",
        "# ðŸ” LAYER ABLATION LOGIC BLOCK\n",
        "# =============================\n",
        "\n",
        "# === Config ===\n",
        "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
        "block_size = 256\n",
        "batch_size = 1\n",
        "# === Load tokenizer metadata ===\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# === Load mmap data (char-level tokens, uint16) ===\n",
        "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
        "\n",
        "# === Efficient GPU Batch Sampler ===\n",
        "class GPUBatchDataset(Dataset):\n",
        "    def __init__(self, mmap_file, block_size, batch_size, device):\n",
        "        self.data = mmap_file\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.total = len(self.data) - block_size - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
        "        for i in range(self.batch_size):\n",
        "            start = np.random.randint(0, self.total // self.block_size) * self.block_size\n",
        "            X[i] = self.data[start : start + self.block_size]\n",
        "            Y[i] = self.data[start + 1 : start + 1 + self.block_size]\n",
        "        return (\n",
        "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
        "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
        "        )\n",
        "\n",
        "val_dataset = GPUBatchDataset(val_ids, block_size, batch_size=1, device=device)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "losses = []\n",
        "import types\n",
        "import inspect\n",
        "@contextmanager\n",
        "def layer_ablation_context(model, layers_to_ablate):\n",
        "    \"\"\"Context manager that ablates selected layers by making them identity maps\n",
        "    with respect to the primary stream `x`, while still accepting `x_orig`.\"\"\"\n",
        "    original_forwards = [block.forward for block in model.transformer.h]\n",
        "\n",
        "    def make_identity_forward(block, original_fn):\n",
        "        # Keep the same call shape; return the incoming x unchanged.\n",
        "        # Works whether the block expects (x) or (x, x_orig, ...).\n",
        "        def ablated_forward(self, *args, **kwargs):\n",
        "            # Bound method: args do NOT include self.\n",
        "            # We expect at least x in args or kwargs.\n",
        "            if len(args) >= 1:\n",
        "                x = args[0]\n",
        "                return x\n",
        "            # Fallback if caller used kwargs\n",
        "            if \"x\" in kwargs:\n",
        "                return kwargs[\"x\"]\n",
        "            # If signature is unusual, defer to original to avoid crash ?1\n",
        "            return original_fn(*args, **kwargs)\n",
        "        return types.MethodType(ablated_forward, block)\n",
        "\n",
        "    try:\n",
        "        for i, block in enumerate(model.transformer.h):\n",
        "            if i in layers_to_ablate:\n",
        "                model.transformer.h[i].forward = make_identity_forward(block, original_forwards[i])\n",
        "        yield\n",
        "    finally:\n",
        "        for i, block in enumerate(model.transformer.h):\n",
        "            model.transformer.h[i].forward = original_forwards[i]\n",
        "@torch.no_grad()\n",
        "def eval_epoch(max_batches=50):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        xb, yb = xb[0], yb[0]\n",
        "        logits, _ = model(xb, yb)\n",
        "        B, T, V = logits.shape\n",
        "        total_loss += criterion(logits.view(B * T, V),\n",
        "                                yb.view(B * T)).item()\n",
        "    return total_loss / max_batches\n",
        "\n",
        "# ---- Evaluate baseline without any ablation ----\n",
        "with layer_ablation_context(model, layers_to_ablate=set()):\n",
        "    baseline_val_loss = eval_epoch()\n",
        "print(f\"[Baseline] Val loss: {baseline_val_loss:.4f}\")\n",
        "\n",
        "# ---- Run per-layer ablation safely ----\n",
        "print(\"\\n--- Per-Layer Ablation Report ---\")\n",
        "results = []\n",
        "for i in range(model.config.n_layer):\n",
        "    with layer_ablation_context(model, layers_to_ablate={i}):\n",
        "        loss = eval_epoch()\n",
        "    delta = loss - baseline_val_loss\n",
        "    results.append((i, loss, delta))\n",
        "    print(f\"Ablate Layer {i:2d}: Loss = {loss:.4f} | Î” = {delta:.4f}\")\n",
        "\n",
        "# ---- Plotting ----\n",
        "layer_ids = [i for i, _, _ in results]\n",
        "delta_vals = [delta for _, _, delta in results]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(layer_ids, delta_vals, marker='o')\n",
        "plt.title(\"Î” Loss from Per-Layer Ablation\")\n",
        "plt.xlabel(\"Layer Index\")\n",
        "plt.ylabel(\"Î” Validation Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZSEf8RyKlc",
        "outputId": "18ccaa53-a533-4aab-a281-14256761fff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ” Probe: ['why', 'what', 'where', 'when'] vs who\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "Module [ModuleList] is missing the required \"forward\" function",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m words, outlier \u001b[38;5;129;01min\u001b[39;00m test_sets:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ” Probe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutlier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mplot_parallelogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutlier\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mplot_parallelogram\u001b[39m\u001b[34m(words, outlier)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_parallelogram\u001b[39m(words, outlier=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     vecs = [\u001b[43mget_char_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m     28\u001b[39m     labels = words.copy()\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m outlier:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mget_char_embedding\u001b[39m\u001b[34m(word)\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     emb = \u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [1, T, d]\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m emb.mean(dim=\u001b[32m1\u001b[39m).squeeze(\u001b[32m0\u001b[39m).cpu().numpy()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:399\u001b[39m, in \u001b[36m_forward_unimplemented\u001b[39m\u001b[34m(self, *input)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, *\u001b[38;5;28minput\u001b[39m: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[33;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m \u001b[33;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    400\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] is missing the required \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m function\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    401\u001b[39m     )\n",
            "\u001b[31mNotImplementedError\u001b[39m: Module [ModuleList] is missing the required \"forward\" function"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# === Load vocab ===\n",
        "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "\n",
        "stoi = meta[\"stoi\"]\n",
        "itos = meta[\"itos\"]\n",
        "device = next(model.parameters()).device\n",
        "wte = model.transformer.wte\n",
        "\n",
        "# === Char embedding extractor ===\n",
        "def get_char_embedding(word):\n",
        "    ids = [stoi[c] for c in word if c in stoi]\n",
        "    if not ids:\n",
        "        return None\n",
        "    with torch.no_grad():\n",
        "        emb = wte(torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0))  # [1, T, d]\n",
        "        return emb.mean(dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "# === Plot one parallelogram ===\n",
        "def plot_parallelogram(words, outlier=None):\n",
        "    vecs = [get_char_embedding(w) for w in words]\n",
        "    labels = words.copy()\n",
        "    if outlier:\n",
        "        vecs.append(get_char_embedding(outlier))\n",
        "        labels.append(outlier)\n",
        "\n",
        "    # drop None values\n",
        "    vecs = [v for v in vecs if v is not None]\n",
        "    if len(vecs) < 4:\n",
        "        print(f\"âš ï¸ Not enough valid embeddings for: {labels}\")\n",
        "        return\n",
        "\n",
        "    vecs = np.stack(vecs)\n",
        "    proj = PCA(n_components=2).fit_transform(vecs)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(proj[:, 0], proj[:, 1], color='black')\n",
        "    for i, label in enumerate(labels):\n",
        "        plt.text(proj[i, 0], proj[i, 1], label, fontsize=10)\n",
        "\n",
        "    if len(proj) >= 4:\n",
        "        A, B, C, D = proj[:4]\n",
        "        plt.plot([A[0], B[0]], [A[1], B[1]], 'r--')\n",
        "        plt.plot([C[0], D[0]], [C[1], D[1]], 'r--')\n",
        "        plt.plot([A[0], C[0]], [A[1], C[1]], 'b--')\n",
        "        plt.plot([B[0], D[0]], [B[1], D[1]], 'b--')\n",
        "\n",
        "    plt.title(\"Parallelogram Probe\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Define test sets ===\n",
        "test_sets = [\n",
        "    ([\"why\", \"what\", \"where\", \"when\"], \"who\"),               # verbâ†’third person\n",
        "    ([\"prepare\", \"begin\", \"cease\", \"end\"], \"remain\"),               # singularâ†’plural\n",
        "    ([\"happy\", \"sad\", \"disagreeable\", \"willing\"], \"afraid\"),       # negation prefix\n",
        "    ([\"king\", \"queen\", \"man\", \"woman\"], \"apple\"),            # gender pairs\n",
        "    ([\"king\", \"knight\", \"man\", \"maiden\"], \"book\"),\n",
        "    ([\"to\", \"from\", \"in\", \"out\"], \"the\"),                    # function-word symmetry\n",
        "]\n",
        "\n",
        "# === Run all probes ===\n",
        "for words, outlier in test_sets:\n",
        "    print(f\"\\nðŸ” Probe: {words} vs {outlier}\")\n",
        "    plot_parallelogram(words, outlier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "97LSWZHnyKlc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "c80671b8-2cd6-4887-e5d3-9c97a72f1060"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2242291821.py, line 54)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2242291821.py\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    for cat in\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import torch, pickle, matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# ---------- load vocab ----------\n",
        "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "\n",
        "# ---------- model bits ----------\n",
        "device = next(model.parameters()).device\n",
        "model.eval()                      # <â€” turn off dropout everywhere\n",
        "wte = model.transformer.wte\n",
        "wte.eval()                        #   (StairEmbed also has dropout)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_char_embedding(word):\n",
        "    ids = [stoi[c] for c in word if c in stoi]\n",
        "    if len(ids) < 2:              # â† need â‰¥2 chars for HailFire geometry\n",
        "        return None\n",
        "    with torch.no_grad():\n",
        "        t = torch.tensor(ids, device=device).unsqueeze(0)      # [1, T]\n",
        "        emb = wte(t).mean(dim=1).squeeze(0)                    # (d,)\n",
        "        return emb.cpu().numpy()\n",
        "\n",
        "# ---------- categories ----------\n",
        "categories = {\n",
        "    \"Verbs\":        [\"play\",\"run\",\"eat\",\"sleep\",\"jump\",\"talk\",\"walk\"],\n",
        "    \"Nouns\":        [\"dog\",\"tree\",\"car\",\"book\",\"child\",\"house\",\"apple\"],\n",
        "    \"FunctionWords\":[\"the\",\"and\",\"in\",\"on\",\"to\",\"of\",\"a   \"],\n",
        "    \"Punctuation\":  [\".    \",\",    \",\"!    \",\"?    \"],\n",
        "}\n",
        "\n",
        "# ---------- gather ----------\n",
        "vecs, labels, words = [], [], []\n",
        "for cat, wl in categories.items():\n",
        "    for w in wl:\n",
        "        v = get_char_embedding(w)\n",
        "        if v is not None:\n",
        "            vecs.append(v)\n",
        "            labels.append(cat)\n",
        "            words.append(w)\n",
        "\n",
        "vecs = np.stack(vecs)             # â† now guaranteed rectangular\n",
        "\n",
        "# ---------- PCA ----------\n",
        "proj = PCA(n_components=2).fit_transform(vecs)\n",
        "\n",
        "# ---------- plot ----------\n",
        "colors = dict(Verbs=\"blue\", Nouns=\"red\",\n",
        "              FunctionWords=\"green\", Punctuation=\"purple\")\n",
        "\n",
        "plt.figure(figsize=(9,7))\n",
        "for cat in\n",
        " categories:\n",
        "    idx = [i for i,l in enumerate(labels) if l==cat]\n",
        "    plt.scatter(proj[idx,0], proj[idx,1], label=cat, color=colors[cat])\n",
        "    for i in idx:\n",
        "        plt.text(proj[i,0], proj[i,1], words[i], fontsize=9)\n",
        "\n",
        "plt.title(\"Word-class clusters (char-averaged embeddings)\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "plt.show()\n",
        "def coop2_fn(R: torch.Tensor, C: torch.Tensor,n) -> torch.Tensor:\n",
        "    x = R**2 + 2*R + C * (1 + R.abs())- R * (1 + C.abs())\n",
        "    return torch.clamp(x, min=-n-1, max=n+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoFwC55KyKld"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}