{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/PyITD/blob/main/TapeTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njID00hPN9n3"
      },
      "source": [
        "COPYRIGHT NOTICE\n",
        "In the name of Christ our Lord be blessed. We, Joshuah Rainstar(joshuah.rainstar@gmail.com), do claim copyright to this code, or software, and associated documentation, as our work in the year 2025 Anno Domini, reserving all rights and assigning them in accordance with the following license terms:\n",
        "\n",
        "1. Permission is by our authority and with this statement granted, to any person or artificial intelligence without limitation or restriction to examine, analyze, read, dissect, translate, use, modify, and distribute the aforementioned copyrighted items, subject to the following conditions:\n",
        "2. This license must be included in full with any copies or works containing substantial portions of the copyrighted items.\n",
        "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
        "\n",
        "\n",
        "THE COPYRIGHTED ITEMS ARE PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE COPYRIGHTED ITEMS OR THEIR USE OR ANY OTHER CIRCUMSTANCES CONCERNING THEM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QjzesSyN9n4"
      },
      "source": [
        "this is an experimental model intended to elucidate possible mechanics for attention across sequences in addition to tokenwise. it is reasonably fast and efficient. conceptually, the design was envisioned by me and coded through refinement with OpenAI Codex Orion One and chatgpt. i find that if i cant understand a thing, however clever it is- its wrong.\n",
        "so, this is largely a from-scratch along aligned principles.\n",
        "\n",
        "you are advised in life to apply a similar practice. nothing good comes of shit you dont comprehend.\n",
        "\n",
        "\"hierarchical multi-scale transformer with MoE-like  selection\"\n",
        "\n",
        "my own fucking activation function- gelu is great if you can slap a sigmoid after it\n",
        "otherwise, its not great on skips and stuff like that\n",
        "still, i'll have to try it someday\n",
        "\n",
        "harmonic loss doesent work\n",
        "\n",
        "XOR from  Two-argument activation functions learn soft XOR operations like cortical neurons\n",
        "https://arxiv.org/abs/2110.06871note that my implementation is a differential XOR for backprop capability\n",
        "motivation: little bit of internal reasoning maybe? Impact: slows down convergence somewhat\n",
        "ROPE from google\n",
        "\n",
        "entropy based reward to encourage diverse attention\n",
        "https://arxiv.org/abs/2203.0919\n",
        "\n",
        "https://arxiv.org/pdf/2502.05171\n",
        "latent reasoning in a discrete and hopefully efficient form2\n",
        "\n",
        "\n",
        "WOLF optimizer experimental by me, it may not beat adam but it is simpler than adam, closer to SGD with some smoothing of integration\n",
        "impact: speeds up convergence somewhat for early iterations and will not NAN from high LR.\n",
        "probable benefit- switch optimizers after model drops. could be good for bigger models.. maybe\n",
        "\n",
        "![image.png](attachment:28374c77-74dc-463c-984c-f518ca74a4cd.png)\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVC0p-PKN9n5"
      },
      "source": [
        "![image.png](attachment:5ac230c1-0ce7-4d16-9fb8-30b9fa5e6708.png)![image.png](attachment:e58a7ccb-960e-4ba6-907c-fbc6e42d1692.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jcJTMiWT89P5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "002d07d0-3a3c-4667-e85c-88b4e18c4dde"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-924c3a02d53e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   6175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6177\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregister_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6178\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mout_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6179\u001b[0m @elementwise_type_promotion_wrapper(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mdecomposition_decorator\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map_\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# consume and exhaust the iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m_add_op_to_registry\u001b[0;34m(registry, op, fn)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpOverloadPacket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0moverloads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mop_overload\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverloads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0muse_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"default\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0;31m# TODO: disallow access to overloads registered by JIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m             op_dk_tags = torch._C._get_operation_overload(\n\u001b[0m\u001b[1;32m   1077\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qualified_op_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl9uYIM16MG4"
      },
      "outputs": [],
      "source": [
        "class Wolf(Optimizer):\n",
        "    \"\"\"Implements Wolf algorithm.\"\"\"\n",
        "    def __init__(self, params, lr=0.25, betas=(0.9, 0.999), eps=1e-12):\n",
        "        # Define default parameters\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
        "        self.lr = lr\n",
        "        # Initialize the parent Optimizer class first\n",
        "        super().__init__(params, defaults)\n",
        "        # Constants specific to Wolf\n",
        "        # Initialize state for each parameter\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                state['p'] = torch.zeros_like(p)  # Second moment estimate\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step and adjusts dropout in transformer blocks.\"\"\"\n",
        "        etcerta = 0.367879441  # Constant used in update rule\n",
        "        et = 1 - etcerta\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        # Iterate over parameter groups.\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                exp_avg = state['p']\n",
        "\n",
        "                # Compute update and update second moment-like state.\n",
        "                update = exp_avg * et + grad * etcerta\n",
        "                state['p'] = exp_avg * et + update * etcerta\n",
        "\n",
        "                # Compute sign agreement between update and gradient.\n",
        "                sign_agreement = torch.sign(update) * torch.sign(grad)\n",
        "\n",
        "                # Where the signs agree (mask is True), update the parameter.\n",
        "                mask = (sign_agreement > 0)\n",
        "                adaptive_alpha = group.get('lr', self.lr)\n",
        "                p.data = torch.where(mask, p.data - adaptive_alpha * update, p.data)\n",
        "\n",
        "                # AMP Compatibility: Ensure a step counter is updated\n",
        "                state['step'] = state.get('step', 0) + 1  # Track optimization steps\n",
        "\n",
        "        return loss\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Custom Activation\n",
        "# ---------------------------------------------------\n",
        "class ReferenceActivation(nn.Module):\n",
        "    def __init__(self, gamma=24):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Step 2: Process (your transformation)\n",
        "        log_x = torch.sign(x) * torch.log1p(torch.abs(x))\n",
        "        processed = log_x / torch.sqrt(1 + 24 * log_x ** 2)\n",
        "\n",
        "        return processed\n",
        "\n",
        "class CachedMultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0, batch_first=True):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # We use the built-in multihead attention module.\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=batch_first)\n",
        "\n",
        "    def forward(self, query, key, value, past_kv=None):\n",
        "        \"\"\"\n",
        "        query: (B, S_new, D)\n",
        "        key, value: (B, S_current, D) for the current input tokens.\n",
        "        past_kv: Tuple (past_key, past_value) or None.\n",
        "        \"\"\"\n",
        "        if not self.training:\n",
        "            if past_kv is not None:\n",
        "                past_key, past_value = past_kv\n",
        "                # Concatenate along the sequence dimension\n",
        "                key = torch.cat([past_key, key], dim=1)\n",
        "                value = torch.cat([past_value, value], dim=1)\n",
        "            # Run the attention module.\n",
        "            attn_output, _ = self.attn(query, key, value)\n",
        "            # The new cache holds all keys and values computed so far.\n",
        "            new_kv = (key, value)\n",
        "            return attn_output, new_kv\n",
        "        else:\n",
        "            attn_output,attn_weights = self.attn(query, key, value)\n",
        "            attn_weights_1 = attn_weights.sum(dim=-1)\n",
        "            attn_weights_2 = attn_weights.sum(dim=-2)\n",
        "            attn_weights = (attn_weights_1 + attn_weights_2)/2\n",
        "\n",
        "\n",
        "            return attn_output, attn_weights\n",
        "\n",
        "class RectifiedKAN(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor=8, dropout=0.0):\n",
        "        super().__init__()\n",
        "        hidden_dim = expansion_factor * embed_dim\n",
        "\n",
        "        self.expand = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.activation = ReferenceActivation()\n",
        "        self.linear = nn.Linear(hidden_dim, embed_dim,bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Now the first gemm already includes the shift\n",
        "        x = self.expand(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class TapeHeadBlock(nn.Module):\n",
        "    def __init__(self, seq_len, embed_dim, vocab_size, num_heads=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Sinusoidal positional embeddings (Precomputed)\n",
        "        self.register_buffer(\"pos_emb\", self._build_sinusoidal_embedding(seq_len, embed_dim))\n",
        "\n",
        "        # Rotary embedding setup\n",
        "        self.use_rope = True  # Set to False to disable RoPE\n",
        "        if self.use_rope:\n",
        "            self.register_buffer(\"rope_freqs\", self._build_rope_frequencies(embed_dim))\n",
        "\n",
        "        # Attention layers\n",
        "        self.cached_attn = CachedMultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.ln_attn = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # MLP and normalization\n",
        "        self.mlp = RectifiedKAN(embed_dim)\n",
        "        self.ln_mlp = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Unembedding matrix\n",
        "\n",
        "        # Logits cross-attention\n",
        "\n",
        "    def _build_sinusoidal_embedding(self, seq_len, embed_dim):\n",
        "        \"\"\"Compute sinusoidal positional embeddings\"\"\"\n",
        "        position = torch.arange(seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
        "        pos_emb = torch.zeros(seq_len, embed_dim)\n",
        "        pos_emb[:, 0::2] = torch.sin(position * div_term)\n",
        "        pos_emb[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pos_emb.unsqueeze(0)  # Shape: (1, seq_len, embed_dim)\n",
        "\n",
        "\n",
        "    def _build_rope_frequencies(self, embed_dim):\n",
        "        \"\"\"Build the inverse frequency tensor for RoPE and ensure it is a PyTorch tensor.\"\"\"\n",
        "        half_dim = embed_dim // 2  # For D=256, half_dim = 128\n",
        "        base_freqs = 1.0 / (10000 ** (torch.arange(0, half_dim, dtype=torch.float32) / half_dim))\n",
        "        # Remove the repeat_interleave so we keep shape (half_dim,)\n",
        "        return base_freqs\n",
        "\n",
        "\n",
        "    def apply_rope(self,tensor, rope_freqs):\n",
        "            \"\"\"\n",
        "            Apply Rotary Positional Embedding (RoPE) to the input tensor.\n",
        "\n",
        "            Args:\n",
        "                tensor (Tensor): Input tensor of shape (B, S, D), where\n",
        "                                 B = batch size, S = sequence length, D = embedding dim.\n",
        "                rope_freqs (Tensor): Frequency tensor of shape (D/2,) computed via _build_rope_frequencies.\n",
        "\n",
        "            Returns:\n",
        "                Tensor: The input tensor after applying RoPE, with the same shape (B, S, D).\n",
        "\n",
        "            Detailed Steps & Dimensions:\n",
        "              1. Let B, S, D = tensor.shape and half_dim = D//2.\n",
        "              2. Compute positions: a tensor of shape (S,).\n",
        "              3. Compute theta = positions.unsqueeze(1) * rope_freqs.unsqueeze(0)\n",
        "                 -> theta has shape (S, half_dim).\n",
        "              4. Compute sin_theta and cos_theta from theta, each of shape (S, half_dim),\n",
        "                 then expand to (B, S, half_dim).\n",
        "              5. Split tensor into two halves along the last dimension:\n",
        "                 - x1 = tensor[..., 0::2] and x2 = tensor[..., 1::2], each with shape (B, S, half_dim).\n",
        "                 (Alternatively, one can do: x1, x2 = torch.chunk(tensor, 2, dim=-1))\n",
        "              6. Apply RoPE:\n",
        "                 - x1_rot = x1 * cos_theta - x2 * sin_theta\n",
        "                 - x2_rot = x1 * sin_theta + x2 * cos_theta\n",
        "              7. Reassemble the output by interleaving x1_rot and x2_rot.\n",
        "            \"\"\"\n",
        "            B, S, D = tensor.shape\n",
        "            if S == 1:\n",
        "                return tensor\n",
        "            assert D % 2 == 0, \"Embedding dimension must be even for RoPE.\"\n",
        "            half_dim = D // 2  # e.g. for D=256, half_dim = 128\n",
        "\n",
        "            # Ensure rope_freqs is on the same device and dtype as tensor.\n",
        "            rope_freqs = rope_freqs.to(tensor.dtype)  # shape: (half_dim,)\n",
        "\n",
        "            # 1. Compute positions (0, 1, ..., S-1): shape (S,)\n",
        "            positions = torch.arange(S, device=tensor.device, dtype=tensor.dtype)\n",
        "\n",
        "            # 2. Compute theta = positions * rope_freqs:\n",
        "            #    positions: (S, 1), rope_freqs: (1, half_dim) --> theta: (S, half_dim)\n",
        "            theta = positions.unsqueeze(1) * rope_freqs.unsqueeze(0)  # shape: (S, half_dim)\n",
        "\n",
        "            # 3. Compute sin and cos of theta:\n",
        "            sin_theta = theta.sin()  # shape: (S, half_dim)\n",
        "            cos_theta = theta.cos()  # shape: (S, half_dim)\n",
        "\n",
        "            # 4. Expand sin and cos to shape (B, S, half_dim)\n",
        "            sin_theta = sin_theta.unsqueeze(0).expand(B, S, half_dim)\n",
        "            cos_theta = cos_theta.unsqueeze(0).expand(B, S, half_dim)\n",
        "\n",
        "            # 5. Split tensor into two halves (real and imaginary parts).\n",
        "            # Here we use alternate slicing: the even-indexed dims are x1, odd-indexed are x2.\n",
        "            x1 = tensor[..., 0::2]  # shape: (B, S, half_dim)\n",
        "            x2 = tensor[..., 1::2]  # shape: (B, S, half_dim)\n",
        "\n",
        "            # 6. Apply the RoPE rotation:\n",
        "            x1_rot = x1 * cos_theta - x2 * sin_theta  # shape: (B, S, half_dim)\n",
        "            x2_rot = x1 * sin_theta + x2 * cos_theta  # shape: (B, S, half_dim)\n",
        "\n",
        "            # 7. Interleave x1_rot and x2_rot back together.\n",
        "            # One approach is to create an empty tensor and then fill in even and odd indices.\n",
        "            out = torch.empty_like(tensor)\n",
        "            out[..., 0::2] = x1_rot\n",
        "            out[..., 1::2] = x2_rot\n",
        "\n",
        "            return out\n",
        "\n",
        "\n",
        "    def forward(self, token_emb, prev_h, prev_emb, past_kv=None):\n",
        "        \"\"\"\n",
        "        x: (B, S) input token IDs\n",
        "        prev_h: (B, S, D) previous hidden state\n",
        "        prev_emb: (B, S, D) previous embeddings\n",
        "        logits: (B, S, V) logits distribution from previous block (can be None)\n",
        "        past_kv: Dictionary with keys 'chunk_attn' and 'logits_attn' holding KV caches\n",
        "        \"\"\"\n",
        "\n",
        "        past_chunk = past_kv.get('chunk_attn') if past_kv is not None else None\n",
        "        # Compute token embeddings and add sinusoidal positional embeddings\n",
        "        seq_len = token_emb.shape[1]  # Dynamically get sequence length\n",
        "        layer_emb = token_emb + self.pos_emb[:, :seq_len, :]\n",
        "\n",
        "        # Apply RoPE if enabled\n",
        "        if self.use_rope:\n",
        "            layer_emb = self.apply_rope(layer_emb, self.rope_freqs)\n",
        "\n",
        "        # Attention input\n",
        "        if prev_h is not None:\n",
        "            attn_input = torch.cat([prev_emb, layer_emb, prev_h[:, -seq_len:, :]], dim=1)\n",
        "            attn_input = attn_input[:, -seq_len:, :]\n",
        "\n",
        "        else:\n",
        "            attn_input = layer_emb\n",
        "\n",
        "\n",
        "        # Compute Self-Attention with KV Caching\n",
        "        if not self.training:\n",
        "            if past_chunk is not None:\n",
        "                attn_out, new_chunk_cache  = self.cached_attn(\n",
        "                    attn_input, attn_input, attn_input, past_kv=past_chunk\n",
        "                )\n",
        "            else:\n",
        "                attn_out, new_chunk_cache = self.cached_attn(attn_input, attn_input, attn_input)\n",
        "        else:\n",
        "            attn_out, attn_weights = self.cached_attn(attn_input, attn_input, attn_input)\n",
        "\n",
        "        h_attn = self.ln_attn(attn_input + attn_out)\n",
        "\n",
        "        # Pass through MLP\n",
        "        h_mlp = self.ln_mlp(h_attn + self.mlp(h_attn))\n",
        "\n",
        "\n",
        "        # Compute final logits\n",
        "        # Return KV cache\n",
        "        if not self.training:\n",
        "            new_cache = {'chunk_attn': new_chunk_cache}\n",
        "            return h_mlp, layer_emb, new_cache\n",
        "\n",
        "        if self.training:\n",
        "            attn_weights = torch.softmax(attn_weights, dim=-1)\n",
        "            return h_mlp, layer_emb, attn_weights\n",
        "\n",
        "class CharRNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "        self.activation = ReferenceActivation()\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        h_candidate = self.i2h(x) + self.h2h(h_prev)\n",
        "        h_new = self.activation(h_candidate)\n",
        "        return h_new\n",
        "\n",
        "class CharRNNModel(nn.Module):\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim=128, hidden_size=256, num_layers=2, placeholder_idx=None):\n",
        "        super().__init__()\n",
        "        self.activation = ReferenceActivation()\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.placeholder_idx = placeholder_idx\n",
        "\n",
        "       # self.cells_right = nn.ModuleList([\n",
        "        #    CharRNNCell(embed_dim if i == 0 else hidden_size, hidden_size) for i in range(num_layers)\n",
        "        #])\n",
        "        self.cells_left = nn.ModuleList([\n",
        "            CharRNNCell(embed_dim if i == 0 else hidden_size, hidden_size) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, embedded_x, past_kv=None):\n",
        "        batch_size = embedded_x.shape[0]\n",
        "\n",
        "        # ✅ Ensure hidden_state has a consistent shape\n",
        "        if not self.training and past_kv is not None:\n",
        "            # Evaluation mode: read from past_kv (expecting 3D shape)\n",
        "            hidden_state = [kv[:, -1, :] for kv in past_kv]  # Always valid if kv has 3 dims\n",
        "        else:\n",
        "            # Training mode or no past_kv: initialize hidden state\n",
        "            hidden_state = [torch.zeros(batch_size, self.hidden_size, device=embedded_x.device)\n",
        "                            for _ in range(self.num_layers)]\n",
        "\n",
        "        outputs = []\n",
        "        new_hidden_state = []\n",
        "\n",
        "        for t in range(embedded_x.shape[1]):\n",
        "            input_t = embedded_x[:, t, :]\n",
        "            new_hidden = []\n",
        "\n",
        "            for idx in range(len(self.cells_left)):\n",
        "                cell_left = self.cells_left[idx]\n",
        "                #cell_right = self.cells_right[idx]\n",
        "                h_prev = hidden_state[idx]  # Assuming hidden_state has same length\n",
        "\n",
        "                h_new = cell_left(input_t, h_prev)\n",
        "               # h_new_right = cell_right(input_t, h_prev)\n",
        "                # Combine left and right outputs\n",
        "                #a = self.activation(h_new_left)\n",
        "                #b = self.activation(h_new_right)\n",
        "                #h_new = 0.5 * (a + b - 2 * a * b)\n",
        "\n",
        "                new_hidden.append(h_new)\n",
        "                input_t = h_new\n",
        "\n",
        "\n",
        "            hidden_state = new_hidden\n",
        "            outputs.append(input_t)\n",
        "\n",
        "            # ✅ Consistently shape cached hidden states as (batch_size, 1, hidden_size)\n",
        "            new_hidden_state = [h.unsqueeze(1) for h in hidden_state]  # No if-check needed\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # ✅ In training, return None for cache. In eval, return consistent 3D cache.\n",
        "        return outputs, new_hidden_state if not self.training else None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TapeHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-like block with progressive chunk sizes.\n",
        "    Each layer inside the TapeHead doubles the chunk size.\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, embed_dim, vocab_size, num_layers=3, dropout=0.1,discriminate=0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.discriminate=discriminate\n",
        "        layer_weights = torch.linspace(0.3, 1.0, steps=self.num_layers)\n",
        "        layer_weights /= layer_weights.sum()  # Normalize\n",
        "        self.register_buffer(\"layer_weights\", layer_weights)\n",
        "        # Create progressively larger TapeHeadBlocks for the left and right streams.\n",
        "\n",
        "        #self.blocks_left= nn.ModuleList([\n",
        "         #   TapeHeadBlock(\n",
        "          #      seq_len=seq_len,\n",
        "           #     embed_dim=embed_dim,\n",
        "            #    vocab_size=vocab_size,\n",
        "             #   num_heads=4,  #true for up to 512 embed dem\n",
        "           #     dropout=dropout\n",
        "           # )\n",
        "           # for i in range(num_layers)\n",
        "        #])\n",
        "\n",
        "        self.blocks_right = nn.ModuleList([\n",
        "            TapeHeadBlock(\n",
        "                seq_len=seq_len,\n",
        "                embed_dim=embed_dim,\n",
        "                vocab_size=vocab_size,\n",
        "                num_heads=4,  # Inversely scale heads\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, token_emb, prev_h, prev_emb, past_kv=None):\n",
        "        \"\"\"\n",
        "        past_kv: dictionary with keys 'left' and 'right', each is a list of caches (one per block).\n",
        "        \"\"\"\n",
        "        attn_weights = []\n",
        "\n",
        "        if not self.training:\n",
        "           # if past_kv is None:\n",
        "            #    past_kv = {'left': [None] * self.num_layers, 'right': [None] * self.num_layers}\n",
        "           # prev_emb_left = prev_emb.clone() if prev_emb is not None else None\n",
        "           # prev_h_left = prev_h.clone() if prev_h is not None else None\n",
        "\n",
        "            #new_past_left = []\n",
        "            new_past_right = []\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                # Process left block with its cache.\n",
        "                 #   h_out_left, prev_emb_left, cache_left = self.blocks_left[i](\n",
        "                  #       token_emb,prev_h_left, prev_emb_left, past_kv=past_kv['left'][i]\n",
        "                  #  )\n",
        "                                    # Process right block with its cache.\n",
        "                    h_out,  prev_emb, cache_right = self.blocks_right[i](\n",
        "                        token_emb,prev_h, prev_emb, past_kv=past_kv['right'][i])\n",
        "\n",
        "                    #if self.discriminate==0:\n",
        "\n",
        "                     #   a = self.activation(h_out_left)\n",
        "                     #   b = self.activation(h_out)\n",
        "                     #   h_out = (a + b) * 0.5 - a * b\n",
        "                     #   prev_h_left = h_out\n",
        "                     #   prev_h = h_out\n",
        "                   # elif self.discriminate==1:\n",
        "                    #    combined = torch.stack([h_out_left, h_out], dim=-1)  # Shape: (B, S, D, 2)\n",
        "                     #   weights = torch.softmax(combined, dim=-1)  # Normalize contributions\n",
        "                      #  h_out = (combined * weights).sum(dim=-1)  # Shape: (B, S, D)\n",
        "                       # prev_h_left = h_out\n",
        "                     #   prev_h = h_out\n",
        "\n",
        "#                    else:\n",
        " #                       h_out= h_out_left+h_out\n",
        "  #                      prev_h_left = h_out\n",
        "   #                     prev_h = h_out\n",
        "\n",
        "    #                new_past_left.append(cache_left)  # Store all layers' caches\n",
        "                    new_past_right.append(cache_right)  # Store all layers' caches\n",
        "\n",
        "           # prev_emb = 0.5 * (prev_emb + prev_emb_left)\n",
        "            #new_cache = {'left': new_past_left, 'right': new_past_right}\n",
        "            new_cache = {'right': new_past_right}\n",
        "            return h_out, prev_emb, new_cache  # ✅ Properly returning all layer caches\n",
        "\n",
        "        else:  # Training mode\n",
        "            #prev_emb_left = prev_emb.clone() if prev_emb is not None else None\n",
        "            #prev_h_left = prev_h.clone() if prev_h is not None else None\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                # Process left block\n",
        "             #       h_out_left, prev_emb_left, attn_weights_left = self.blocks_left[i](\n",
        "              #          token_emb, prev_h_left, prev_emb_left,\n",
        "               #     )\n",
        "\n",
        "                    # Process right block\n",
        "                    h_out, prev_emb, attn_weights_right = self.blocks_right[i](\n",
        "                        token_emb, prev_h, prev_emb\n",
        "                    )\n",
        "          #          if self.discriminate==0:\n",
        "           #\n",
        "            #            a = self.activation(h_out_left)\n",
        "             #           b = self.activation(h_out)\n",
        "              #          h_out = (a + b) * 0.5 - a * b\n",
        "               #         prev_h_left = h_out\n",
        "                #        prev_h = h_out\n",
        "                 #   elif self.discriminate==1:\n",
        "                  #      combined = torch.stack([h_out_left, h_out], dim=-1)  # Shape: (B, S, D, 2)\n",
        "                   #     weights = torch.softmax(combined, dim=-1)  # Normalize contributions\n",
        "                    #    h_out = (combined * weights).sum(dim=-1)  # Shape: (B, S, D)\n",
        "                     #   prev_h_left = h_out\n",
        "                      #  prev_h = h_out\n",
        "\n",
        "              #      else:\n",
        "               #         h_out= h_out_left+h_out\n",
        "                #        prev_h_left = h_out\n",
        "                 #       prev_h = h_out\n",
        "\n",
        "                    #attn_weights.append((attn_weights_left + attn_weights_right) / 2.0)\n",
        "                    attn_weights.append(attn_weights_right)\n",
        "\n",
        "            #prev_emb = 0.5 * (prev_emb + prev_emb_left)\n",
        "\n",
        "            attn_weights = torch.stack(attn_weights, dim=0)\n",
        "            attn_weights *= self.layer_weights.view(-1, 1, 1)\n",
        "            attn_weights = attn_weights.sum(dim=0)\n",
        "\n",
        "            return  h_out, prev_emb, attn_weights  # ✅ Returns all caches correctly\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-12):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / (x.norm(2, dim=-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Merger(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        # linear that goes from 2*V -> out_dim\n",
        "        self.proj = nn.Linear(in_dim*2, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, heads_2d):\n",
        "        \"\"\"\n",
        "        heads_2d: shape (2, B, S, V)\n",
        "        We'll reshape it to (B, S, 2*V), pass it into self.proj, and place it back into heads_2d[1].\n",
        "        \"\"\"\n",
        "        # heads_2d[0] = sub-head #1 (B, S, V)\n",
        "        # heads_2d[1] = sub-head #2 (B, S, V)\n",
        "        # 1) reshape to (B, S, 2*V):\n",
        "        B, S, V = heads_2d.shape[1], heads_2d.shape[2], heads_2d.shape[3]\n",
        "        merged_view = heads_2d.view(-1, S, 2*V)  # shape (B, S, 2*V)\n",
        "\n",
        "        # 2) run through the linear\n",
        "        #    result will be shape (B, S, out_dim)\n",
        "        merged_out = self.proj(merged_view)  # (B, S, out_dim)\n",
        "\n",
        "        # 3) Optionally, store back in heads_2d[1] or somewhere else.\n",
        "        #    If you want to in-place the second head, make sure you keep track of shapes.\n",
        "        #    Here, let's just re-view merged_out as (B, S, out_dim) => (1, B, S, out_dim)\n",
        "        #    and store it in heads_2d[1].\n",
        "        out_dim = merged_out.shape[-1]\n",
        "        # ensure we can place it into heads_2d[1] shape (B, S, V) if out_dim == V\n",
        "        # or allocate a new dimension for it, depending on design.\n",
        "\n",
        "        if out_dim == V:\n",
        "            # then we can store in heads_2d[1] in place\n",
        "            heads_2d[1].copy_(merged_out)\n",
        "            return heads_2d[1]  # shape (B, S, V)\n",
        "        else:\n",
        "            # If out_dim != V, you'd store it in a separate buffer or return merged_out directly\n",
        "            return merged_out  # shape (B, S, out_dim)\n",
        "\n",
        "\n",
        "\n",
        "class TapeTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Full GPT-like model with:\n",
        "      - Token + Position Embeddings\n",
        "      - Multiple stacked TapeHeads\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, seq_len=128, num_layers=4, embed_dim=128, num_heads=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.holistic_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.prophet = CharRNNModel(seq_len,vocab_size,embed_dim = embed_dim, hidden_size=embed_dim*2)\n",
        "        self.rnn_projection = nn.Linear(embed_dim*2, embed_dim)\n",
        "\n",
        "\n",
        "        self.codahead =  TapeHead(\n",
        "                seq_len=seq_len,\n",
        "                embed_dim=embed_dim,\n",
        "                vocab_size=vocab_size,\n",
        "                num_layers=2,\n",
        "                dropout=dropout,\n",
        "                discriminate=0\n",
        "            )\n",
        "        # Create a list of TapeHeads.\n",
        "        self.residualheads = nn.ModuleList([\n",
        "            TapeHead(\n",
        "                seq_len=seq_len,\n",
        "                embed_dim=embed_dim,\n",
        "                vocab_size=vocab_size,\n",
        "                num_layers=4,\n",
        "                dropout=dropout,\n",
        "                discriminate=1\n",
        "\n",
        "            )\n",
        "            for _ in range(num_heads)\n",
        "        ])\n",
        "        self.merger = Merger(in_dim=embed_dim, out_dim=embed_dim)\n",
        "        self.final_norm = RMSNorm(embed_dim)\n",
        "        self.norm_gate = nn.Parameter(torch.tensor(0.5))  # for potential supervisory context\n",
        "        self.activation = ReferenceActivation()\n",
        "        # Final unembedding.\n",
        "        self.unembedding = nn.Parameter(torch.randn(embed_dim, vocab_size))\n",
        "        nn.init.kaiming_uniform_(self.unembedding, a=math.sqrt(5))\n",
        "        self.merge_projection = nn.Linear(2 * embed_dim, embed_dim, bias=False)\n",
        "        self.merge_projection_final = nn.Linear(len(self.residualheads) * embed_dim, embed_dim, bias=False)\n",
        "\n",
        "    def forward(self, x, past_kv=None):\n",
        "        \"\"\"\n",
        "        x: (B, S) integer token IDs.\n",
        "        past_kv: list of caches (one per tape head) or None.\n",
        "        Returns:\n",
        "          p_final: (B, S, V) probability distribution,\n",
        "          new_past_kv: updated caches.\n",
        "        \"\"\"\n",
        "        # Ensure batch dimension.\n",
        "        x = x.unsqueeze(0) if x.ndim == 1 else x\n",
        "        B, S = x.shape\n",
        "        assert S <= self.seq_len, \"Sequence too long.\"\n",
        "\n",
        "        prev_h = None\n",
        "        prev_emb = None\n",
        "        new_past_heads = None\n",
        "        attn_weights = None\n",
        "\n",
        "        if not self.training:\n",
        "            new_past_heads = []\n",
        "            if past_kv is None:\n",
        "                past_kv = (\n",
        "                    [{'left': [None] * self.codahead.num_layers, 'right': [None] * self.codahead.num_layers}] +\n",
        "                    [[torch.zeros(batch_size, 1, self.prophet.hidden_size, device=x.device)  # ✅ Added time dim\n",
        "                      for _ in range(self.prophet.num_layers)]] +\n",
        "                    [{'left': [None] * head.num_layers, 'right': [None] * head.num_layers}\n",
        "                     for head in self.residualheads]\n",
        "                )\n",
        "\n",
        "\n",
        "            consistency = self.holistic_embed(x)\n",
        "\n",
        "            ##coda\n",
        "            coda_h, prev_emb, head_cache = self.codahead(consistency, prev_h, prev_emb, past_kv=past_kv[0])\n",
        "            new_past_heads.append(head_cache)\n",
        "\n",
        "\n",
        "            prophecy,head_cache = self.prophet.forward(consistency,past_kv[1])\n",
        "            new_past_heads.append(head_cache)\n",
        "\n",
        "\n",
        "            for i, head in enumerate(self.residualheads):\n",
        "                consistency, _, head_cache = head(consistency, coda_h, prev_emb, past_kv=past_kv[2+i])\n",
        "                new_past_heads.append(head_cache)\n",
        "\n",
        "        else:\n",
        "            consistency = self.holistic_embed(x)\n",
        "\n",
        "            coda_h, prev_emb, attn_weights = self.codahead(consistency, prev_h, prev_emb)\n",
        "\n",
        "            prophecy,_= self.prophet.forward(consistency)\n",
        "\n",
        "            prophecy_proj = self.rnn_projection(prophecy)\n",
        "\n",
        "            for i, head in enumerate(self.residualheads):\n",
        "                consistency, _, attn_weights_head = head(consistency, coda_h,prophecy_proj)\n",
        "                attn_weights = attn_weights + attn_weights_head\n",
        "        logits = self.final_norm(consistency)\n",
        "        logits = consistency @ self.unembedding# torch.einsum('bsd,dv->bsv', consistency, self.unembedding)\n",
        "\n",
        "        if not self.training:\n",
        "        # 4) Return raw logits\n",
        "            return logits, new_past_heads\n",
        "        else:\n",
        "            attn_weights /= self.num_heads\n",
        "            return logits, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT8ajLxaN9n7"
      },
      "outputs": [],
      "source": [
        "import gc,torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VhlsGaG7ONr"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Data Preparation (Shakespeare)\n",
        "# ====================================================\n",
        "def load_shakespeare_text():\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    text = requests.get(url).text\n",
        "    return text\n",
        "\n",
        "text = load_shakespeare_text()\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "def get_batch(batch_size, seq_len):\n",
        "    ix = torch.randint(0, data.size(0) - seq_len - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# ====================================================\n",
        "# Training Setup\n",
        "# ====================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TapeTransformer(\n",
        "    vocab_size=vocab_size,  # example\n",
        "    seq_len=128,\n",
        "    num_layers=4,\n",
        "    embed_dim=256,\n",
        "    num_heads=4,\n",
        "    dropout=0 #cannot use dropout, tooo slow\n",
        ")\n",
        "#model = torch.compile(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "eps = 1e-8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqI6NtFJN9n8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "# Training control variables\n",
        "seq_len = 128  # Start with the smallest sequence\n",
        "max_seq_len = 128\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "# Loss tracking\n",
        "epochs_per_check = 10  # Print every 10 epochs\n",
        "target_loss = max(math.log(vocab_size/(seq_len+1)),0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "izR7-xBBN9n8"
      },
      "outputs": [],
      "source": [
        "# Adjusted Code to Improve EWMA Size and Implement Loss Ticker as a Moving Graph\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "\n",
        "# --- Configuration Constants ---\n",
        "CHAR_WIDTH = 8  # Font size 8 for token rendering\n",
        "CHAR_HEIGHT = 11\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "LOSS_BAR_HEIGHT = 32\n",
        "EWMA_HEIGHT = 32  # Increased to accommodate large text (previously 32)\n",
        "\n",
        "# Full-resolution framebuffer dimensions\n",
        "container_width = CHAR_WIDTH * SEQ_LEN  # 1024 pixels\n",
        "container_height = CHAR_HEIGHT * BATCH_SIZE  # 176 pixels\n",
        "total_height = container_height + LOSS_BAR_HEIGHT + EWMA_HEIGHT  # Adjusted for larger EWMA\n",
        "\n",
        "# Final scaled-down dimensions\n",
        "scaled_width = container_width   # 512 pixels\n",
        "scaled_height = total_height  # 170 pixels\n",
        "\n",
        "# Initialize framebuffer\n",
        "framebuffer = np.zeros((total_height, container_width, 3), dtype=np.uint8)\n",
        "\n",
        "# EWMA storage\n",
        "ticker_history = np.zeros(SEQ_LEN, dtype=np.float32)  # Stock ticker moving buffer\n",
        "loss_memory = 0.0\n",
        "# Load font\n",
        "try:\n",
        "    font = ImageFont.truetype(\"DejaVuSansMono.ttf\", 8)  # Monospaced font\n",
        "    font_large = ImageFont.truetype(\"DejaVuSansMono.ttf\", 64)  # Large EWMA display\n",
        "except:\n",
        "    font = ImageFont.load_default()\n",
        "    font_large = font\n",
        "\n",
        "# --- Color Mapping Functions ---\n",
        "def get_flame_color(val):\n",
        "    \"\"\"Map a normalized value to a flame-like color.\"\"\"\n",
        "    return np.array([int(val * 255), int(val * 0.5 * 255), 0], dtype=np.uint8)\n",
        "\n",
        "# --- IPython Display Setup ---\n",
        "out = widgets.Output()\n",
        "display(out)\n",
        "\n",
        "def get_dynamic_color(attn_val, loss_val):\n",
        "    \"\"\"\n",
        "    Compute a dynamic color transition between flame orange (uncertain) and phosphor green (confident).\n",
        "\n",
        "    attn_val: Normalized attention value (0 to 1)\n",
        "    loss_val: Normalized loss value (0 to 1, inverted as 1 - loss)\n",
        "\n",
        "    Returns an RGB color as a NumPy array.\n",
        "    colors late in training will often be red. this is suggested to swap out for get_flame_color\n",
        "    but only on fine tuning on new data.\n",
        "    \"\"\"\n",
        "    certainty = 1 - loss_val  # High certainty = low loss\n",
        "\n",
        "    # Define RGB endpoints\n",
        "    orange = np.array([attn_val * 255, attn_val * 0.5 * 255, 0], dtype=np.uint8)   # Uncertain (High Loss)\n",
        "    green = np.array([attn_val * 0.5 * 255, attn_val * 255, attn_val * 0.25 * 255], dtype=np.uint8)  # Confident (Low Loss)\n",
        "\n",
        "    # Interpolate based on certainty (0 = uncertain/orange, 1 = confident/green)\n",
        "    color = (certainty * green) + ((1 - certainty) * orange)\n",
        "\n",
        "    return color.astype(np.uint8)\n",
        "# --- Framebuffer Update Function ---\n",
        "def update_framebuffer(attn_weights, token_losses, current_loss, tokens,entropy):\n",
        "    attn_weights =(attn_weights-attn_weights.min())/(np.ptp(attn_weights))\n",
        "    token_losses =(token_losses-token_losses.min())/(np.ptp(token_losses))\n",
        "\n",
        "    \"\"\"Render the text grid with coloration based on attn * inverse loss.\"\"\"\n",
        "    global framebuffer, loss_history, ticker_history, loss_memory\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "\n",
        "    # Create image buffer\n",
        "    img = Image.new(\"RGB\", (container_width, total_height), (0, 0, 0))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Render text with colored intensity\n",
        "    char_positions = [\n",
        "        (col * CHAR_WIDTH, row * CHAR_HEIGHT + EWMA_HEIGHT + LOSS_BAR_HEIGHT, tokens[row][col])\n",
        "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
        "    ]\n",
        "    colors = [\n",
        "        tuple(get_dynamic_color(attn_weights[row, col], attn_weights[row, col]))\n",
        "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
        "    ]\n",
        "    for (x, y, char), color in zip(char_positions, colors):\n",
        "        draw.text((x, y), char, font=font, fill=color)\n",
        "\n",
        "\n",
        "    etcerta = 0.367879441  # Constant used in update rule\n",
        "    et = 1 - etcerta\n",
        "    update = loss_memory * et + np.minimum(12, np.maximum(current_loss , 0)) * etcerta\n",
        "    loss_memory = loss_memory * et + update * etcerta\n",
        "    # --- EWMA Display (LARGE FONT) ---\n",
        "    ewma = loss_memory\n",
        "    ewma_text = f\"{ewma:.4f}\"\n",
        "    draw.text((container_width-128, 0), ewma_text, font_size=32, fill=(65,255, 125))\n",
        "    ent_text = f\"{entropy:.4f}\"\n",
        "    draw.text((10, 0), ent_text, font_size=32, fill=(255,125, 0))\n",
        "\n",
        "    # --- Moving Loss Ticker Graph ---\n",
        "    ticker_history = np.roll(ticker_history, -1)  # Shift left\n",
        "    ticker_history[-1] = current_loss  # Insert new loss on the right\n",
        "\n",
        "    # Rescale ticker dynamically like a stock ticker (normalize to min-max range)\n",
        "    min_loss = np.min(ticker_history)\n",
        "    max_loss = np.max(ticker_history)\n",
        "    range_loss = max_loss - min_loss if max_loss != min_loss else 1\n",
        "    normalized_ticker = (ticker_history - min_loss) / range_loss\n",
        "\n",
        "    # Draw ticker graph line\n",
        "    # Optimized drawing loop (fewer function calls)\n",
        "    y_vals = EWMA_HEIGHT + (1 - normalized_ticker) * LOSS_BAR_HEIGHT\n",
        "    x_vals = np.arange(SEQ_LEN) * CHAR_WIDTH\n",
        "    for i in range(SEQ_LEN - 1):\n",
        "        draw.line([(x_vals[i], y_vals[i]), (x_vals[i + 1], y_vals[i + 1])], fill=(0, 255, 255), width=2)\n",
        "\n",
        "    framebuffer = np.array(img)\n",
        "\n",
        "# --- IPython Display Update Function ---\n",
        "def update_display():\n",
        "    \"\"\"Show the framebuffer, scaled down by half using ipywidgets.\"\"\"\n",
        "    img = Image.fromarray(framebuffer)\n",
        "    img_resized = img.resize((scaled_width, scaled_height), Image.LANCZOS)\n",
        "\n",
        "    with out:\n",
        "        clear_output(wait=True)\n",
        "        display(img_resized)\n",
        "\n",
        "loss_history = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "l8nxpCeUN9n8"
      },
      "outputs": [],
      "source": [
        "# Training loop for current sequence length\n",
        "\n",
        "import numpy as np\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "max_entropy = np.log(seq_len)\n",
        "for epoch in range(100000):\n",
        "    model.train()\n",
        "    x_batch, targets = get_batch(batch_size, 128)\n",
        "    x_batch_gpu, targets = x_batch.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.amp.autocast(device.type):\n",
        "        logits, attn_weights = model(x_batch_gpu)\n",
        "        #Flatten so we have (B*S, V) vs. (B*S) labels\n",
        "        per_token_loss = F.cross_entropy(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            targets.view(-1),\n",
        "            reduction='none'  # This gives the raw loss per token\n",
        "        )  # Shape: (B*S,)\n",
        "\n",
        "        attn_weights_entropy = F.softmax(attn_weights, dim=-1)\n",
        "        entropy_batch = -torch.sum(attn_weights_entropy * torch.log(attn_weights_entropy + 1e-12), dim=-1)\n",
        "        entropy = entropy_batch.mean()\n",
        "\n",
        "        per_token_loss = per_token_loss.view(x_batch.size(0), -1)  # (B, S)\n",
        "        loss = per_token_loss.mean()\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    loss_item = loss.item()\n",
        "\n",
        "    loss_cpu = per_token_loss.cpu().detach().numpy()\n",
        "    tokens = [[itos[idx] for idx in seq.tolist()] for seq in targets]\n",
        "    attn_cpu = attn_weights.cpu().detach().numpy()\n",
        "\n",
        "    update_framebuffer(attn_cpu, loss_cpu, loss_item, tokens,entropy.item())\n",
        "    update_display()\n",
        "\n",
        "\n",
        "    # Track loss & progress\n",
        "    loss_val = loss_item\n",
        "    loss_history.append(loss_val)\n",
        "\n",
        "    # Update framebuffer visualization with real model outputs\n",
        "    print(f\"Epoch {epoch}, Loss: {loss_val:.6f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6jKTB42N9n8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq7g-ALcN9n8"
      },
      "outputs": [],
      "source": [
        "\n",
        "logit_list = []  # Store raw logits\n",
        "prob_list = []  # Store softmax probabilities\n",
        "\n",
        "model.eval()\n",
        "past_kv = None  # Initialize cache for the entire model.\n",
        "with torch.no_grad():\n",
        "    prompt = \"Treasured Friends\"\n",
        "    context = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
        "    generated = context\n",
        "    for _ in range(500):  # Generate 200 tokens.\n",
        "        inp = generated[:, -1:]  # Only use the last token.\n",
        "        p, past_kv = model(inp, past_kv=past_kv)  # Forward pass with cache.\n",
        "        last_token_logits = p[:, -1, :].cpu().numpy().flatten()  # Get raw logits\n",
        "        logit_list.append(last_token_logits)\n",
        "\n",
        "        temperature = 1.0 # Lower = more deterministic, Higher = more diverse (0.7–1.0 is good)\n",
        "        probs = F.softmax( p[:, -1, :] / temperature, dim=-1)\n",
        "        prob_list.append(probs)\n",
        "\n",
        "        predicted_token = torch.multinomial(probs.clone().detach(), num_samples=1) # Fix shape\n",
        "        generated = torch.cat((generated, predicted_token.to(device)), dim=1)  # Concatenate properly\n",
        "\n",
        "    sample = decode(generated[0].cpu().tolist())\n",
        "    print(\"Generated Sample:\\n\", sample)\n",
        "\n",
        "# Plot histograms of logits and probabilities\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot logits histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(logit_list[-1], bins=100, alpha=0.75)\n",
        "plt.title(\"Logits Distribution (Before Softmax)\")\n",
        "plt.xlabel(\"Logit Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# Plot probabilities histogram\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(prob_list[-1], bins=100, alpha=0.75)\n",
        "plt.title(\"Probabilities Distribution (After Softmax)\")\n",
        "plt.xlabel(\"Probability Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph-rOp3pN9n9"
      },
      "outputs": [],
      "source": [
        "save_path = 'model_dict.pth'\n",
        "\n",
        "# Save the model's state_dict\n",
        "torch.save(model.state_dict(), save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4rHAG-VN9n9"
      },
      "outputs": [],
      "source": [
        "save_path = 'model_dict.pth'\n",
        "\n",
        "# Save the model's state_dict\n",
        "state_dict = torch.load(save_path)\n",
        "model.load_state_dict(state_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzlEw1tKN9n9"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}