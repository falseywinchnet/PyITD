{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqF2WpA6s36Ug2FQ8YV9ha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/PyITD/blob/main/FusedQKVT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMyAv_7rxOyW",
        "outputId": "fddba3bf-c32b-4a56-b48f-b88d6028efac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Prepare the Shakespeare dataset for character-level language modeling.\n",
        "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
        "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
        "encoder and decoder and some other related info.\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    base_dir = Path(__file__).parent\n",
        "except NameError:\n",
        "    base_dir = Path(os.getcwd())  # fallback if __file__ is not defined (e.g. in REPL)\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(base_dir), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(base_dir), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(base_dir), 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(os.path.join(os.path.dirname(base_dir), 'meta.pkl'), 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#if you use my ideas, please credit me, dont just steal\n",
        "joshuah.rainstar@gmail.com\n"
      ],
      "metadata": {
        "id": "q47er8Ni84xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class RainstarActivation(nn.Module):\n",
        "    def __init__(self, p=0.5, kappa=1.0, gamma=2.0, s_min=0.48):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.kappa = kappa\n",
        "        self.gamma = gamma\n",
        "        self.s_min = s_min\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: any shape\n",
        "        a = x.abs()\n",
        "        # avoid log(0)\n",
        "        t = torch.log(a + 1e-12) + self.p * torch.log(torch.tanh(a) + 1e-12)\n",
        "        base = x.sign() * torch.exp(t)\n",
        "        clamp = self.s_min + (1 - self.s_min) / (1 + (a / self.kappa) ** self.gamma)\n",
        "        return base * clamp\n",
        "\n",
        "\n",
        "class ExpertMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Convex mixture-of-linear-experts MLP with Rainstar gating.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_experts: int = 4,\n",
        "                 p=0.5, kappa=1.0, gamma=2.0, s_min=0.48, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.eps = eps\n",
        "\n",
        "        # Rainstar gate applied to gate logits\n",
        "        self.rainstar = RainstarActivation(p, kappa, gamma, s_min)\n",
        "        # gating network: produces E pre-activations per token\n",
        "        self.gate = nn.Linear(d_model, num_experts)\n",
        "\n",
        "        # expert linear maps\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Linear(d_model, d_model) for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        returns: (B, T, D)\n",
        "        \"\"\"\n",
        "        # 1) compute raw gates and apply Rainstar\n",
        "        #    → shape (B, T, E)\n",
        "        gates_raw = self.rainstar(self.gate(x))\n",
        "\n",
        "        # 2) ensure non-negativity & normalize to convex weights\n",
        "        gates = gates_raw.abs() + self.eps\n",
        "        gates = gates / gates.sum(dim=-1, keepdim=True)  # along expert dim\n",
        "\n",
        "        # 3) compute each expert's linear output: (B, T, D, E)\n",
        "        expert_outs = torch.stack([expert(x) for expert in self.experts],\n",
        "                                  dim=-1)\n",
        "\n",
        "        # 4) convex combination over experts → (B, T, D)\n",
        "        y = (expert_outs * gates.unsqueeze(2)).sum(dim=-1)\n",
        "        return y\n",
        "\n",
        "def trig_modulate(x, pos, phase_index=None, phase_divisions=3):\n",
        "    \"\"\"\n",
        "    Modulate `x` with a phase-shifted cosine, split evenly over 180 degrees.\n",
        "\n",
        "    x: [B, H, T, D]\n",
        "    pos: [T]\n",
        "    kind: standard mode: 'cos', 'sin', etc.\n",
        "    phase_index: if not None, use phase shift as (index * π / phase_divisions)\n",
        "    \"\"\"\n",
        "    B, H, T, D = x.shape\n",
        "    inv_freq = 1.0 / (10000 ** (torch.arange(0, D, dtype=torch.float32) / D)).to(x.device)\n",
        "    θ = torch.outer(pos.float(), inv_freq).to(x.device)  # [T, D]\n",
        "\n",
        "    if phase_index is not None:\n",
        "        φ = (math.pi / phase_divisions) * phase_index  # shift in radians\n",
        "        θ = θ + φ\n",
        "        mod = torch.cos(θ)\n",
        "\n",
        "    mod = mod[None, None, :, :]  # [1, 1, T, D]\n",
        "    return x * mod\n",
        "\n",
        "\n",
        "\n",
        "class TrigQKVAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q_mlp = ExpertMLP(d_model)\n",
        "        self.k_mlp =  ExpertMLP(d_model)\n",
        "        self.v_mlp =  ExpertMLP(d_model)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        H, d = self.n_heads, self.head_dim\n",
        "\n",
        "        # Positional indices\n",
        "        pos = torch.arange(T, device=x.device)\n",
        "\n",
        "        # Generate Q, K, V via distinct MLPs\n",
        "        Q = self.q_mlp(x).view(B, T, H, d).transpose(1, 2)  # [B, H, T, d]\n",
        "        K = self.k_mlp(x).view(B, T, H, d).transpose(1, 2)\n",
        "        V = self.v_mlp(x).view(B, T, H, d).transpose(1, 2)\n",
        "\n",
        "        # Trig-based modulation\n",
        "        Q = trig_modulate(Q, pos, 1)\n",
        "        K = trig_modulate(K, pos, 2)\n",
        "        V = trig_modulate(V, pos, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d)  # [B, H, T, T]\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = (attn @ V)  # [B, H, T, d]\n",
        "\n",
        "        out = out.transpose(1, 2).reshape(B, T, D)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class HybridBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-level processor: attention + MLP with residuals.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.attn = TrigQKVAttention(d_model, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.mlp =  ExpertMLP(d_model)\n",
        "        self.rainstar = RainstarActivation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.norm1(x)\n",
        "        attn_out = self.rainstar(self.attn(h))\n",
        "        x = x + attn_out\n",
        "        h2 = self.norm2(x)\n",
        "        mlp_out = self.mlp(h2)\n",
        "        x = x + self.rainstar(mlp_out)\n",
        "        return x\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Transformer-style model using HybridBlocks as layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=6):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            HybridBlock(d_model=d_model, n_heads=n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.rainstar = RainstarActivation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # If `x` is integer indices → embed; else assume it's already embeddings\n",
        "        if x.dim() == 2:\n",
        "            # x: [B, T] long indices\n",
        "            x = self.token_emb(x)\n",
        "        elif x.dim() == 3:\n",
        "            # x: [B, T, D] continuous embeddings\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Expected 2D idx or 3D embedding, got shape {x.shape}\")\n",
        "\n",
        "        # apply all layers with Rainstar gating\n",
        "        for block in self.blocks:\n",
        "            x = self.rainstar(block(x))\n",
        "        x = self.norm(x)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "MLs7qQUOxRPQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# 1) Load data and meta as before\n",
        "data_dir  = os.path.dirname(base_dir)\n",
        "train_ids = np.fromfile(os.path.join(data_dir, 'train.bin'), dtype=np.uint16)\n",
        "val_ids   = np.fromfile(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16)\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# 2) Compute data‐marginal q[v]\n",
        "counts = np.bincount(train_ids, minlength=vocab_size).astype(float)\n",
        "q = torch.tensor(counts / counts.sum(), dtype=torch.float32, device=device)  # [V]\n",
        "\n",
        "# 3) Dataset + DataLoader\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = torch.from_numpy(data).long()\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx : idx + self.block_size]\n",
        "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "block_size = 128\n",
        "train_loader = DataLoader(CharDataset(train_ids, block_size),\n",
        "                          batch_size=32, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(CharDataset(val_ids,   block_size),\n",
        "                          batch_size=32, shuffle=False, drop_last=True)\n",
        "\n",
        "# 4) Model, optimizer, loss\n",
        "device    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model     = TinyTransformer(vocab_size=vocab_size,\n",
        "                            d_model=64, n_heads=4, n_layers=3).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 5) Regularization weights\n",
        "λ_ent = 0.1   # entropy penalty weight\n",
        "λ_kl  = 0.5   # marginal‐KL penalty weight\n",
        "\n",
        "# 6) Train / eval functions\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # Forward\n",
        "        logits = model(xb)                 # (B, T, V)\n",
        "        B, T, V = logits.shape\n",
        "        p = F.softmax(logits, dim=-1)      # (B, T, V)\n",
        "\n",
        "        # 1) Standard CE\n",
        "        ce_loss = criterion(logits.view(B*T, V),\n",
        "                            yb.view(B*T))\n",
        "\n",
        "        # 2) Entropy penalty\n",
        "        ent = -(p * (p + 1e-12).log()).sum(dim=-1)  # (B, T)\n",
        "        ent_loss = ent.mean()\n",
        "\n",
        "        # 3) Marginal‐KL penalty\n",
        "        p_m = p.mean(dim=(0,1))            # [V]\n",
        "        kl_loss = (p_m * (p_m + 1e-12).log() - p_m * q.log()).sum()\n",
        "\n",
        "        # 4) Combined loss\n",
        "        loss = ce_loss + λ_ent * ent_loss + λ_kl * kl_loss\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item())\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        total_loss += criterion(logits.view(B*T,V),\n",
        "                                yb.view(B*T)).item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# 7) Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = eval_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8WKjMhexTa1",
        "outputId": "bc779df5-4b30-4cd2-92fa-4fd0a473fa1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.561736106872559\n",
            "5.14707612991333\n",
            "4.806702136993408\n",
            "4.521371841430664\n",
            "4.252530097961426\n",
            "3.957996368408203\n",
            "3.8177695274353027\n",
            "3.6656382083892822\n",
            "3.5798213481903076\n",
            "3.5220091342926025\n",
            "3.4580957889556885\n",
            "3.4668848514556885\n",
            "3.3198084831237793\n",
            "3.314347267150879\n",
            "3.308380126953125\n",
            "3.2155938148498535\n",
            "3.2345564365386963\n",
            "3.163290023803711\n",
            "3.1368649005889893\n",
            "3.1584765911102295\n",
            "3.075021266937256\n",
            "3.065476179122925\n",
            "3.0484812259674072\n",
            "3.035484790802002\n",
            "3.0182101726531982\n",
            "2.97692608833313\n",
            "2.907874822616577\n",
            "2.966028928756714\n",
            "2.9087138175964355\n",
            "2.9139814376831055\n",
            "2.9336345195770264\n",
            "2.910454511642456\n",
            "2.859374761581421\n",
            "2.8687925338745117\n",
            "2.8858675956726074\n",
            "2.8819539546966553\n",
            "2.882741689682007\n",
            "2.867968797683716\n",
            "2.8312394618988037\n",
            "2.8095672130584717\n",
            "2.8124420642852783\n",
            "2.8279128074645996\n",
            "2.838529348373413\n",
            "2.817338228225708\n",
            "2.8387370109558105\n",
            "2.8122143745422363\n",
            "2.769068717956543\n",
            "2.8188352584838867\n",
            "2.755561113357544\n",
            "2.790550470352173\n",
            "2.8425467014312744\n",
            "2.8016514778137207\n",
            "2.783837080001831\n",
            "2.767820358276367\n",
            "2.7907211780548096\n",
            "2.7598893642425537\n",
            "2.804464101791382\n",
            "2.77693772315979\n",
            "2.7989096641540527\n",
            "2.779869556427002\n",
            "2.7908284664154053\n",
            "2.7393107414245605\n",
            "2.7834761142730713\n",
            "2.781963348388672\n",
            "2.7482666969299316\n",
            "2.757020950317383\n",
            "2.7368502616882324\n",
            "2.7507543563842773\n",
            "2.782351016998291\n",
            "2.7160191535949707\n",
            "2.750859260559082\n",
            "2.7245893478393555\n",
            "2.7779757976531982\n",
            "2.770822286605835\n",
            "2.714289426803589\n",
            "2.744891881942749\n",
            "2.753012180328369\n",
            "2.7347378730773926\n",
            "2.725109100341797\n",
            "2.7411375045776367\n",
            "2.712087631225586\n",
            "2.6999266147613525\n",
            "2.7140846252441406\n",
            "2.6967556476593018\n",
            "2.7505557537078857\n",
            "2.7338688373565674\n",
            "2.721114158630371\n",
            "2.7324435710906982\n",
            "2.7357330322265625\n",
            "2.6848537921905518\n",
            "2.684034585952759\n",
            "2.707169771194458\n",
            "2.7221081256866455\n",
            "2.710811138153076\n",
            "2.6536238193511963\n",
            "2.686340093612671\n",
            "2.7119221687316895\n",
            "2.7094571590423584\n",
            "2.710190773010254\n",
            "2.698925733566284\n",
            "2.7381343841552734\n",
            "2.7177000045776367\n",
            "2.6929173469543457\n",
            "2.6971001625061035\n",
            "2.7141687870025635\n",
            "2.6858177185058594\n",
            "2.670180559158325\n",
            "2.680626630783081\n",
            "2.684969425201416\n",
            "2.7225778102874756\n",
            "2.714931011199951\n",
            "2.7012147903442383\n",
            "2.701819658279419\n",
            "2.7069039344787598\n",
            "2.7124712467193604\n",
            "2.6994380950927734\n",
            "2.6656274795532227\n",
            "2.7209949493408203\n",
            "2.680604934692383\n",
            "2.683708429336548\n",
            "2.6894726753234863\n",
            "2.6829233169555664\n",
            "2.683805227279663\n",
            "2.6827332973480225\n",
            "2.6938111782073975\n",
            "2.660320281982422\n",
            "2.6294121742248535\n",
            "2.681405544281006\n",
            "2.6787378787994385\n",
            "2.68290638923645\n",
            "2.6490345001220703\n",
            "2.62530255317688\n",
            "2.673050880432129\n",
            "2.6670234203338623\n",
            "2.6652441024780273\n",
            "2.629873752593994\n",
            "2.694765567779541\n",
            "2.643319606781006\n",
            "2.666198968887329\n",
            "2.607548236846924\n",
            "2.654171943664551\n",
            "2.6456074714660645\n",
            "2.658583641052246\n",
            "2.6503989696502686\n",
            "2.6511266231536865\n",
            "2.68000864982605\n",
            "2.6282427310943604\n",
            "2.647897243499756\n",
            "2.6379144191741943\n",
            "2.6470284461975098\n",
            "2.6622025966644287\n",
            "2.6074118614196777\n",
            "2.637248992919922\n",
            "2.6245250701904297\n",
            "2.628675937652588\n",
            "2.646165609359741\n",
            "2.6510632038116455\n",
            "2.571991443634033\n",
            "2.660273790359497\n",
            "2.6112022399902344\n",
            "2.625051259994507\n",
            "2.6477913856506348\n",
            "2.61397123336792\n",
            "2.6723203659057617\n",
            "2.5848007202148438\n",
            "2.6692583560943604\n",
            "2.6446373462677\n",
            "2.6288464069366455\n",
            "2.683140754699707\n",
            "2.5946123600006104\n",
            "2.6099987030029297\n",
            "2.590423345565796\n",
            "2.6322579383850098\n",
            "2.6220626831054688\n",
            "2.617616653442383\n",
            "2.618342161178589\n",
            "2.6400907039642334\n",
            "2.616795063018799\n",
            "2.5832483768463135\n",
            "2.6147522926330566\n",
            "2.6190173625946045\n",
            "2.602151870727539\n",
            "2.601867198944092\n",
            "2.6166515350341797\n",
            "2.6258718967437744\n",
            "2.6002376079559326\n",
            "2.591341495513916\n",
            "2.646193504333496\n",
            "2.57930850982666\n",
            "2.6032207012176514\n",
            "2.5873425006866455\n",
            "2.621532440185547\n",
            "2.5982413291931152\n",
            "2.596196413040161\n",
            "2.630180835723877\n",
            "2.5755133628845215\n",
            "2.6085636615753174\n",
            "2.5775649547576904\n",
            "2.5990335941314697\n",
            "2.5327117443084717\n",
            "2.538327693939209\n",
            "2.5286946296691895\n",
            "2.5701236724853516\n",
            "2.5403857231140137\n",
            "2.570322036743164\n",
            "2.583664894104004\n",
            "2.6014797687530518\n",
            "2.5611069202423096\n",
            "2.599843978881836\n",
            "2.523895740509033\n",
            "2.5404598712921143\n",
            "2.537602186203003\n",
            "2.549043655395508\n",
            "2.5172324180603027\n",
            "2.5596470832824707\n",
            "2.5498037338256836\n",
            "2.567129611968994\n",
            "2.537247657775879\n",
            "2.549778699874878\n",
            "2.559957265853882\n",
            "2.5132644176483154\n",
            "2.5577969551086426\n",
            "2.516173839569092\n",
            "2.5498950481414795\n",
            "2.522106170654297\n",
            "2.5287485122680664\n",
            "2.5268192291259766\n",
            "2.5153236389160156\n",
            "2.5200116634368896\n",
            "2.555767297744751\n",
            "2.5297319889068604\n",
            "2.519836664199829\n",
            "2.4634571075439453\n",
            "2.4905054569244385\n",
            "2.488978147506714\n",
            "2.49017071723938\n",
            "2.4630825519561768\n",
            "2.52236270904541\n",
            "2.5134687423706055\n",
            "2.499739646911621\n",
            "2.5136539936065674\n",
            "2.444007396697998\n",
            "2.4517436027526855\n",
            "2.4397332668304443\n",
            "2.449069023132324\n",
            "2.4465291500091553\n",
            "2.4377567768096924\n",
            "2.4211974143981934\n",
            "2.4772441387176514\n",
            "2.463132381439209\n",
            "2.421898603439331\n",
            "2.451659917831421\n",
            "2.396596670150757\n",
            "2.414564609527588\n",
            "2.436993360519409\n",
            "2.4030344486236572\n",
            "2.4283177852630615\n",
            "2.4337809085845947\n",
            "2.3880040645599365\n",
            "2.3910624980926514\n",
            "2.4134328365325928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bcontext_str = \"To be, or not to be,\"\n",
        "context_ids  = torch.tensor([[ stoi[c] for c in bcontext_str ]], dtype=torch.long)\n",
        "\n",
        "max_new_tokens = 5000\n",
        "temperature     = 1.0\n",
        "top_k           = 50\n",
        "block_size      = 128\n",
        "\n",
        "generated = context_ids  # (1, T)\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    # only pass at most block_size tokens into the model\n",
        "    input_ids = generated if generated.size(1) <= block_size \\\n",
        "                else generated[:, -block_size:]\n",
        "\n",
        "    logits = model(input_ids)              # (1, T_cur, vocab_size)\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    if top_k is not None:\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        logits[logits < v[:, [-1]]] = -1e10\n",
        "\n",
        "    probs   = F.softmax(logits, dim=-1)\n",
        "    next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
        "    generated = torch.cat([generated, next_id], dim=1)\n",
        "\n",
        "output_str = ''.join([itos[i] for i in generated[0].tolist()])\n",
        "print(output_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fGDFCshznDk",
        "outputId": "25190270-8083-4b2f-b854-f972c04ebad8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be,                                  ff f                                                                                   F OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOtOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOeOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOeOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOUOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOeOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOKOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n"
          ]
        }
      ]
    }
  ]
}