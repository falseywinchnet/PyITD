{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhpvvs3eqg3LSgTyHeHHLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/PyITD/blob/main/FusedQKVT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMyAv_7rxOyW",
        "outputId": "fddba3bf-c32b-4a56-b48f-b88d6028efac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Prepare the Shakespeare dataset for character-level language modeling.\n",
        "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
        "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
        "encoder and decoder and some other related info.\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    base_dir = Path(__file__).parent\n",
        "except NameError:\n",
        "    base_dir = Path(os.getcwd())  # fallback if __file__ is not defined (e.g. in REPL)\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(base_dir), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(base_dir), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(base_dir), 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(os.path.join(os.path.dirname(base_dir), 'meta.pkl'), 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#if you use my ideas, please credit me, dont just steal\n",
        "joshuah.rainstar@gmail.com\n"
      ],
      "metadata": {
        "id": "q47er8Ni84xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class RainstarActivation(nn.Module):\n",
        "    def __init__(self, p=0.5, kappa=1.0, gamma=2.0, s_min=0.48):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.kappa = kappa\n",
        "        self.gamma = gamma\n",
        "        self.s_min = s_min\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: any shape\n",
        "        a = x.abs()\n",
        "        # avoid log(0)\n",
        "        t = torch.log(a + 1e-12) + self.p * torch.log(torch.tanh(a) + 1e-12)\n",
        "        base = x.sign() * torch.exp(t)\n",
        "        clamp = self.s_min + (1 - self.s_min) / (1 + (a / self.kappa) ** self.gamma)\n",
        "        return base * clamp\n",
        "\n",
        "\n",
        "class ExpertMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Convex mixture-of-linear-experts MLP with Rainstar gating.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_experts: int = 4,\n",
        "                 p=0.5, kappa=1.0, gamma=2.0, s_min=0.48, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.eps = eps\n",
        "\n",
        "        # Rainstar gate applied to gate logits\n",
        "        self.rainstar = RainstarActivation(p, kappa, gamma, s_min)\n",
        "        # gating network: produces E pre-activations per token\n",
        "        self.gate = nn.Linear(d_model, num_experts)\n",
        "\n",
        "        # expert linear maps\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Linear(d_model, d_model) for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        returns: (B, T, D)\n",
        "        \"\"\"\n",
        "        # 1) compute raw gates and apply Rainstar\n",
        "        #    → shape (B, T, E)\n",
        "        gates_raw = self.rainstar(self.gate(x))\n",
        "\n",
        "        # 2) ensure non-negativity & normalize to convex weights\n",
        "        gates = gates_raw.abs() + self.eps\n",
        "        gates = gates / gates.sum(dim=-1, keepdim=True)  # along expert dim\n",
        "\n",
        "        # 3) compute each expert's linear output: (B, T, D, E)\n",
        "        expert_outs = torch.stack([expert(x) for expert in self.experts],\n",
        "                                  dim=-1)\n",
        "\n",
        "        # 4) convex combination over experts → (B, T, D)\n",
        "        y = (expert_outs * gates.unsqueeze(2)).sum(dim=-1)\n",
        "        return y\n",
        "\n",
        "@torch.jit.script\n",
        "def trig_modulate(x: torch.Tensor, pos: torch.Tensor,\n",
        "                  phase_index: int= 0) -> torch.Tensor:\n",
        "    B, H, T, D = x.shape\n",
        "    inv_freq = 1.0 / (10000 ** (torch.arange(0, D, dtype=torch.float32) / D)).to(x.device)\n",
        "    theta = torch.outer(pos.float(), inv_freq).to(x.device)  # [T, D]\n",
        "    phi = (math.pi / 3) * float(phase_index)\n",
        "    theta = theta + phi\n",
        "\n",
        "    mod = torch.cos(theta)\n",
        "    mod = mod.unsqueeze(0).unsqueeze(0)  # [1, 1, T, D]\n",
        "    return x * mod\n",
        "\n",
        "\n",
        "class TrigQKVAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q_mlp = ExpertMLP(d_model)\n",
        "        self.k_mlp =  ExpertMLP(d_model)\n",
        "        self.v_mlp =  ExpertMLP(d_model)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        H, d = self.n_heads, self.head_dim\n",
        "\n",
        "        # Positional indices\n",
        "        pos = torch.arange(T, device=x.device)\n",
        "\n",
        "        # Generate Q, K, V via distinct MLPs\n",
        "        Q = self.q_mlp(x).view(B, T, H, d).transpose(1, 2)  # [B, H, T, d]\n",
        "        K = self.k_mlp(x).view(B, T, H, d).transpose(1, 2)\n",
        "        V = self.v_mlp(x).view(B, T, H, d).transpose(1, 2)\n",
        "\n",
        "        # Trig-based modulation\n",
        "        Q = trig_modulate(Q, pos, 1)\n",
        "        K = trig_modulate(K, pos, 2)\n",
        "        V = trig_modulate(V, pos, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d)  # [B, H, T, T]\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = (attn @ V)  # [B, H, T, d]\n",
        "\n",
        "        out = out.transpose(1, 2).reshape(B, T, D)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class HybridBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-level processor: attention + MLP with residuals.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.attn = TrigQKVAttention(d_model, n_heads)\n",
        "        self.mlp =  ExpertMLP(d_model)\n",
        "        self.rainstar = RainstarActivation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.rainstar(self.attn(x))\n",
        "        mlp_out = self.mlp(attn_out)\n",
        "        x = x + self.rainstar(mlp_out)\n",
        "        return x\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Transformer-style model using HybridBlocks as layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=6):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            HybridBlock(d_model=d_model, n_heads=n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.rainstar = RainstarActivation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "\n",
        "        # apply all layers with Rainstar gating\n",
        "        for block in self.blocks:\n",
        "            x = self.rainstar(block(x))\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "MLs7qQUOxRPQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "@torch.jit.script\n",
        "def wolf_update(p: torch.Tensor,\n",
        "                g: torch.Tensor,\n",
        "                state_p: torch.Tensor,\n",
        "                lr: float):\n",
        "    # define your constants here instead of capturing them\n",
        "    etcerta: float = 0.367879441\n",
        "    et:      float = 1.0 - etcerta\n",
        "\n",
        "    # same logic as before\n",
        "    update    = state_p * et + g * etcerta\n",
        "    new_state = state_p * et + update * etcerta\n",
        "    sign_agree = torch.sign(update) * torch.sign(g)\n",
        "    update    = update + (torch.rand_like(update)*2 - 1) * etcerta * update\n",
        "    p_new     = torch.where(sign_agree > 0, p - lr * update, p)\n",
        "    return p_new, new_state\n",
        "\n",
        "class Wolf(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3):\n",
        "        defaults = dict(lr=lr)\n",
        "        super().__init__(params, defaults)\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                self.state[p]['p'] = torch.zeros_like(p)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = closure() if closure is not None else None\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state_p = self.state[p]['p']\n",
        "                p_new, new_state = wolf_update(p.data, p.grad, state_p, lr)\n",
        "                p.data.copy_(p_new)\n",
        "                state_p.copy_(new_state)\n",
        "        return loss\n",
        "\n",
        "# 1) Load data and meta as before\n",
        "data_dir  = os.path.dirname(base_dir)\n",
        "train_ids = np.fromfile(os.path.join(data_dir, 'train.bin'), dtype=np.uint16)\n",
        "val_ids   = np.fromfile(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16)\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# 2) Compute data‐marginal q[v]\n",
        "counts = np.bincount(train_ids, minlength=vocab_size).astype(float)\n",
        "q = torch.tensor(counts / counts.sum(), dtype=torch.float32, device=device)  # [V]\n",
        "\n",
        "# 3) Dataset + DataLoader\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = torch.from_numpy(data).long()\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx : idx + self.block_size]\n",
        "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "block_size = 128\n",
        "train_loader = DataLoader(CharDataset(train_ids, block_size),\n",
        "                          batch_size=32, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(CharDataset(val_ids,   block_size),\n",
        "                          batch_size=32, shuffle=False, drop_last=True)\n",
        "\n",
        "# 4) Model, optimizer, loss\n",
        "device    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "virgin     = TinyTransformer(vocab_size=vocab_size,\n",
        "                            d_model=256, n_heads=4, n_layers=4)\n",
        "model = torch.jit.script(virgin)\n",
        "model.to(device)\n",
        "optimizer = Wolf(model.parameters(), lr=0.3)#adam would explode at this training rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 5) Regularization weights\n",
        "λ_ent = 0.1   # entropy penalty weight\n",
        "λ_kl  = 0.5   # marginal‐KL penalty weight\n",
        "\n",
        "# 6) Train / eval functions\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # Forward\n",
        "        logits = model(xb)                 # (B, T, V)\n",
        "        B, T, V = logits.shape\n",
        "        p = F.softmax(logits, dim=-1)      # (B, T, V)\n",
        "\n",
        "        # 1) Standard CE\n",
        "        ce_loss = criterion(logits.view(B*T, V),\n",
        "                            yb.view(B*T))\n",
        "\n",
        "        # 2) Entropy penalty\n",
        "        ent = -(p * (p + 1e-12).log()).sum(dim=-1)  # (B, T)\n",
        "        ent_loss = ent.mean()\n",
        "\n",
        "        # 3) Marginal‐KL penalty\n",
        "        p_m = p.mean(dim=(0,1))            # [V]\n",
        "        kl_loss = (p_m * (p_m + 1e-12).log() - p_m * q.log()).sum()\n",
        "\n",
        "        # 4) Combined loss\n",
        "        loss = ce_loss + λ_ent * ent_loss + λ_kl * kl_loss\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item())\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        total_loss += criterion(logits.view(B*T,V),\n",
        "                                yb.view(B*T)).item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# 7) Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = eval_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "a8WKjMhexTa1",
        "outputId": "a4a3c67d-498b-448f-e417-068dceff7102"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.279724597930908\n",
            "5.275017261505127\n",
            "5.271737098693848\n",
            "5.265516757965088\n",
            "5.260200500488281\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-94ecbaf59932>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mval_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-94ecbaf59932>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# (B, T, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# (B, T, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bcontext_str = \"To be, or not to be,\"\n",
        "context_ids  = torch.tensor([[ stoi[c] for c in bcontext_str ]], dtype=torch.long)\n",
        "\n",
        "max_new_tokens = 5000\n",
        "temperature     = 1.0\n",
        "top_k           = 50\n",
        "block_size      = 128\n",
        "\n",
        "generated = context_ids  # (1, T)\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    # only pass at most block_size tokens into the model\n",
        "    input_ids = generated if generated.size(1) <= block_size \\\n",
        "                else generated[:, -block_size:]\n",
        "\n",
        "    logits = model(input_ids)              # (1, T_cur, vocab_size)\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    if top_k is not None:\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        logits[logits < v[:, [-1]]] = -1e10\n",
        "\n",
        "    probs   = F.softmax(logits, dim=-1)\n",
        "    next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
        "    generated = torch.cat([generated, next_id], dim=1)\n",
        "\n",
        "output_str = ''.join([itos[i] for i in generated[0].tolist()])\n",
        "print(output_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fGDFCshznDk",
        "outputId": "b4221b9c-f0b8-48e9-9c6a-f239359e369a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be, t\n",
            "te, t, t, t t, t, t,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,','E,,E,,,,,,,,,R,,,,,,,,,,,,,,'ZE'''''''''''''''''''''''''''''''''''''''''''E''''''''''''''''''q''''''''''''''''''''''''''''''''''''E''''''Q''''''''''''E'''''''''''''''''''''''''''''''''''''''''E'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E''''''''''''''''''''''''''''''''''''''''''''''''''''E''''''''''''''''''E'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''Q'E'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E''''''''''E''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E''''''''''''''''''''''''''''''''E''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E''''''''''''''''''''''''''''''''q''''''''''''''''''''''''''''''''''''''''''''''E'''''''''''''''''''''E'''''''Q''''''''''''''''''''''''''''''''''''''''''''''E''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E'''''''''''''''''''''''''''''''''''''''EQ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E'''''''''''''''''''''''''''''''''''''''x'''''''''''''''''''''''''''''''''''''E''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''E'''''''''''xxxxxxXBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBGBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n"
          ]
        }
      ]
    }
  ]
}