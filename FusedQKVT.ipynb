{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgKCxn7tDrT7fkddY0r5Lb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/PyITD/blob/main/FusedQKVT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMyAv_7rxOyW",
        "outputId": "1aeebb50-1f0b-438e-e877-ad4cce903459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Prepare the Shakespeare dataset for character-level language modeling.\n",
        "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
        "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
        "encoder and decoder and some other related info.\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    base_dir = Path(__file__).parent\n",
        "except NameError:\n",
        "    base_dir = Path(os.getcwd())  # fallback if __file__ is not defined (e.g. in REPL)\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(base_dir), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(base_dir), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(base_dir), 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(os.path.join(os.path.dirname(base_dir), 'meta.pkl'), 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#if you use my ideas, please credit me, dont just steal\n",
        "joshuah.rainstar@gmail.com\n"
      ],
      "metadata": {
        "id": "q47er8Ni84xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class BandHannFilter(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts the designated sub-band of x via RFFT + Hann window.\n",
        "    \"\"\"\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, depth, path_index):\n",
        "        B, T, D = x.shape\n",
        "        Fbins = T // 2 + 1\n",
        "        band_size = Fbins // (2 ** (depth + 1))\n",
        "        start = path_index * band_size\n",
        "        end = start + band_size\n",
        "\n",
        "        Xf = torch.fft.rfft(x, dim=1)\n",
        "        mask = torch.zeros(Fbins, device=x.device)\n",
        "        hann = torch.hann_window(band_size, device=x.device)\n",
        "        mask[start:end] = hann\n",
        "        mask = mask.view(1, Fbins, 1)\n",
        "        out = torch.fft.irfft(Xf * mask, n=T, dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def apply_rope(x):\n",
        "    B, H, T, D = x.shape\n",
        "    half = D // 2\n",
        "    freq = torch.exp(-torch.arange(0, half, 2, device=x.device) * (math.log(10000.0) / half))\n",
        "    pos = torch.arange(T, device=x.device).unsqueeze(1)\n",
        "    angles = pos * freq.unsqueeze(0)\n",
        "    sin = torch.sin(angles).repeat(1, 2)\n",
        "    cos = torch.cos(angles).repeat(1, 2)\n",
        "    x1, x2 = x[..., :half], x[..., half:]\n",
        "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
        "\n",
        "class QKVHybridAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid attention block: uses depth/path_index to filter K, returns x_out and per-node significance.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "        self.filter = BandHannFilter()\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, depth, path_index):\n",
        "        # Query via self-attention\n",
        "        q_out, _ = self.attn(x, x, x, need_weights=False)\n",
        "        # Key via band-pass filter at this depth/path_index\n",
        "        k_band = self.filter(x, depth, path_index)\n",
        "        # Value via MLP injection\n",
        "        v_out = self.mlp(x)\n",
        "\n",
        "        B, T, D = x.shape\n",
        "        def split(z): return z.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        Qh, Kh, Vh = split(q_out), split(k_band), split(v_out)\n",
        "        Qh = apply_rope(Qh)\n",
        "        Kh = apply_rope(Kh)\n",
        "\n",
        "        scores = (Qh @ Kh.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        w = F.softmax(scores, dim=-1)\n",
        "        significance = Qh.mean(dim=[1, 2, 3])  # [B]\n",
        "\n",
        "        o = (w @ Vh).transpose(1, 2).reshape(B, T, D)\n",
        "        return self.out(o), significance\n",
        "\n",
        "class HybridBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-level processor: attention + MLP with residuals.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.attn = QKVHybridAttention(d_model, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, depth, path_index):\n",
        "        h = self.norm1(x)\n",
        "        attn_out, sig = self.attn(h, depth, path_index)\n",
        "        x = x + attn_out\n",
        "        h2 = self.norm2(x)\n",
        "        mlp_out = self.mlp(h2)\n",
        "        x = x + mlp_out\n",
        "        return x, sig\n",
        "\n",
        "class TinyHybridGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Constructs a binary tree of HybridBlocks according to n_layers.\n",
        "    Final depth nodes' x and significance are used for selection, gating, fusion.\n",
        "    Blocks are pre-created in __init__ for each depth and node.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=3):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        # Pre-create HybridBlocks for each depth 0..n_layers-1\n",
        "        self.blocks_per_depth = nn.ModuleList()\n",
        "        for depth in range(n_layers):\n",
        "            num_blocks = 2 ** (depth + 1)\n",
        "            layer_blocks = nn.ModuleList([\n",
        "                HybridBlock(d_model=d_model, n_heads=n_heads)\n",
        "                for _ in range(num_blocks)\n",
        "            ])\n",
        "            self.blocks_per_depth.append(layer_blocks)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        x0 = self.token_emb(idx)\n",
        "        # BFS tree traversal\n",
        "        nodes = [(x0, 0)]  # (x, path_index)\n",
        "        sigs = []\n",
        "        for depth in range(self.n_layers):\n",
        "            next_nodes = []\n",
        "            # each depth has 2**(depth+1) blocks: one per child slot\n",
        "            blocks = self.blocks_per_depth[depth]\n",
        "            for idx_node, (x, pidx) in enumerate(nodes):\n",
        "                # assign distinct blocks for low and high children\n",
        "                low_block = blocks[2 * idx_node]\n",
        "                high_block = blocks[2 * idx_node + 1]\n",
        "                # compute child path indices\n",
        "                low_idx = pidx * 2\n",
        "                high_idx = pidx * 2 + 1\n",
        "                # process children with their own blocks\n",
        "                x_low, sig_low = low_block(x, depth, low_idx)\n",
        "                x_high, sig_high = high_block(x, depth, high_idx)\n",
        "                next_nodes.append((x_low, low_idx))\n",
        "                next_nodes.append((x_high, high_idx))\n",
        "            nodes = next_nodes\n",
        "        # At final depth, nodes has 2**n_layers items\n",
        "        # Collect final leaves and significance by re-running last depth's blocks\n",
        "        sigs = []\n",
        "        final_blocks = self.blocks_per_depth[self.n_layers-1]\n",
        "        for idx_node, (x, pidx) in enumerate(nodes):\n",
        "            blk = final_blocks[idx_node]\n",
        "            x_out, sig = blk(x, self.n_layers-1, pidx)\n",
        "            sigs.append((x_out, sig))\n",
        "        xs, S = zip(*sigs)\n",
        "        X = torch.stack(xs, dim=1)   # [B, M, T, D]\n",
        "        S = torch.stack(S, dim=1)    # [B, M]\n",
        "        gates = F.softmax(S, dim=1)               # [B, M], all >0, sum to 1\n",
        "        fused = (gates.unsqueeze(-1).unsqueeze(-1) * X).sum(dim=1)\n",
        "        x = self.norm(fused)\n",
        "        logits = self.head(x)\n",
        "        return logits, S\n"
      ],
      "metadata": {
        "id": "MLs7qQUOxRPQ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "import math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "class Wolf(Optimizer):\n",
        "  \"\"\"Implements Wolf algorithm.\"\"\"\n",
        "  #Wolf, also called Rainstar Optimizer, is fast. it is resistant to valleys and other things where adam hangs.\n",
        "  #on some problems, it is faster than adam. Try a high LR and lower it until it doesnt explode.\n",
        "  #wolf is initially smoother than adam over difficult plateaus and at high LR.\n",
        "\n",
        "\n",
        "  def __init__(self, params, lr=0.25, betas=(0.9, 0.999), eps=1e-8):\n",
        "        # Define default parameters\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
        "        self.lr = lr\n",
        "        # Initialize the parent Optimizer class first\n",
        "        super().__init__(params, defaults)\n",
        "        # Constants specific to Wolf\n",
        "        # Initialize state for each parameter\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                state['p'] = torch.zeros_like(p)  # Second moment estimate\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    \"\"\"Performs a single optimization step.\n",
        "\n",
        "    Args:\n",
        "      closure (callable, optional): A closure that reevaluates the model\n",
        "        and returns the loss.\n",
        "\n",
        "    Returns:\n",
        "      the loss.\n",
        "    \"\"\"\n",
        "    etcerta = 0.367879441\n",
        "    et = 1 - etcerta\n",
        "\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        state = self.state[p]\n",
        "\n",
        "            # Update step count\n",
        "\n",
        "\n",
        "        # Perform stepweight decay\n",
        "        grad = p.grad\n",
        "        state = self.state[p]\n",
        "        # State initialization\n",
        "\n",
        "        exp_avg = state['p']\n",
        "        # Weight update\n",
        "        update = exp_avg * et + grad * etcerta\n",
        "        state['p']  = exp_avg * et + update * etcerta\n",
        "        sign_agreement = torch.sign(update) * torch.sign(grad)\n",
        "\n",
        "        update = update + (torch.rand_like(update)*2 - 1) * etcerta * update\n",
        "        # Where signs agree (positive), apply normal update\n",
        "        mask = (sign_agreement > 0)\n",
        "        p.data = torch.where(mask,\n",
        "                            p.data - self.lr * update,\n",
        "                            p.data)\n",
        "    return loss\n",
        "# 2. Load the binary data\n",
        "data_dir = os.path.dirname(base_dir)\n",
        "train_ids = np.fromfile(os.path.join(data_dir, 'train.bin'), dtype=np.uint16)\n",
        "val_ids   = np.fromfile(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16)\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "vocab_size = meta['vocab_size']\n",
        "\n",
        "# 3. Define a simple PyTorch Dataset for next‐char prediction\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = torch.from_numpy(data).long()\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx : idx + self.block_size]\n",
        "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "block_size = 128\n",
        "train_ds = CharDataset(train_ids, block_size)\n",
        "val_ds   = CharDataset(val_ids,   block_size)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, drop_last=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = TinyHybridGPT(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters()), ' parameters')\n",
        "# 5. Set up optimizer & loss\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 6. Training loop\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits,S = model(xb)               # (B, T, vocab_size)\n",
        "        print(S)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        print(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# 7. Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = eval_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a8WKjMhexTa1",
        "outputId": "6c51db9f-fe4a-4680-911d-b0b8bd686378"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1229696  parameters\n",
            "tensor([[ 2.4489e-03, -5.0535e-03,  5.3906e-03,  1.8213e-02,  6.2389e-03,\n",
            "         -1.9977e-02,  1.2376e-04,  1.1495e-02],\n",
            "        [-3.2691e-03, -1.0262e-02,  9.8340e-03,  1.5876e-02,  1.1024e-02,\n",
            "         -1.8255e-02, -3.4833e-03,  1.5504e-02],\n",
            "        [ 4.1652e-03, -4.7368e-03,  1.2671e-02,  1.8207e-02,  1.0422e-02,\n",
            "         -1.7236e-02,  1.7473e-04,  9.9058e-03],\n",
            "        [ 1.7250e-03, -5.3229e-03,  1.4014e-02,  1.6561e-02,  7.5398e-03,\n",
            "         -2.3941e-02, -3.5752e-03,  1.5962e-02],\n",
            "        [-1.7691e-03, -4.9972e-03,  6.6807e-03,  2.0760e-02,  9.5559e-03,\n",
            "         -2.2263e-02, -1.8436e-05,  1.3636e-02],\n",
            "        [ 3.4264e-03, -5.7315e-03,  1.2903e-02,  1.3000e-02,  8.8614e-03,\n",
            "         -1.6049e-02, -3.9150e-03,  1.8445e-02],\n",
            "        [ 6.3008e-03, -5.9741e-03,  3.9536e-03,  2.2181e-02,  1.1852e-02,\n",
            "         -1.1942e-02, -4.1958e-04,  1.1580e-02],\n",
            "        [ 2.0568e-03, -6.7012e-03,  8.2796e-03,  1.8119e-02,  9.1765e-03,\n",
            "         -1.9789e-02, -3.7834e-03,  1.3285e-02],\n",
            "        [ 1.9551e-03, -1.0251e-02,  9.4484e-03,  2.1286e-02,  1.2276e-02,\n",
            "         -2.1185e-02, -2.0097e-03,  1.3709e-02],\n",
            "        [ 1.7207e-03, -7.4960e-03,  6.9138e-03,  2.1398e-02,  1.0322e-02,\n",
            "         -2.1272e-02, -1.2862e-03,  1.4224e-02],\n",
            "        [ 1.6272e-03, -1.1207e-02,  3.7510e-03,  1.8483e-02,  1.1942e-02,\n",
            "         -1.6305e-02, -3.3584e-03,  1.2801e-02],\n",
            "        [-1.6197e-03, -3.9170e-03,  1.0794e-02,  1.7218e-02,  1.2418e-02,\n",
            "         -1.4627e-02,  1.1881e-03,  9.1106e-03],\n",
            "        [ 1.7273e-03, -8.4745e-03,  1.1661e-02,  2.3106e-02,  1.6074e-02,\n",
            "         -1.8342e-02, -2.8032e-03,  1.4971e-02],\n",
            "        [ 1.5336e-03, -6.8299e-03,  1.1712e-02,  2.0968e-02,  1.3524e-02,\n",
            "         -1.8061e-02, -2.0154e-03,  1.2007e-02],\n",
            "        [ 6.3106e-03, -2.0198e-03,  9.1843e-03,  1.9101e-02,  1.0174e-02,\n",
            "         -1.6262e-02,  4.0993e-03,  9.1348e-03],\n",
            "        [-3.2300e-03, -9.4745e-03,  4.9265e-03,  1.8087e-02,  8.5427e-03,\n",
            "         -2.1926e-02, -3.1320e-03,  1.3172e-02],\n",
            "        [ 2.7183e-03, -5.4024e-03,  5.1455e-03,  2.6825e-02,  1.3120e-02,\n",
            "         -2.1687e-02,  1.4034e-03,  1.1497e-02],\n",
            "        [ 5.5779e-03, -7.7866e-03, -1.0062e-03,  1.8878e-02,  7.6584e-03,\n",
            "         -1.5918e-02, -3.2889e-03,  1.4607e-02],\n",
            "        [ 6.4947e-03, -5.2611e-03,  1.1639e-02,  2.1609e-02,  1.0103e-02,\n",
            "         -1.8582e-02, -2.8479e-03,  1.0883e-02],\n",
            "        [ 5.3667e-03, -4.8889e-03,  7.5345e-03,  2.1569e-02,  1.3451e-02,\n",
            "         -1.6838e-02, -2.3519e-03,  1.2214e-02],\n",
            "        [ 5.9228e-03, -4.2784e-03,  1.2067e-02,  1.9839e-02,  1.4408e-02,\n",
            "         -1.7011e-02,  4.3508e-03,  1.1681e-02],\n",
            "        [ 2.8681e-03, -8.9830e-03,  7.4779e-03,  1.8713e-02,  7.9454e-03,\n",
            "         -2.3252e-02, -5.2209e-03,  1.2930e-02],\n",
            "        [ 5.2317e-03, -7.9201e-03,  6.0307e-03,  2.4098e-02,  1.0549e-02,\n",
            "         -1.8341e-02, -1.0689e-03,  1.4959e-02],\n",
            "        [ 1.8278e-03, -9.5109e-03,  6.1275e-03,  2.0535e-02,  6.2138e-03,\n",
            "         -2.3207e-02, -3.6104e-03,  1.4832e-02],\n",
            "        [ 7.7832e-03, -7.7178e-03,  7.6254e-03,  2.0949e-02,  1.5620e-02,\n",
            "         -1.5308e-02, -3.2135e-03,  1.4857e-02],\n",
            "        [ 3.7805e-03, -7.9500e-03,  6.8504e-03,  2.0725e-02,  9.7504e-03,\n",
            "         -2.0367e-02, -1.6383e-03,  1.2165e-02],\n",
            "        [ 7.2816e-03, -5.2172e-03,  8.3611e-03,  2.1922e-02,  6.0942e-03,\n",
            "         -1.6605e-02, -7.3773e-04,  1.5395e-02],\n",
            "        [ 5.7277e-03, -7.0779e-03,  3.9924e-03,  1.4814e-02,  7.8916e-03,\n",
            "         -1.3956e-02, -5.7315e-03,  1.6879e-02],\n",
            "        [-3.5484e-03, -1.0234e-02,  8.5674e-03,  1.9859e-02,  7.1787e-03,\n",
            "         -2.5150e-02, -2.3483e-03,  1.3899e-02],\n",
            "        [ 2.3034e-03, -9.5092e-03,  8.9564e-03,  1.7364e-02,  9.5430e-03,\n",
            "         -2.0205e-02, -3.5311e-03,  1.5266e-02],\n",
            "        [ 1.2378e-03, -7.7421e-03,  1.4185e-02,  1.8211e-02,  9.1882e-03,\n",
            "         -2.0294e-02, -3.1347e-03,  1.6304e-02],\n",
            "        [ 5.7749e-03, -7.7286e-03,  3.0862e-03,  1.7953e-02,  1.0298e-02,\n",
            "         -1.6702e-02, -1.2156e-03,  1.4984e-02]], grad_fn=<StackBackward0>)\n",
            "4.266358852386475\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-66d40e5b8807>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mval_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:2d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-66d40e5b8807>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (B, T, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-d5fcb21bfe14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpidx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mblk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_blocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_node\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mx_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0msigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-d5fcb21bfe14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, depth, path_index)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mattn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-d5fcb21bfe14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, depth, path_index)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mQh\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mKh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0msignificance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits,S = model(xb)               # (B, T, vocab_size)\n",
        "        print(S)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        print(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# 7. Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = eval_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fzr5Nd2oJvqy",
        "outputId": "007842f9-8315-4978-8793-70cbed82a40d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "2.5811734199523926\n",
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "2.6293561458587646\n",
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "2.6391026973724365\n",
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "2.5947511196136475\n",
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "2.615562915802002\n",
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "2.588054895401001\n",
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "2.6159610748291016\n",
            "tensor([[0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078],\n",
            "        [0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078]],\n",
            "       grad_fn=<StackBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-bb6c535cd887>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mval_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:2d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-bb6c535cd887>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits,S = model(xb)               # (B, T, vocab_size)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        print(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# 7. Run training\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = eval_epoch()\n",
        "    print(f\"Epoch {epoch:2d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zdAbw3RjCGkX",
        "outputId": "63c70e06-558c-4fe0-e8f2-2514601d24ff"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.3272504806518555\n",
            "3.304229497909546\n",
            "3.2672877311706543\n",
            "3.2476792335510254\n",
            "3.275294303894043\n",
            "3.3815929889678955\n",
            "3.3547918796539307\n",
            "3.301131010055542\n",
            "3.3120720386505127\n",
            "3.323371410369873\n",
            "3.3096659183502197\n",
            "3.2997119426727295\n",
            "3.334392547607422\n",
            "3.2494325637817383\n",
            "3.360769510269165\n",
            "3.2848989963531494\n",
            "3.2806174755096436\n",
            "3.27695369720459\n",
            "3.331876039505005\n",
            "3.3024120330810547\n",
            "3.414670705795288\n",
            "3.354112148284912\n",
            "3.2829952239990234\n",
            "3.3061180114746094\n",
            "3.3639492988586426\n",
            "3.300365924835205\n",
            "3.3417983055114746\n",
            "3.287287473678589\n",
            "3.284991979598999\n",
            "3.3076436519622803\n",
            "3.2893238067626953\n",
            "3.3513002395629883\n",
            "3.3044443130493164\n",
            "3.3326029777526855\n",
            "3.396044969558716\n",
            "3.305065393447876\n",
            "3.280106544494629\n",
            "3.325437545776367\n",
            "3.3612101078033447\n",
            "3.3514082431793213\n",
            "3.287095546722412\n",
            "3.2867677211761475\n",
            "3.2750561237335205\n",
            "3.274765729904175\n",
            "3.314539909362793\n",
            "3.291142702102661\n",
            "3.330069065093994\n",
            "3.375945806503296\n",
            "3.3013930320739746\n",
            "3.2833380699157715\n",
            "3.292588710784912\n",
            "3.308565378189087\n",
            "3.2666268348693848\n",
            "3.29486346244812\n",
            "3.282639741897583\n",
            "3.305605173110962\n",
            "3.335564136505127\n",
            "3.275789976119995\n",
            "3.3261590003967285\n",
            "3.339806318283081\n",
            "3.3095180988311768\n",
            "3.3972091674804688\n",
            "3.3557708263397217\n",
            "3.2836122512817383\n",
            "3.3361451625823975\n",
            "3.3132545948028564\n",
            "3.299725294113159\n",
            "3.347503423690796\n",
            "3.318114995956421\n",
            "3.297595739364624\n",
            "3.3605594635009766\n",
            "3.2879254817962646\n",
            "3.3403639793395996\n",
            "3.297257423400879\n",
            "3.2924370765686035\n",
            "3.2564990520477295\n",
            "3.334789991378784\n",
            "3.278595447540283\n",
            "3.251507520675659\n",
            "3.345829725265503\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-41f2602f4452>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mval_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:2d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-41f2602f4452>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bcontext_str = \"To be, or not to be,\"\n",
        "context_ids  = torch.tensor([[ stoi[c] for c in bcontext_str ]], dtype=torch.long)\n",
        "\n",
        "max_new_tokens = 5000\n",
        "temperature     = 1.0\n",
        "top_k           = 50\n",
        "block_size      = 128\n",
        "\n",
        "generated = context_ids  # (1, T)\n",
        "\n",
        "for _ in range(max_new_tokens):\n",
        "    # only pass at most block_size tokens into the model\n",
        "    input_ids = generated if generated.size(1) <= block_size \\\n",
        "                else generated[:, -block_size:]\n",
        "\n",
        "    logits = model(input_ids)              # (1, T_cur, vocab_size)\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    if top_k is not None:\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        logits[logits < v[:, [-1]]] = -1e10\n",
        "\n",
        "    probs   = F.softmax(logits, dim=-1)\n",
        "    next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
        "    generated = torch.cat([generated, next_id], dim=1)\n",
        "\n",
        "output_str = ''.join([itos[i] for i in generated[0].tolist()])\n",
        "print(output_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fGDFCshznDk",
        "outputId": "b0426b43-3717-465a-bf89-e41d5980976f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be, t beet beet bebet bebet bebete be beToe bereribereribereribererieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieieioieieioieieeeieneeoieeoeoieioieieioeieoioeieeieieioeieieioeioioieioioaie eieeeeioeeeeioeoioioioioioeeeeeeeeeeeeeeeneeee eee eioeioeoeeeeeeeeeeeeeeeeeeeeeeeeeeeeenooooooooooooooo eeeeeeee eeeeoeeeeoeoeeoeoeeoeeeeeeeeeeeeeeeeeeioooooooooooooooooooooooo,ooeoeoeoeoeoeoeoeoooeoooooooooooo'eeeeeeeeeeeeeeeeeeeeeoeeoeoeoeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeoeeeeeeeeeeeeeeeeeoee eoeoeoeoeoeeneonooooooooooooooooooooooooooooooooooooooooooooooooooeeeeeeeeeeeeeeeeeeoooeeeooeoeoeoeooeoononoooonooooeeeeeooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooeeoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooaoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.linalg as la\n",
        "\n",
        "# ── Primitive definitions ─────────────────────────────────────────────────────\n",
        "\n",
        "def attention_map(Q, K, V):\n",
        "    \"\"\"\n",
        "    Q,K,V: (T, d)\n",
        "    returns: (T, d)\n",
        "    \"\"\"\n",
        "    scores = (Q @ K.T) / np.sqrt(Q.shape[1])\n",
        "    exp_scores = np.exp(scores - scores.max(axis=1, keepdims=True))\n",
        "    weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
        "    return weights @ V\n",
        "\n",
        "def s4d_map(x, B, C, alpha, beta):\n",
        "    \"\"\"\n",
        "    x: (T, d)\n",
        "    B,C,alpha,beta: (N,)\n",
        "    returns: y of shape (T, d)\n",
        "    \"\"\"\n",
        "    T, d = x.shape\n",
        "    N = len(B)\n",
        "    L = 2*T - 1\n",
        "\n",
        "    # Build complex-exponential kernels for each mode\n",
        "    t = np.arange(T)\n",
        "    A = -np.exp(alpha) + 1j*beta               # (N,)\n",
        "    modes = np.exp(A[:,None] * t[None,:])      # (N, T)\n",
        "    K = (B[:,None] * modes) * C[:,None]        # (N, T)\n",
        "    Kf = np.fft.fft(K, n=L, axis=1)            # (N, L)\n",
        "\n",
        "    # FFT of input features: (T,d) → (L,d)\n",
        "    Xf = np.fft.fft(x, n=L, axis=0)            # (L, d)\n",
        "\n",
        "    # Multiply per-mode and sum, then IFFT back\n",
        "    Yf = np.sum(Kf[:,:,None] * Xf[None,:,:], axis=0)  # (L, d)\n",
        "    y  = np.fft.ifft(Yf, n=L, axis=0).real            # (L, d)\n",
        "\n",
        "    return y[:T,:]                                    # (T, d)\n",
        "\n",
        "def mlp_map(x, W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "    x: (T, d)\n",
        "    returns: (T, d)\n",
        "    \"\"\"\n",
        "    # using ReLU for simplicity\n",
        "    h = np.maximum(0, x @ W1.T + b1)\n",
        "    return h @ W2.T + b2\n",
        "\n",
        "# ── Six permutation definitions ────────────────────────────────────────────────\n",
        "\n",
        "def Y_mode(m, x, params):\n",
        "    \"\"\"\n",
        "    x: (T, d)\n",
        "    params: dict of weight matrices/vectors\n",
        "    m: 1..6\n",
        "    returns: Y^{(m)}(x) of shape (T, d)\n",
        "    \"\"\"\n",
        "    # compute primitives with correct matmuls\n",
        "    Qx = attention_map(x @ params['WQ'], x @ params['WK'], x @ params['WV'])\n",
        "    Sx = s4d_map(x, params['B'], params['C'], params['alpha'], params['beta'])\n",
        "    Mx = mlp_map(x, params['W1'], params['b1'], params['W2'], params['b2'])\n",
        "\n",
        "    assign = {\n",
        "        1: (Qx, Sx, Mx),\n",
        "        2: (Qx, Mx, Sx),\n",
        "        3: (Sx, Qx, Mx),\n",
        "        4: (Sx, Mx, Qx),\n",
        "        5: (Mx, Qx, Sx),\n",
        "        6: (Mx, Sx, Qx),\n",
        "    }[m]\n",
        "\n",
        "    return attention_map(*assign)\n",
        "\n",
        "# ── Spectral‐norm Lipschitz bound ──────────────────────────────────────────────\n",
        "\n",
        "def spectral_norm(mat):\n",
        "    return la.svd(mat, compute_uv=False)[0]\n",
        "\n",
        "def compute_L_bound(m, norms):\n",
        "    pick = {\n",
        "        1: ('A','S','M'),\n",
        "        2: ('A','M','S'),\n",
        "        3: ('S','A','M'),\n",
        "        4: ('S','M','A'),\n",
        "        5: ('M','A','S'),\n",
        "        6: ('M','S','A'),\n",
        "    }[m]\n",
        "    def get_norm(role, prim):\n",
        "        if prim == 'A':\n",
        "            return norms[f\"A_{role}\"]\n",
        "        else:\n",
        "            return norms[prim]\n",
        "    LQ = get_norm('Q', pick[0])\n",
        "    LK = get_norm('K', pick[1])\n",
        "    LV = get_norm('V', pick[2])\n",
        "    return LQ * LK * LV\n",
        "\n",
        "# ── Hessian top‐eigenvalue via finite‐difference Hv ────────────────────────────\n",
        "\n",
        "def numeric_grad(f, x, h=1e-3):\n",
        "    T, d = x.shape\n",
        "    g = np.zeros_like(x)\n",
        "    fx = f(x)\n",
        "    for i in range(T):\n",
        "        for j in range(d):\n",
        "            xp = x.copy()\n",
        "            xp[i,j] += h\n",
        "            g[i,j] = (f(xp).sum() - fx.sum()) / h\n",
        "    return g\n",
        "\n",
        "def approx_top_eig(Yfunc, x0, eps=1e-3, iters=5):\n",
        "    theta = x0.flatten()\n",
        "    v = np.random.randn(theta.size)\n",
        "    v /= np.linalg.norm(v)\n",
        "    for _ in range(iters):\n",
        "        g0 = numeric_grad(Yfunc, x0).flatten()\n",
        "        xp = x0 + eps * v.reshape(x0.shape)\n",
        "        g1 = numeric_grad(Yfunc, xp).flatten()\n",
        "        Hv = (g1 - g0) / eps\n",
        "        v = Hv / (np.linalg.norm(Hv) + 1e-12)\n",
        "    return float(v @ Hv)\n",
        "\n",
        "# ── Main exploration ──────────────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    T, d, N = 20, 16, 4\n",
        "\n",
        "    # Random “trained” weights\n",
        "    params = {\n",
        "        'WQ':    np.random.randn(d,d),\n",
        "        'WK':    np.random.randn(d,d),\n",
        "        'WV':    np.random.randn(d,d),\n",
        "        'W1':    np.random.randn(4*d,d),\n",
        "        'b1':    np.random.randn(4*d),\n",
        "        'W2':    np.random.randn(d,4*d),\n",
        "        'b2':    np.random.randn(d),\n",
        "        'B':     np.random.randn(N),\n",
        "        'C':     np.random.randn(N),\n",
        "        'alpha': np.random.randn(N),\n",
        "        'beta':  np.random.randn(N),\n",
        "    }\n",
        "\n",
        "    # Precompute primitive norms\n",
        "    norms = {\n",
        "        'A_Q': spectral_norm(params['WQ']),\n",
        "        'A_K': spectral_norm(params['WK']),\n",
        "        'A_V': spectral_norm(params['WV']),\n",
        "        'M':   spectral_norm(params['W2']) * spectral_norm(params['W1']),\n",
        "        'S':   max(np.abs(np.fft.fft(\n",
        "                   s4d_map(np.random.randn(T,d),\n",
        "                          params['B'],params['C'],\n",
        "                          params['alpha'],params['beta']),\n",
        "                   n=2*T-1, axis=0).flatten())),\n",
        "    }\n",
        "\n",
        "    x0 = np.random.randn(T, d)\n",
        "\n",
        "    print(\"Mode | L_bound  | Hessian λ_max\")\n",
        "    for m in range(1, 7):\n",
        "        Lb  = compute_L_bound(m, norms)\n",
        "        eig = approx_top_eig(lambda x: Y_mode(m, x, params), x0)\n",
        "        print(f\"  {m}  | {Lb:8.3f} | {eig:8.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-gbQ2dob6hYU",
        "outputId": "5b9f85c4-1fdf-4f31-9e66-44e03dfc002c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode | L_bound  | Hessian λ_max\n",
            "  1  | 16971.954 | 76178.107\n",
            "  2  | 16971.954 | 399011.787\n",
            "  3  | 18144.262 | 24709.401\n",
            "  4  | 15723.201 | 3325.412\n",
            "  5  | 18144.262 | 13574.675\n",
            "  6  | 15723.201 | 8001.522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.linalg as la\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ── [ primitives: attention_map, s4d_map, mlp_map ] ─────────────────────────\n",
        "# (unchanged—copy from previous script)\n",
        "\n",
        "# ── Jacobian singular-value spectrum via randomized SVD ──────────────────────\n",
        "\n",
        "def topk_jac_svs(Yfunc, x0, k=10, n_probes=20, eps=1e-3):\n",
        "    \"\"\"\n",
        "    Approximate the top-k singular values of the Jacobian J=∂Y/∂x at x0.\n",
        "    We use randomized power method with directional derivatives.\n",
        "    \"\"\"\n",
        "    T, d = x0.shape\n",
        "    D = T*d\n",
        "    # probes: random vectors in input space\n",
        "    probes = np.random.randn(n_probes, D)\n",
        "    probes /= np.linalg.norm(probes, axis=1, keepdims=True)\n",
        "    # build Jv for each probe\n",
        "    Jv = np.zeros((D, n_probes))\n",
        "    for i, v in enumerate(probes):\n",
        "        v_mat = v.reshape(T, d)\n",
        "        # directional derivative: (Y(x+eps v) - Y(x-eps v)) / (2 eps)\n",
        "        Yp = Yfunc(x0 + eps*v_mat)\n",
        "        Ym = Yfunc(x0 - eps*v_mat)\n",
        "        diff = ((Yp - Ym) / (2*eps)).reshape(D)\n",
        "        Jv[:, i] = diff\n",
        "    # now approximate SVs of J via SVD on Jv (D×n_probes)\n",
        "    U, s, VT = la.svd(Jv, full_matrices=False)\n",
        "    return s[:k]\n",
        "\n",
        "# ── Directional second derivatives ───────────────────────────────────────────\n",
        "\n",
        "def directional_second_derivative(Yfunc, x0, v, eps=1e-3):\n",
        "    \"\"\"\n",
        "    Approximate v^T Hessian v at x0 via finite differences:\n",
        "      v^T H v ≈ (Y(x+eps v) - 2Y(x) + Y(x-eps v))·v / eps^2\n",
        "    \"\"\"\n",
        "    Y0 = Yfunc(x0).reshape(-1)\n",
        "    v_mat = v.reshape(x0.shape)\n",
        "    Yp = Yfunc(x0 + eps*v_mat).reshape(-1)\n",
        "    Ym = Yfunc(x0 - eps*v_mat).reshape(-1)\n",
        "    # project onto v\n",
        "    return float(((Yp - 2*Y0 + Ym) @ v) / (eps**2))\n",
        "\n",
        "def sample_second_derivatives(Yfunc, x0, num_dirs=50):\n",
        "    \"\"\"\n",
        "    Sample directional curvatures v^T H v for random unit v's.\n",
        "    \"\"\"\n",
        "    D = x0.size\n",
        "    vals = []\n",
        "    for _ in range(num_dirs):\n",
        "        v = np.random.randn(D)\n",
        "        v /= np.linalg.norm(v)\n",
        "        vals.append(directional_second_derivative(Yfunc, x0, v))\n",
        "    return np.array(vals)\n",
        "\n",
        "# ── Tangent vs normal gains on a PCA manifold ────────────────────────────────\n",
        "\n",
        "def tangent_normal_gains(Yfunc, X_batch, pca_dim=5, eps=1e-3):\n",
        "    \"\"\"\n",
        "    Estimate average gain ||Y(x+δ)-Y(x)||/||δ|| for δ along\n",
        "    - top-pca_dim directions (tangent)\n",
        "    - random orthonormal directions in complement (normal)\n",
        "    \"\"\"\n",
        "    B, T, d = X_batch.shape\n",
        "    flat = X_batch.reshape(B, T*d)\n",
        "    pca = PCA(n_components=pca_dim).fit(flat)\n",
        "    tangent_dirs = pca.components_       # (pca_dim, D)\n",
        "    # find orth complement basis via QR\n",
        "    Q, _ = np.linalg.qr(flat.T)           # Q: (D, D)\n",
        "    normal_dirs = Q[:, pca_dim:]          # (D, D-pca_dim)\n",
        "\n",
        "    gains = {'tangent': [], 'normal': []}\n",
        "    for x in flat:\n",
        "        x_mat = x.reshape(T, d)\n",
        "        Y0 = Yfunc(x_mat)\n",
        "        for v in tangent_dirs:\n",
        "            δ = v / np.linalg.norm(v)\n",
        "            Yp = Yfunc(x_mat + eps*δ.reshape(T,d))\n",
        "            gains['tangent'].append(np.linalg.norm(Yp-Y0)/eps)\n",
        "        for j in range(5):  # sample 5 random normals\n",
        "            v = normal_dirs[:, np.random.randint(normal_dirs.shape[1])]\n",
        "            v /= np.linalg.norm(v)\n",
        "            Yp = Yfunc(x_mat + eps*v.reshape(T,d))\n",
        "            gains['normal'].append(np.linalg.norm(Yp-Y0)/eps)\n",
        "    return {k: np.mean(v) for k,v in gains.items()}\n",
        "\n",
        "# ── Main exploration with extended analysis ─────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # [ set T,d,N, params, norms, x0 as before ]\n",
        "    # ...\n",
        "    X_batch = np.random.randn(10, T, d)  # synthetic “validation batch”\n",
        "\n",
        "    print(\"Mode | L_bound | λ_max |  top10 SVs... | 2nd-deriv stats | tangent vs normal\")\n",
        "    for m in range(1,7):\n",
        "        # Lipschitz & Hessian max\n",
        "        Lb  = compute_L_bound(m, norms)\n",
        "        lmax= approx_top_eig(lambda x: Y_mode(m,x,params), x0)\n",
        "        # Jacobian SVs\n",
        "        svs = topk_jac_svs(lambda x: Y_mode(m,x,params), x0, k=6)\n",
        "        # 2nd-derivative distribution\n",
        "        curvs = sample_second_derivatives(lambda x: Y_mode(m,x,params), x0, num_dirs=20)\n",
        "        sec_mean, sec_std = curvs.mean(), curvs.std()\n",
        "        # manifold gains\n",
        "        gains = tangent_normal_gains(lambda x: Y_mode(m,x,params), X_batch)\n",
        "        # ... compute Lb, lmax, svs, sec_mean, sec_std, gains as before ...\n",
        "        svs_str = \", \".join(f\"{v:.3f}\" for v in svs)\n",
        "        print(f\"{m:>4} | {Lb:7.1f} | {lmax:7.1f} | [{svs_str}] | \"\n",
        "              f\"2ndμ={sec_mean:.1f},σ={sec_std:.1f} | \"\n",
        "              f\"tan={gains['tangent']:.3f}, nor={gains['normal']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RltG9Nuq8Oeq",
        "outputId": "e2b3adf5-6859-45a5-cec8-784f8864a42b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode | L_bound | λ_max |  top10 SVs... | 2nd-deriv stats | tangent vs normal\n",
            "   1 | 16972.0 | 82645.0 | [194.092, 159.507, 123.940, 94.827, 74.558, 67.483] | 2ndμ=3.1,σ=28.7 | tan=80.706, nor=73.236\n",
            "   2 | 16972.0 | 399011.8 | [164.661, 23.946, 1.821, 1.603, 1.506, 1.492] | 2ndμ=1.2,σ=120.7 | tan=12.434, nor=16.881\n",
            "   3 | 18144.3 | 27116.0 | [339.537, 180.237, 134.718, 103.053, 84.968, 65.123] | 2ndμ=1.8,σ=6.9 | tan=88.987, nor=83.698\n",
            "   4 | 15723.2 |  3444.1 | [94.983, 79.802, 45.735, 42.337, 33.154, 29.991] | 2ndμ=0.1,σ=5.6 | tan=21.483, nor=20.794\n",
            "   5 | 18144.3 | 13575.5 | [64.068, 42.104, 28.238, 10.195, 8.300, 6.610] | 2ndμ=-0.2,σ=4.0 | tan=17.864, nor=15.867\n",
            "   6 | 15723.2 |  8283.2 | [68.914, 47.907, 43.308, 38.520, 35.510, 31.590] | 2ndμ=-0.1,σ=1.8 | tan=22.553, nor=22.915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Together, these hyper-dimensional signatures show Mode 4 is most predisposed to (a) collapse irrelevant dimensions, (b) preserve task-relevant structure, and (c) yield clusters of semantically related inputs in its output space."
      ],
      "metadata": {
        "id": "8H0sn-Xd8uKn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LxHwNgk8trz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}